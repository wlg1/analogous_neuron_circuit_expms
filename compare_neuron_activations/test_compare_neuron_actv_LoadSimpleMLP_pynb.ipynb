{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb"
      ],
      "metadata": {
        "id": "fjXPiaYf3JUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os \n",
        "# import torchvision.models as models\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image"
      ],
      "metadata": {
        "id": "oI54g5-954Ei"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Feedforward(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Feedforward, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size  = hidden_size\n",
        "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        hidden = self.fc1(x)\n",
        "        relu = self.relu(hidden)\n",
        "        output = self.fc2(relu)\n",
        "        output = self.sigmoid(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "mSBMQ-0Z7QAk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Feedforward(2, 2)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "iiisCNBx69JC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get previous last layer name\n",
        "named_layers = dict(model.named_modules())\n",
        "layers = list(named_layers.keys())\n",
        "\n",
        "# too many branches, so just get the converged branch points\n",
        "# '' is first layer, the input, so disregard it\n",
        "layers = [x for x in layers if '.' not in x and x != '']  "
      ],
      "metadata": {
        "id": "yy5KYfSz5_Uk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IfyDxoH718P",
        "outputId": "b82a52ef-bfd0-4b82-fb3f-f342c0bda82a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fc1', 'relu', 'fc2', 'sigmoid']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE RANDOM DATA POINTS\n",
        "from sklearn.datasets import make_blobs\n",
        "def blob_label(y, label, loc): # assign labels\n",
        "    target = np.copy(y)\n",
        "    for l in loc:\n",
        "        target[y == l] = label\n",
        "    return target\n",
        "x_train, y_train = make_blobs(n_samples=100, n_features=2, cluster_std=1.5, shuffle=True)\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
        "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
        "x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
        "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))"
      ],
      "metadata": {
        "id": "Bh0213vQy8_Q"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred = model(x_test)\n",
        "before_train = criterion(y_pred.squeeze(), y_test)\n",
        "print('Test loss before training' , before_train.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIjtEr_G1egc",
        "outputId": "0b31820a-a739-4d75-a81a-2a64c5d46ec1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss before training 0.9458913803100586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "epoch = 3000\n",
        "\n",
        "for epoch in range(epoch):\n",
        "    #sets the gradients to zero before we start backpropagation. \n",
        "    #This is a necessary step as PyTorch accumulates the gradients from the backward passes from the previous epochs.\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    y_pred = model(x_train)\n",
        "    # Compute Loss\n",
        "    loss = criterion(y_pred.squeeze(), y_train)\n",
        "   \n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVkrObyb1PeJ",
        "outputId": "552891a4-9d6c-4e94-b3e9-11b06b89eb75"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train loss: 0.18348564207553864\n",
            "Epoch 1: train loss: 0.1834333837032318\n",
            "Epoch 2: train loss: 0.18338237702846527\n",
            "Epoch 3: train loss: 0.18332979083061218\n",
            "Epoch 4: train loss: 0.18327760696411133\n",
            "Epoch 5: train loss: 0.18322676420211792\n",
            "Epoch 6: train loss: 0.1831742525100708\n",
            "Epoch 7: train loss: 0.1831221729516983\n",
            "Epoch 8: train loss: 0.18307146430015564\n",
            "Epoch 9: train loss: 0.1830189824104309\n",
            "Epoch 10: train loss: 0.18296703696250916\n",
            "Epoch 11: train loss: 0.18291644752025604\n",
            "Epoch 12: train loss: 0.18286414444446564\n",
            "Epoch 13: train loss: 0.18281228840351105\n",
            "Epoch 14: train loss: 0.1827617883682251\n",
            "Epoch 15: train loss: 0.18270964920520782\n",
            "Epoch 16: train loss: 0.1826578676700592\n",
            "Epoch 17: train loss: 0.182607501745224\n",
            "Epoch 18: train loss: 0.1825554519891739\n",
            "Epoch 19: train loss: 0.18250374495983124\n",
            "Epoch 20: train loss: 0.1824534833431244\n",
            "Epoch 21: train loss: 0.18240152299404144\n",
            "Epoch 22: train loss: 0.18234993517398834\n",
            "Epoch 23: train loss: 0.18229980766773224\n",
            "Epoch 24: train loss: 0.18224798142910004\n",
            "Epoch 25: train loss: 0.1821964979171753\n",
            "Epoch 26: train loss: 0.18214648962020874\n",
            "Epoch 27: train loss: 0.1820947676897049\n",
            "Epoch 28: train loss: 0.1820434033870697\n",
            "Epoch 29: train loss: 0.1819934844970703\n",
            "Epoch 30: train loss: 0.18194183707237244\n",
            "Epoch 31: train loss: 0.18189063668251038\n",
            "Epoch 32: train loss: 0.18184076249599457\n",
            "Epoch 33: train loss: 0.1817893236875534\n",
            "Epoch 34: train loss: 0.18173816800117493\n",
            "Epoch 35: train loss: 0.18168848752975464\n",
            "Epoch 36: train loss: 0.18163703382015228\n",
            "Epoch 37: train loss: 0.18158601224422455\n",
            "Epoch 38: train loss: 0.18153642117977142\n",
            "Epoch 39: train loss: 0.1814851611852646\n",
            "Epoch 40: train loss: 0.1814342886209488\n",
            "Epoch 41: train loss: 0.18138478696346283\n",
            "Epoch 42: train loss: 0.18133355677127838\n",
            "Epoch 43: train loss: 0.18128280341625214\n",
            "Epoch 44: train loss: 0.18123339116573334\n",
            "Epoch 45: train loss: 0.18118230998516083\n",
            "Epoch 46: train loss: 0.18113166093826294\n",
            "Epoch 47: train loss: 0.1810823231935501\n",
            "Epoch 48: train loss: 0.18103131651878357\n",
            "Epoch 49: train loss: 0.18098081648349762\n",
            "Epoch 50: train loss: 0.18093158304691315\n",
            "Epoch 51: train loss: 0.18088077008724213\n",
            "Epoch 52: train loss: 0.18083035945892334\n",
            "Epoch 53: train loss: 0.18078121542930603\n",
            "Epoch 54: train loss: 0.1807304322719574\n",
            "Epoch 55: train loss: 0.18068015575408936\n",
            "Epoch 56: train loss: 0.1806311458349228\n",
            "Epoch 57: train loss: 0.1805804818868637\n",
            "Epoch 58: train loss: 0.1805303394794464\n",
            "Epoch 59: train loss: 0.18048137426376343\n",
            "Epoch 60: train loss: 0.1804308146238327\n",
            "Epoch 61: train loss: 0.18038076162338257\n",
            "Epoch 62: train loss: 0.18033190071582794\n",
            "Epoch 63: train loss: 0.18028146028518677\n",
            "Epoch 64: train loss: 0.18023155629634857\n",
            "Epoch 65: train loss: 0.18018274009227753\n",
            "Epoch 66: train loss: 0.1801324486732483\n",
            "Epoch 67: train loss: 0.18008266389369965\n",
            "Epoch 68: train loss: 0.18003395199775696\n",
            "Epoch 69: train loss: 0.1799837350845337\n",
            "Epoch 70: train loss: 0.1799340695142746\n",
            "Epoch 71: train loss: 0.17988547682762146\n",
            "Epoch 72: train loss: 0.17983537912368774\n",
            "Epoch 73: train loss: 0.1797858029603958\n",
            "Epoch 74: train loss: 0.17973729968070984\n",
            "Epoch 75: train loss: 0.1796872466802597\n",
            "Epoch 76: train loss: 0.17963792383670807\n",
            "Epoch 77: train loss: 0.1795894056558609\n",
            "Epoch 78: train loss: 0.17953947186470032\n",
            "Epoch 79: train loss: 0.17949023842811584\n",
            "Epoch 80: train loss: 0.1794419139623642\n",
            "Epoch 81: train loss: 0.17939208447933197\n",
            "Epoch 82: train loss: 0.17934295535087585\n",
            "Epoch 83: train loss: 0.1792946457862854\n",
            "Epoch 84: train loss: 0.17924495041370392\n",
            "Epoch 85: train loss: 0.17919600009918213\n",
            "Epoch 86: train loss: 0.17914773523807526\n",
            "Epoch 87: train loss: 0.17909817397594452\n",
            "Epoch 88: train loss: 0.17904934287071228\n",
            "Epoch 89: train loss: 0.17900113761425018\n",
            "Epoch 90: train loss: 0.17895165085792542\n",
            "Epoch 91: train loss: 0.17890292406082153\n",
            "Epoch 92: train loss: 0.17885485291481018\n",
            "Epoch 93: train loss: 0.1788054257631302\n",
            "Epoch 94: train loss: 0.17875683307647705\n",
            "Epoch 95: train loss: 0.17870883643627167\n",
            "Epoch 96: train loss: 0.1786595731973648\n",
            "Epoch 97: train loss: 0.1786111295223236\n",
            "Epoch 98: train loss: 0.17856313288211823\n",
            "Epoch 99: train loss: 0.1785140037536621\n",
            "Epoch 100: train loss: 0.17846569418907166\n",
            "Epoch 101: train loss: 0.17841777205467224\n",
            "Epoch 102: train loss: 0.1783687025308609\n",
            "Epoch 103: train loss: 0.17832055687904358\n",
            "Epoch 104: train loss: 0.17827270925045013\n",
            "Epoch 105: train loss: 0.17822369933128357\n",
            "Epoch 106: train loss: 0.17817570269107819\n",
            "Epoch 107: train loss: 0.1781279742717743\n",
            "Epoch 108: train loss: 0.17807908356189728\n",
            "Epoch 109: train loss: 0.17803117632865906\n",
            "Epoch 110: train loss: 0.17798349261283875\n",
            "Epoch 111: train loss: 0.17793472111225128\n",
            "Epoch 112: train loss: 0.1778869777917862\n",
            "Epoch 113: train loss: 0.17783933877944946\n",
            "Epoch 114: train loss: 0.17779064178466797\n",
            "Epoch 115: train loss: 0.1777430772781372\n",
            "Epoch 116: train loss: 0.17769552767276764\n",
            "Epoch 117: train loss: 0.17764689028263092\n",
            "Epoch 118: train loss: 0.17759941518306732\n",
            "Epoch 119: train loss: 0.17755194008350372\n",
            "Epoch 120: train loss: 0.17750345170497894\n",
            "Epoch 121: train loss: 0.17745612561702728\n",
            "Epoch 122: train loss: 0.17740869522094727\n",
            "Epoch 123: train loss: 0.17736032605171204\n",
            "Epoch 124: train loss: 0.17731314897537231\n",
            "Epoch 125: train loss: 0.1772657334804535\n",
            "Epoch 126: train loss: 0.17721743881702423\n",
            "Epoch 127: train loss: 0.17717041075229645\n",
            "Epoch 128: train loss: 0.17712312936782837\n",
            "Epoch 129: train loss: 0.17707495391368866\n",
            "Epoch 130: train loss: 0.17702798545360565\n",
            "Epoch 131: train loss: 0.17698077857494354\n",
            "Epoch 132: train loss: 0.1769326776266098\n",
            "Epoch 133: train loss: 0.17688590288162231\n",
            "Epoch 134: train loss: 0.1768386960029602\n",
            "Epoch 135: train loss: 0.1767907291650772\n",
            "Epoch 136: train loss: 0.17674413323402405\n",
            "Epoch 137: train loss: 0.17669695615768433\n",
            "Epoch 138: train loss: 0.1766490936279297\n",
            "Epoch 139: train loss: 0.1766025722026825\n",
            "Epoch 140: train loss: 0.17655548453330994\n",
            "Epoch 141: train loss: 0.17650774121284485\n",
            "Epoch 142: train loss: 0.1764613687992096\n",
            "Epoch 143: train loss: 0.1764143407344818\n",
            "Epoch 144: train loss: 0.17636661231517792\n",
            "Epoch 145: train loss: 0.176320418715477\n",
            "Epoch 146: train loss: 0.17627346515655518\n",
            "Epoch 147: train loss: 0.1762259155511856\n",
            "Epoch 148: train loss: 0.17617976665496826\n",
            "Epoch 149: train loss: 0.1761329025030136\n",
            "Epoch 150: train loss: 0.17608538269996643\n",
            "Epoch 151: train loss: 0.17603947222232819\n",
            "Epoch 152: train loss: 0.17599260807037354\n",
            "Epoch 153: train loss: 0.1759452223777771\n",
            "Epoch 154: train loss: 0.1758994162082672\n",
            "Epoch 155: train loss: 0.17585259675979614\n",
            "Epoch 156: train loss: 0.17580528557300568\n",
            "Epoch 157: train loss: 0.1757597029209137\n",
            "Epoch 158: train loss: 0.17571288347244263\n",
            "Epoch 159: train loss: 0.1756657361984253\n",
            "Epoch 160: train loss: 0.1756202131509781\n",
            "Epoch 161: train loss: 0.17557352781295776\n",
            "Epoch 162: train loss: 0.175526425242424\n",
            "Epoch 163: train loss: 0.17548105120658875\n",
            "Epoch 164: train loss: 0.1754343956708908\n",
            "Epoch 165: train loss: 0.1753873974084854\n",
            "Epoch 166: train loss: 0.1753421425819397\n",
            "Epoch 167: train loss: 0.17529553174972534\n",
            "Epoch 168: train loss: 0.17524868249893188\n",
            "Epoch 169: train loss: 0.1752035766839981\n",
            "Epoch 170: train loss: 0.17515701055526733\n",
            "Epoch 171: train loss: 0.17511020600795746\n",
            "Epoch 172: train loss: 0.1750652939081192\n",
            "Epoch 173: train loss: 0.175018772482872\n",
            "Epoch 174: train loss: 0.1749720722436905\n",
            "Epoch 175: train loss: 0.1749272495508194\n",
            "Epoch 176: train loss: 0.17488080263137817\n",
            "Epoch 177: train loss: 0.174834206700325\n",
            "Epoch 178: train loss: 0.17478960752487183\n",
            "Epoch 179: train loss: 0.1747429072856903\n",
            "Epoch 180: train loss: 0.1746968775987625\n",
            "Epoch 181: train loss: 0.17465196549892426\n",
            "Epoch 182: train loss: 0.17460551857948303\n",
            "Epoch 183: train loss: 0.17455966770648956\n",
            "Epoch 184: train loss: 0.1745147556066513\n",
            "Epoch 185: train loss: 0.17446836829185486\n",
            "Epoch 186: train loss: 0.17442269623279572\n",
            "Epoch 187: train loss: 0.17437779903411865\n",
            "Epoch 188: train loss: 0.17433159053325653\n",
            "Epoch 189: train loss: 0.17428608238697052\n",
            "Epoch 190: train loss: 0.17424114048480988\n",
            "Epoch 191: train loss: 0.17419496178627014\n",
            "Epoch 192: train loss: 0.17414970695972443\n",
            "Epoch 193: train loss: 0.17410480976104736\n",
            "Epoch 194: train loss: 0.17405876517295837\n",
            "Epoch 195: train loss: 0.17401355504989624\n",
            "Epoch 196: train loss: 0.17396870255470276\n",
            "Epoch 197: train loss: 0.17392271757125854\n",
            "Epoch 198: train loss: 0.17387773096561432\n",
            "Epoch 199: train loss: 0.17383290827274323\n",
            "Epoch 200: train loss: 0.17378704249858856\n",
            "Epoch 201: train loss: 0.1737421602010727\n",
            "Epoch 202: train loss: 0.17369739711284637\n",
            "Epoch 203: train loss: 0.1736515760421753\n",
            "Epoch 204: train loss: 0.17360691726207733\n",
            "Epoch 205: train loss: 0.17356213927268982\n",
            "Epoch 206: train loss: 0.17351645231246948\n",
            "Epoch 207: train loss: 0.17347188293933868\n",
            "Epoch 208: train loss: 0.17342722415924072\n",
            "Epoch 209: train loss: 0.17338158190250397\n",
            "Epoch 210: train loss: 0.1733371764421463\n",
            "Epoch 211: train loss: 0.17329250276088715\n",
            "Epoch 212: train loss: 0.17324700951576233\n",
            "Epoch 213: train loss: 0.17320279777050018\n",
            "Epoch 214: train loss: 0.17315807938575745\n",
            "Epoch 215: train loss: 0.173112690448761\n",
            "Epoch 216: train loss: 0.1730685979127884\n",
            "Epoch 217: train loss: 0.173023983836174\n",
            "Epoch 218: train loss: 0.17297865450382233\n",
            "Epoch 219: train loss: 0.17293474078178406\n",
            "Epoch 220: train loss: 0.1728900969028473\n",
            "Epoch 221: train loss: 0.17284487187862396\n",
            "Epoch 222: train loss: 0.17280106246471405\n",
            "Epoch 223: train loss: 0.17275652289390564\n",
            "Epoch 224: train loss: 0.17271141707897186\n",
            "Epoch 225: train loss: 0.17266777157783508\n",
            "Epoch 226: train loss: 0.17262323200702667\n",
            "Epoch 227: train loss: 0.1725781410932541\n",
            "Epoch 228: train loss: 0.17253467440605164\n",
            "Epoch 229: train loss: 0.1724901646375656\n",
            "Epoch 230: train loss: 0.1724451780319214\n",
            "Epoch 231: train loss: 0.17240189015865326\n",
            "Epoch 232: train loss: 0.17235736548900604\n",
            "Epoch 233: train loss: 0.17231251299381256\n",
            "Epoch 234: train loss: 0.17226935923099518\n",
            "Epoch 235: train loss: 0.17222490906715393\n",
            "Epoch 236: train loss: 0.1721801608800888\n",
            "Epoch 237: train loss: 0.17213711142539978\n",
            "Epoch 238: train loss: 0.17209270596504211\n",
            "Epoch 239: train loss: 0.17204800248146057\n",
            "Epoch 240: train loss: 0.17200513184070587\n",
            "Epoch 241: train loss: 0.1719607561826706\n",
            "Epoch 242: train loss: 0.17191612720489502\n",
            "Epoch 243: train loss: 0.17187339067459106\n",
            "Epoch 244: train loss: 0.17182902991771698\n",
            "Epoch 245: train loss: 0.17178457975387573\n",
            "Epoch 246: train loss: 0.17174197733402252\n",
            "Epoch 247: train loss: 0.1716974377632141\n",
            "Epoch 248: train loss: 0.17165344953536987\n",
            "Epoch 249: train loss: 0.17161062359809875\n",
            "Epoch 250: train loss: 0.17156627774238586\n",
            "Epoch 251: train loss: 0.17152252793312073\n",
            "Epoch 252: train loss: 0.17147965729236603\n",
            "Epoch 253: train loss: 0.17143535614013672\n",
            "Epoch 254: train loss: 0.1713918298482895\n",
            "Epoch 255: train loss: 0.1713489145040512\n",
            "Epoch 256: train loss: 0.17130476236343384\n",
            "Epoch 257: train loss: 0.17126138508319855\n",
            "Epoch 258: train loss: 0.17121846973896027\n",
            "Epoch 259: train loss: 0.17117443680763245\n",
            "Epoch 260: train loss: 0.17113114893436432\n",
            "Epoch 261: train loss: 0.17108827829360962\n",
            "Epoch 262: train loss: 0.1710442751646042\n",
            "Epoch 263: train loss: 0.17100124061107635\n",
            "Epoch 264: train loss: 0.17095838487148285\n",
            "Epoch 265: train loss: 0.170914426445961\n",
            "Epoch 266: train loss: 0.1708715856075287\n",
            "Epoch 267: train loss: 0.17082872986793518\n",
            "Epoch 268: train loss: 0.17078489065170288\n",
            "Epoch 269: train loss: 0.17074216902256012\n",
            "Epoch 270: train loss: 0.1706993728876114\n",
            "Epoch 271: train loss: 0.17065557837486267\n",
            "Epoch 272: train loss: 0.17061306536197662\n",
            "Epoch 273: train loss: 0.17057017982006073\n",
            "Epoch 274: train loss: 0.17052656412124634\n",
            "Epoch 275: train loss: 0.17048415541648865\n",
            "Epoch 276: train loss: 0.17044135928153992\n",
            "Epoch 277: train loss: 0.17039771378040314\n",
            "Epoch 278: train loss: 0.17035549879074097\n",
            "Epoch 279: train loss: 0.17031268775463104\n",
            "Epoch 280: train loss: 0.1702692061662674\n",
            "Epoch 281: train loss: 0.17022718489170074\n",
            "Epoch 282: train loss: 0.1701844036579132\n",
            "Epoch 283: train loss: 0.17014095187187195\n",
            "Epoch 284: train loss: 0.17009903490543365\n",
            "Epoch 285: train loss: 0.1700562983751297\n",
            "Epoch 286: train loss: 0.170012965798378\n",
            "Epoch 287: train loss: 0.16997119784355164\n",
            "Epoch 288: train loss: 0.16992850601673126\n",
            "Epoch 289: train loss: 0.16988521814346313\n",
            "Epoch 290: train loss: 0.16984358429908752\n",
            "Epoch 291: train loss: 0.16980083286762238\n",
            "Epoch 292: train loss: 0.16975772380828857\n",
            "Epoch 293: train loss: 0.1697162389755249\n",
            "Epoch 294: train loss: 0.1696735918521881\n",
            "Epoch 295: train loss: 0.16963054239749908\n",
            "Epoch 296: train loss: 0.16958917677402496\n",
            "Epoch 297: train loss: 0.16954652965068817\n",
            "Epoch 298: train loss: 0.16950355470180511\n",
            "Epoch 299: train loss: 0.1694623976945877\n",
            "Epoch 300: train loss: 0.16941970586776733\n",
            "Epoch 301: train loss: 0.16937683522701263\n",
            "Epoch 302: train loss: 0.16933582723140717\n",
            "Epoch 303: train loss: 0.16929315030574799\n",
            "Epoch 304: train loss: 0.16925041377544403\n",
            "Epoch 305: train loss: 0.1692095398902893\n",
            "Epoch 306: train loss: 0.16916675865650177\n",
            "Epoch 307: train loss: 0.16912446916103363\n",
            "Epoch 308: train loss: 0.16908332705497742\n",
            "Epoch 309: train loss: 0.16904063522815704\n",
            "Epoch 310: train loss: 0.16899865865707397\n",
            "Epoch 311: train loss: 0.1689574122428894\n",
            "Epoch 312: train loss: 0.16891483962535858\n",
            "Epoch 313: train loss: 0.16887301206588745\n",
            "Epoch 314: train loss: 0.16883176565170288\n",
            "Epoch 315: train loss: 0.16878929734230042\n",
            "Epoch 316: train loss: 0.16874763369560242\n",
            "Epoch 317: train loss: 0.16870641708374023\n",
            "Epoch 318: train loss: 0.16866397857666016\n",
            "Epoch 319: train loss: 0.16862250864505768\n",
            "Epoch 320: train loss: 0.1685812771320343\n",
            "Epoch 321: train loss: 0.16853900253772736\n",
            "Epoch 322: train loss: 0.16849762201309204\n",
            "Epoch 323: train loss: 0.16845640540122986\n",
            "Epoch 324: train loss: 0.1684141904115677\n",
            "Epoch 325: train loss: 0.1683729887008667\n",
            "Epoch 326: train loss: 0.1683318018913269\n",
            "Epoch 327: train loss: 0.16828961670398712\n",
            "Epoch 328: train loss: 0.16824866831302643\n",
            "Epoch 329: train loss: 0.16820739209651947\n",
            "Epoch 330: train loss: 0.16816535592079163\n",
            "Epoch 331: train loss: 0.1681244671344757\n",
            "Epoch 332: train loss: 0.1680833101272583\n",
            "Epoch 333: train loss: 0.16804128885269165\n",
            "Epoch 334: train loss: 0.16800060868263245\n",
            "Epoch 335: train loss: 0.16795942187309265\n",
            "Epoch 336: train loss: 0.16791750490665436\n",
            "Epoch 337: train loss: 0.1678769737482071\n",
            "Epoch 338: train loss: 0.16783583164215088\n",
            "Epoch 339: train loss: 0.16779395937919617\n",
            "Epoch 340: train loss: 0.16775357723236084\n",
            "Epoch 341: train loss: 0.16771242022514343\n",
            "Epoch 342: train loss: 0.16767066717147827\n",
            "Epoch 343: train loss: 0.1676304042339325\n",
            "Epoch 344: train loss: 0.1675892472267151\n",
            "Epoch 345: train loss: 0.1675475835800171\n",
            "Epoch 346: train loss: 0.16750751435756683\n",
            "Epoch 347: train loss: 0.16746637225151062\n",
            "Epoch 348: train loss: 0.16742481291294098\n",
            "Epoch 349: train loss: 0.16738487780094147\n",
            "Epoch 350: train loss: 0.16734369099140167\n",
            "Epoch 351: train loss: 0.167302206158638\n",
            "Epoch 352: train loss: 0.16726242005825043\n",
            "Epoch 353: train loss: 0.167221337556839\n",
            "Epoch 354: train loss: 0.1671798825263977\n",
            "Epoch 355: train loss: 0.16714023053646088\n",
            "Epoch 356: train loss: 0.16709911823272705\n",
            "Epoch 357: train loss: 0.1670577973127365\n",
            "Epoch 358: train loss: 0.16701827943325043\n",
            "Epoch 359: train loss: 0.16697722673416138\n",
            "Epoch 360: train loss: 0.1669360101222992\n",
            "Epoch 361: train loss: 0.16689662635326385\n",
            "Epoch 362: train loss: 0.16685539484024048\n",
            "Epoch 363: train loss: 0.1668146550655365\n",
            "Epoch 364: train loss: 0.16677503287792206\n",
            "Epoch 365: train loss: 0.16673387587070465\n",
            "Epoch 366: train loss: 0.16669341921806335\n",
            "Epoch 367: train loss: 0.16665370762348175\n",
            "Epoch 368: train loss: 0.1666126847267151\n",
            "Epoch 369: train loss: 0.16657236218452454\n",
            "Epoch 370: train loss: 0.16653263568878174\n",
            "Epoch 371: train loss: 0.16649170219898224\n",
            "Epoch 372: train loss: 0.16645152866840363\n",
            "Epoch 373: train loss: 0.16641183197498322\n",
            "Epoch 374: train loss: 0.1663709580898285\n",
            "Epoch 375: train loss: 0.166330948472023\n",
            "Epoch 376: train loss: 0.1662912368774414\n",
            "Epoch 377: train loss: 0.16625043749809265\n",
            "Epoch 378: train loss: 0.1662106066942215\n",
            "Epoch 379: train loss: 0.16617092490196228\n",
            "Epoch 380: train loss: 0.1661301851272583\n",
            "Epoch 381: train loss: 0.1660904884338379\n",
            "Epoch 382: train loss: 0.16605082154273987\n",
            "Epoch 383: train loss: 0.16601017117500305\n",
            "Epoch 384: train loss: 0.16597066819667816\n",
            "Epoch 385: train loss: 0.16593097150325775\n",
            "Epoch 386: train loss: 0.1658904254436493\n",
            "Epoch 387: train loss: 0.16585102677345276\n",
            "Epoch 388: train loss: 0.16581134498119354\n",
            "Epoch 389: train loss: 0.16577088832855225\n",
            "Epoch 390: train loss: 0.16573165357112885\n",
            "Epoch 391: train loss: 0.16569198668003082\n",
            "Epoch 392: train loss: 0.16565153002738953\n",
            "Epoch 393: train loss: 0.16561244428157806\n",
            "Epoch 394: train loss: 0.16557283699512482\n",
            "Epoch 395: train loss: 0.16553246974945068\n",
            "Epoch 396: train loss: 0.16549354791641235\n",
            "Epoch 397: train loss: 0.16545386612415314\n",
            "Epoch 398: train loss: 0.16541366279125214\n",
            "Epoch 399: train loss: 0.16537483036518097\n",
            "Epoch 400: train loss: 0.16533523797988892\n",
            "Epoch 401: train loss: 0.1652950644493103\n",
            "Epoch 402: train loss: 0.16525641083717346\n",
            "Epoch 403: train loss: 0.16521677374839783\n",
            "Epoch 404: train loss: 0.16517668962478638\n",
            "Epoch 405: train loss: 0.1651381552219391\n",
            "Epoch 406: train loss: 0.16509857773780823\n",
            "Epoch 407: train loss: 0.16505855321884155\n",
            "Epoch 408: train loss: 0.1650201678276062\n",
            "Epoch 409: train loss: 0.16498059034347534\n",
            "Epoch 410: train loss: 0.16494064033031464\n",
            "Epoch 411: train loss: 0.16490240395069122\n",
            "Epoch 412: train loss: 0.16486282646656036\n",
            "Epoch 413: train loss: 0.16482298076152802\n",
            "Epoch 414: train loss: 0.16478484869003296\n",
            "Epoch 415: train loss: 0.16474531590938568\n",
            "Epoch 416: train loss: 0.16470550000667572\n",
            "Epoch 417: train loss: 0.1646675169467926\n",
            "Epoch 418: train loss: 0.16462798416614532\n",
            "Epoch 419: train loss: 0.1645883321762085\n",
            "Epoch 420: train loss: 0.16455048322677612\n",
            "Epoch 421: train loss: 0.16451077163219452\n",
            "Epoch 422: train loss: 0.1644716113805771\n",
            "Epoch 423: train loss: 0.16443343460559845\n",
            "Epoch 424: train loss: 0.16439391672611237\n",
            "Epoch 425: train loss: 0.16435493528842926\n",
            "Epoch 426: train loss: 0.16431672871112823\n",
            "Epoch 427: train loss: 0.16427725553512573\n",
            "Epoch 428: train loss: 0.16423846781253815\n",
            "Epoch 429: train loss: 0.1642003059387207\n",
            "Epoch 430: train loss: 0.1641608625650406\n",
            "Epoch 431: train loss: 0.16412222385406494\n",
            "Epoch 432: train loss: 0.1640840470790863\n",
            "Epoch 433: train loss: 0.16404467821121216\n",
            "Epoch 434: train loss: 0.16400623321533203\n",
            "Epoch 435: train loss: 0.1639680117368698\n",
            "Epoch 436: train loss: 0.16392876207828522\n",
            "Epoch 437: train loss: 0.16389040648937225\n",
            "Epoch 438: train loss: 0.163852259516716\n",
            "Epoch 439: train loss: 0.163813054561615\n",
            "Epoch 440: train loss: 0.16377483308315277\n",
            "Epoch 441: train loss: 0.16373664140701294\n",
            "Epoch 442: train loss: 0.16369757056236267\n",
            "Epoch 443: train loss: 0.16365951299667358\n",
            "Epoch 444: train loss: 0.16362136602401733\n",
            "Epoch 445: train loss: 0.16358231008052826\n",
            "Epoch 446: train loss: 0.16354437172412872\n",
            "Epoch 447: train loss: 0.16350623965263367\n",
            "Epoch 448: train loss: 0.16346725821495056\n",
            "Epoch 449: train loss: 0.16342948377132416\n",
            "Epoch 450: train loss: 0.1633913666009903\n",
            "Epoch 451: train loss: 0.16335242986679077\n",
            "Epoch 452: train loss: 0.16331477463245392\n",
            "Epoch 453: train loss: 0.16327665746212006\n",
            "Epoch 454: train loss: 0.16323786973953247\n",
            "Epoch 455: train loss: 0.16320031881332397\n",
            "Epoch 456: train loss: 0.16316227614879608\n",
            "Epoch 457: train loss: 0.16312351822853088\n",
            "Epoch 458: train loss: 0.16308611631393433\n",
            "Epoch 459: train loss: 0.16304799914360046\n",
            "Epoch 460: train loss: 0.16300936043262482\n",
            "Epoch 461: train loss: 0.162972092628479\n",
            "Epoch 462: train loss: 0.16293403506278992\n",
            "Epoch 463: train loss: 0.16289547085762024\n",
            "Epoch 464: train loss: 0.16285833716392517\n",
            "Epoch 465: train loss: 0.16282020509243011\n",
            "Epoch 466: train loss: 0.1627817004919052\n",
            "Epoch 467: train loss: 0.16274471580982208\n",
            "Epoch 468: train loss: 0.162706658244133\n",
            "Epoch 469: train loss: 0.16266818344593048\n",
            "Epoch 470: train loss: 0.1626313328742981\n",
            "Epoch 471: train loss: 0.1625933051109314\n",
            "Epoch 472: train loss: 0.16255493462085724\n",
            "Epoch 473: train loss: 0.1625182032585144\n",
            "Epoch 474: train loss: 0.16248014569282532\n",
            "Epoch 475: train loss: 0.1624419093132019\n",
            "Epoch 476: train loss: 0.16240529716014862\n",
            "Epoch 477: train loss: 0.16236728429794312\n",
            "Epoch 478: train loss: 0.16232901811599731\n",
            "Epoch 479: train loss: 0.16229255497455597\n",
            "Epoch 480: train loss: 0.16225455701351166\n",
            "Epoch 481: train loss: 0.1622164100408554\n",
            "Epoch 482: train loss: 0.1621800810098648\n",
            "Epoch 483: train loss: 0.16214212775230408\n",
            "Epoch 484: train loss: 0.16210408508777618\n",
            "Epoch 485: train loss: 0.16206781566143036\n",
            "Epoch 486: train loss: 0.1620296835899353\n",
            "Epoch 487: train loss: 0.1619921326637268\n",
            "Epoch 488: train loss: 0.16195562481880188\n",
            "Epoch 489: train loss: 0.16191764175891876\n",
            "Epoch 490: train loss: 0.1618802845478058\n",
            "Epoch 491: train loss: 0.1618436574935913\n",
            "Epoch 492: train loss: 0.16180576384067535\n",
            "Epoch 493: train loss: 0.16176863014698029\n",
            "Epoch 494: train loss: 0.16173198819160461\n",
            "Epoch 495: train loss: 0.16169416904449463\n",
            "Epoch 496: train loss: 0.16165710985660553\n",
            "Epoch 497: train loss: 0.16162046790122986\n",
            "Epoch 498: train loss: 0.1615827977657318\n",
            "Epoch 499: train loss: 0.16154582798480988\n",
            "Epoch 500: train loss: 0.1615092158317566\n",
            "Epoch 501: train loss: 0.16147156059741974\n",
            "Epoch 502: train loss: 0.16143478453159332\n",
            "Epoch 503: train loss: 0.16139815747737885\n",
            "Epoch 504: train loss: 0.16136056184768677\n",
            "Epoch 505: train loss: 0.1613239049911499\n",
            "Epoch 506: train loss: 0.161287322640419\n",
            "Epoch 507: train loss: 0.1612498164176941\n",
            "Epoch 508: train loss: 0.16121327877044678\n",
            "Epoch 509: train loss: 0.1611766815185547\n",
            "Epoch 510: train loss: 0.16113923490047455\n",
            "Epoch 511: train loss: 0.16110281646251678\n",
            "Epoch 512: train loss: 0.16106624901294708\n",
            "Epoch 513: train loss: 0.1610288769006729\n",
            "Epoch 514: train loss: 0.16099262237548828\n",
            "Epoch 515: train loss: 0.16095604002475739\n",
            "Epoch 516: train loss: 0.16091875731945038\n",
            "Epoch 517: train loss: 0.1608826071023941\n",
            "Epoch 518: train loss: 0.16084609925746918\n",
            "Epoch 519: train loss: 0.16080883145332336\n",
            "Epoch 520: train loss: 0.16077281534671783\n",
            "Epoch 521: train loss: 0.1607362926006317\n",
            "Epoch 522: train loss: 0.16069908440113068\n",
            "Epoch 523: train loss: 0.1606631875038147\n",
            "Epoch 524: train loss: 0.16062670946121216\n",
            "Epoch 525: train loss: 0.16058960556983948\n",
            "Epoch 526: train loss: 0.16055384278297424\n",
            "Epoch 527: train loss: 0.16051730513572693\n",
            "Epoch 528: train loss: 0.1604803055524826\n",
            "Epoch 529: train loss: 0.16044461727142334\n",
            "Epoch 530: train loss: 0.1604081690311432\n",
            "Epoch 531: train loss: 0.16037116944789886\n",
            "Epoch 532: train loss: 0.16033567488193512\n",
            "Epoch 533: train loss: 0.16029922664165497\n",
            "Epoch 534: train loss: 0.16026225686073303\n",
            "Epoch 535: train loss: 0.16022688150405884\n",
            "Epoch 536: train loss: 0.16019049286842346\n",
            "Epoch 537: train loss: 0.16015364229679108\n",
            "Epoch 538: train loss: 0.16011835634708405\n",
            "Epoch 539: train loss: 0.16008195281028748\n",
            "Epoch 540: train loss: 0.16004520654678345\n",
            "Epoch 541: train loss: 0.1600099802017212\n",
            "Epoch 542: train loss: 0.1599736511707306\n",
            "Epoch 543: train loss: 0.15993696451187134\n",
            "Epoch 544: train loss: 0.15990179777145386\n",
            "Epoch 545: train loss: 0.15986549854278564\n",
            "Epoch 546: train loss: 0.15982888638973236\n",
            "Epoch 547: train loss: 0.15979385375976562\n",
            "Epoch 548: train loss: 0.1597575545310974\n",
            "Epoch 549: train loss: 0.1597210168838501\n",
            "Epoch 550: train loss: 0.1596861332654953\n",
            "Epoch 551: train loss: 0.15964986383914948\n",
            "Epoch 552: train loss: 0.15961340069770813\n",
            "Epoch 553: train loss: 0.1595785766839981\n",
            "Epoch 554: train loss: 0.1595422774553299\n",
            "Epoch 555: train loss: 0.1595059037208557\n",
            "Epoch 556: train loss: 0.15947124361991882\n",
            "Epoch 557: train loss: 0.15943501889705658\n",
            "Epoch 558: train loss: 0.15939867496490479\n",
            "Epoch 559: train loss: 0.15936410427093506\n",
            "Epoch 560: train loss: 0.15932773053646088\n",
            "Epoch 561: train loss: 0.15929186344146729\n",
            "Epoch 562: train loss: 0.15925706923007965\n",
            "Epoch 563: train loss: 0.15922079980373383\n",
            "Epoch 564: train loss: 0.15918512642383575\n",
            "Epoch 565: train loss: 0.15915025770664215\n",
            "Epoch 566: train loss: 0.15911410748958588\n",
            "Epoch 567: train loss: 0.15907858312129974\n",
            "Epoch 568: train loss: 0.15904366970062256\n",
            "Epoch 569: train loss: 0.15900754928588867\n",
            "Epoch 570: train loss: 0.15897217392921448\n",
            "Epoch 571: train loss: 0.1589372605085373\n",
            "Epoch 572: train loss: 0.15890124440193176\n",
            "Epoch 573: train loss: 0.15886594355106354\n",
            "Epoch 574: train loss: 0.15883107483386993\n",
            "Epoch 575: train loss: 0.15879510343074799\n",
            "Epoch 576: train loss: 0.15875999629497528\n",
            "Epoch 577: train loss: 0.15872512757778168\n",
            "Epoch 578: train loss: 0.1586892008781433\n",
            "Epoch 579: train loss: 0.158654123544693\n",
            "Epoch 580: train loss: 0.15861935913562775\n",
            "Epoch 581: train loss: 0.15858349204063416\n",
            "Epoch 582: train loss: 0.15854860842227936\n",
            "Epoch 583: train loss: 0.15851373970508575\n",
            "Epoch 584: train loss: 0.15847797691822052\n",
            "Epoch 585: train loss: 0.1584431529045105\n",
            "Epoch 586: train loss: 0.15840831398963928\n",
            "Epoch 587: train loss: 0.1583726704120636\n",
            "Epoch 588: train loss: 0.15833796560764313\n",
            "Epoch 589: train loss: 0.1583031862974167\n",
            "Epoch 590: train loss: 0.158267542719841\n",
            "Epoch 591: train loss: 0.1582329124212265\n",
            "Epoch 592: train loss: 0.15819819271564484\n",
            "Epoch 593: train loss: 0.15816260874271393\n",
            "Epoch 594: train loss: 0.15812811255455017\n",
            "Epoch 595: train loss: 0.15809336304664612\n",
            "Epoch 596: train loss: 0.15805786848068237\n",
            "Epoch 597: train loss: 0.15802347660064697\n",
            "Epoch 598: train loss: 0.1579887866973877\n",
            "Epoch 599: train loss: 0.15795335173606873\n",
            "Epoch 600: train loss: 0.1579190492630005\n",
            "Epoch 601: train loss: 0.15788443386554718\n",
            "Epoch 602: train loss: 0.1578490287065506\n",
            "Epoch 603: train loss: 0.15781483054161072\n",
            "Epoch 604: train loss: 0.15778015553951263\n",
            "Epoch 605: train loss: 0.1577448844909668\n",
            "Epoch 606: train loss: 0.1577107459306717\n",
            "Epoch 607: train loss: 0.15767617523670197\n",
            "Epoch 608: train loss: 0.15764091908931732\n",
            "Epoch 609: train loss: 0.15760689973831177\n",
            "Epoch 610: train loss: 0.15757235884666443\n",
            "Epoch 611: train loss: 0.15753713250160217\n",
            "Epoch 612: train loss: 0.15750324726104736\n",
            "Epoch 613: train loss: 0.1574687361717224\n",
            "Epoch 614: train loss: 0.15743359923362732\n",
            "Epoch 615: train loss: 0.15739978849887848\n",
            "Epoch 616: train loss: 0.15736527740955353\n",
            "Epoch 617: train loss: 0.1573302447795868\n",
            "Epoch 618: train loss: 0.15729652345180511\n",
            "Epoch 619: train loss: 0.15726204216480255\n",
            "Epoch 620: train loss: 0.1572270542383194\n",
            "Epoch 621: train loss: 0.15719346702098846\n",
            "Epoch 622: train loss: 0.1571589708328247\n",
            "Epoch 623: train loss: 0.15712407231330872\n",
            "Epoch 624: train loss: 0.15709054470062256\n",
            "Epoch 625: train loss: 0.15705612301826477\n",
            "Epoch 626: train loss: 0.15702126920223236\n",
            "Epoch 627: train loss: 0.15698781609535217\n",
            "Epoch 628: train loss: 0.15695343911647797\n",
            "Epoch 629: train loss: 0.1569187045097351\n",
            "Epoch 630: train loss: 0.1568852961063385\n",
            "Epoch 631: train loss: 0.15685099363327026\n",
            "Epoch 632: train loss: 0.1568162441253662\n",
            "Epoch 633: train loss: 0.15678304433822632\n",
            "Epoch 634: train loss: 0.1567486822605133\n",
            "Epoch 635: train loss: 0.15671397745609283\n",
            "Epoch 636: train loss: 0.1566808670759201\n",
            "Epoch 637: train loss: 0.15664659440517426\n",
            "Epoch 638: train loss: 0.15661197900772095\n",
            "Epoch 639: train loss: 0.1565789431333542\n",
            "Epoch 640: train loss: 0.15654465556144714\n",
            "Epoch 641: train loss: 0.15651017427444458\n",
            "Epoch 642: train loss: 0.156477153301239\n",
            "Epoch 643: train loss: 0.15644297003746033\n",
            "Epoch 644: train loss: 0.15640848875045776\n",
            "Epoch 645: train loss: 0.15637560188770294\n",
            "Epoch 646: train loss: 0.15634137392044067\n",
            "Epoch 647: train loss: 0.15630702674388885\n",
            "Epoch 648: train loss: 0.1562742441892624\n",
            "Epoch 649: train loss: 0.15624010562896729\n",
            "Epoch 650: train loss: 0.15620577335357666\n",
            "Epoch 651: train loss: 0.15617302060127258\n",
            "Epoch 652: train loss: 0.15613895654678345\n",
            "Epoch 653: train loss: 0.15610463917255402\n",
            "Epoch 654: train loss: 0.15607206523418427\n",
            "Epoch 655: train loss: 0.15603798627853394\n",
            "Epoch 656: train loss: 0.15600377321243286\n",
            "Epoch 657: train loss: 0.15597118437290192\n",
            "Epoch 658: train loss: 0.15593720972537994\n",
            "Epoch 659: train loss: 0.15590304136276245\n",
            "Epoch 660: train loss: 0.15587058663368225\n",
            "Epoch 661: train loss: 0.1558365523815155\n",
            "Epoch 662: train loss: 0.15580247342586517\n",
            "Epoch 663: train loss: 0.15577013790607452\n",
            "Epoch 664: train loss: 0.1557360291481018\n",
            "Epoch 665: train loss: 0.15570230782032013\n",
            "Epoch 666: train loss: 0.15566976368427277\n",
            "Epoch 667: train loss: 0.15563572943210602\n",
            "Epoch 668: train loss: 0.15560226142406464\n",
            "Epoch 669: train loss: 0.15556956827640533\n",
            "Epoch 670: train loss: 0.15553566813468933\n",
            "Epoch 671: train loss: 0.15550225973129272\n",
            "Epoch 672: train loss: 0.15546968579292297\n",
            "Epoch 673: train loss: 0.15543575584888458\n",
            "Epoch 674: train loss: 0.15540246665477753\n",
            "Epoch 675: train loss: 0.15536987781524658\n",
            "Epoch 676: train loss: 0.15533608198165894\n",
            "Epoch 677: train loss: 0.15530291199684143\n",
            "Epoch 678: train loss: 0.15527035295963287\n",
            "Epoch 679: train loss: 0.1552366018295288\n",
            "Epoch 680: train loss: 0.15520352125167847\n",
            "Epoch 681: train loss: 0.15517103672027588\n",
            "Epoch 682: train loss: 0.15513725578784943\n",
            "Epoch 683: train loss: 0.15510430932044983\n",
            "Epoch 684: train loss: 0.15507178008556366\n",
            "Epoch 685: train loss: 0.15503814816474915\n",
            "Epoch 686: train loss: 0.15500524640083313\n",
            "Epoch 687: train loss: 0.15497274696826935\n",
            "Epoch 688: train loss: 0.1549391895532608\n",
            "Epoch 689: train loss: 0.1549062877893448\n",
            "Epoch 690: train loss: 0.15487390756607056\n",
            "Epoch 691: train loss: 0.154840350151062\n",
            "Epoch 692: train loss: 0.15480758249759674\n",
            "Epoch 693: train loss: 0.15477518737316132\n",
            "Epoch 694: train loss: 0.1547417789697647\n",
            "Epoch 695: train loss: 0.1547091156244278\n",
            "Epoch 696: train loss: 0.15467675030231476\n",
            "Epoch 697: train loss: 0.15464332699775696\n",
            "Epoch 698: train loss: 0.1546107828617096\n",
            "Epoch 699: train loss: 0.15457843244075775\n",
            "Epoch 700: train loss: 0.1545451134443283\n",
            "Epoch 701: train loss: 0.15451262891292572\n",
            "Epoch 702: train loss: 0.15448035299777985\n",
            "Epoch 703: train loss: 0.1544470340013504\n",
            "Epoch 704: train loss: 0.15441463887691498\n",
            "Epoch 705: train loss: 0.1543823778629303\n",
            "Epoch 706: train loss: 0.1543491780757904\n",
            "Epoch 707: train loss: 0.15431685745716095\n",
            "Epoch 708: train loss: 0.15428459644317627\n",
            "Epoch 709: train loss: 0.15425147116184235\n",
            "Epoch 710: train loss: 0.15421916544437408\n",
            "Epoch 711: train loss: 0.15418699383735657\n",
            "Epoch 712: train loss: 0.15415392816066742\n",
            "Epoch 713: train loss: 0.1541217416524887\n",
            "Epoch 714: train loss: 0.15408962965011597\n",
            "Epoch 715: train loss: 0.1540565937757492\n",
            "Epoch 716: train loss: 0.15402445197105408\n",
            "Epoch 717: train loss: 0.1539923995733261\n",
            "Epoch 718: train loss: 0.15395943820476532\n",
            "Epoch 719: train loss: 0.15392737090587616\n",
            "Epoch 720: train loss: 0.1538953334093094\n",
            "Epoch 721: train loss: 0.15386243164539337\n",
            "Epoch 722: train loss: 0.1538303941488266\n",
            "Epoch 723: train loss: 0.15379846096038818\n",
            "Epoch 724: train loss: 0.15376561880111694\n",
            "Epoch 725: train loss: 0.15373368561267853\n",
            "Epoch 726: train loss: 0.1537017673254013\n",
            "Epoch 727: train loss: 0.15366895496845245\n",
            "Epoch 728: train loss: 0.1536371260881424\n",
            "Epoch 729: train loss: 0.15360519289970398\n",
            "Epoch 730: train loss: 0.1535724699497223\n",
            "Epoch 731: train loss: 0.15354076027870178\n",
            "Epoch 732: train loss: 0.15350887179374695\n",
            "Epoch 733: train loss: 0.15347617864608765\n",
            "Epoch 734: train loss: 0.15344446897506714\n",
            "Epoch 735: train loss: 0.15341264009475708\n",
            "Epoch 736: train loss: 0.15338000655174255\n",
            "Epoch 737: train loss: 0.1533484309911728\n",
            "Epoch 738: train loss: 0.1533166468143463\n",
            "Epoch 739: train loss: 0.15328408777713776\n",
            "Epoch 740: train loss: 0.1532524824142456\n",
            "Epoch 741: train loss: 0.15322080254554749\n",
            "Epoch 742: train loss: 0.1531883031129837\n",
            "Epoch 743: train loss: 0.1531568318605423\n",
            "Epoch 744: train loss: 0.15312518179416656\n",
            "Epoch 745: train loss: 0.15309269726276398\n",
            "Epoch 746: train loss: 0.15306124091148376\n",
            "Epoch 747: train loss: 0.1530296355485916\n",
            "Epoch 748: train loss: 0.15299728512763977\n",
            "Epoch 749: train loss: 0.15296593308448792\n",
            "Epoch 750: train loss: 0.15293435752391815\n",
            "Epoch 751: train loss: 0.15290199220180511\n",
            "Epoch 752: train loss: 0.15287074446678162\n",
            "Epoch 753: train loss: 0.15283919870853424\n",
            "Epoch 754: train loss: 0.15280695259571075\n",
            "Epoch 755: train loss: 0.15277574956417084\n",
            "Epoch 756: train loss: 0.15274423360824585\n",
            "Epoch 757: train loss: 0.15271206200122833\n",
            "Epoch 758: train loss: 0.15268085896968842\n",
            "Epoch 759: train loss: 0.152649387717247\n",
            "Epoch 760: train loss: 0.15261726081371307\n",
            "Epoch 761: train loss: 0.15258613228797913\n",
            "Epoch 762: train loss: 0.15255479514598846\n",
            "Epoch 763: train loss: 0.15252268314361572\n",
            "Epoch 764: train loss: 0.15249162912368774\n",
            "Epoch 765: train loss: 0.15246036648750305\n",
            "Epoch 766: train loss: 0.1524282842874527\n",
            "Epoch 767: train loss: 0.1523973047733307\n",
            "Epoch 768: train loss: 0.152366042137146\n",
            "Epoch 769: train loss: 0.152334064245224\n",
            "Epoch 770: train loss: 0.15230309963226318\n",
            "Epoch 771: train loss: 0.15227192640304565\n",
            "Epoch 772: train loss: 0.15223997831344604\n",
            "Epoch 773: train loss: 0.15220914781093597\n",
            "Epoch 774: train loss: 0.15217795968055725\n",
            "Epoch 775: train loss: 0.152146115899086\n",
            "Epoch 776: train loss: 0.15211528539657593\n",
            "Epoch 777: train loss: 0.15208415687084198\n",
            "Epoch 778: train loss: 0.1520523726940155\n",
            "Epoch 779: train loss: 0.1520216166973114\n",
            "Epoch 780: train loss: 0.15199056267738342\n",
            "Epoch 781: train loss: 0.15195879340171814\n",
            "Epoch 782: train loss: 0.15192806720733643\n",
            "Epoch 783: train loss: 0.151897132396698\n",
            "Epoch 784: train loss: 0.1518653929233551\n",
            "Epoch 785: train loss: 0.15183474123477936\n",
            "Epoch 786: train loss: 0.15180377662181854\n",
            "Epoch 787: train loss: 0.1517721563577652\n",
            "Epoch 788: train loss: 0.15174156427383423\n",
            "Epoch 789: train loss: 0.1517106294631958\n",
            "Epoch 790: train loss: 0.15167909860610962\n",
            "Epoch 791: train loss: 0.15164856612682343\n",
            "Epoch 792: train loss: 0.15161772072315216\n",
            "Epoch 793: train loss: 0.15158618986606598\n",
            "Epoch 794: train loss: 0.15155570209026337\n",
            "Epoch 795: train loss: 0.15152490139007568\n",
            "Epoch 796: train loss: 0.15149345993995667\n",
            "Epoch 797: train loss: 0.15146301686763763\n",
            "Epoch 798: train loss: 0.15143229067325592\n",
            "Epoch 799: train loss: 0.1514008641242981\n",
            "Epoch 800: train loss: 0.15137048065662384\n",
            "Epoch 801: train loss: 0.1513398289680481\n",
            "Epoch 802: train loss: 0.15130846202373505\n",
            "Epoch 803: train loss: 0.15127816796302795\n",
            "Epoch 804: train loss: 0.1512475609779358\n",
            "Epoch 805: train loss: 0.15121620893478394\n",
            "Epoch 806: train loss: 0.15118592977523804\n",
            "Epoch 807: train loss: 0.15115541219711304\n",
            "Epoch 808: train loss: 0.15112414956092834\n",
            "Epoch 809: train loss: 0.15109390020370483\n",
            "Epoch 810: train loss: 0.15106342732906342\n",
            "Epoch 811: train loss: 0.1510322242975235\n",
            "Epoch 812: train loss: 0.15100204944610596\n",
            "Epoch 813: train loss: 0.15097162127494812\n",
            "Epoch 814: train loss: 0.15094047784805298\n",
            "Epoch 815: train loss: 0.15091031789779663\n",
            "Epoch 816: train loss: 0.15087999403476715\n",
            "Epoch 817: train loss: 0.1508488804101944\n",
            "Epoch 818: train loss: 0.15081875026226044\n",
            "Epoch 819: train loss: 0.15078850090503693\n",
            "Epoch 820: train loss: 0.15075747668743134\n",
            "Epoch 821: train loss: 0.15072736144065857\n",
            "Epoch 822: train loss: 0.15069720149040222\n",
            "Epoch 823: train loss: 0.15066617727279663\n",
            "Epoch 824: train loss: 0.15063618123531342\n",
            "Epoch 825: train loss: 0.15060602128505707\n",
            "Epoch 826: train loss: 0.15057511627674103\n",
            "Epoch 827: train loss: 0.15054510533809662\n",
            "Epoch 828: train loss: 0.15051501989364624\n",
            "Epoch 829: train loss: 0.15048417448997498\n",
            "Epoch 830: train loss: 0.15045423805713654\n",
            "Epoch 831: train loss: 0.15042419731616974\n",
            "Epoch 832: train loss: 0.15039338171482086\n",
            "Epoch 833: train loss: 0.15036346018314362\n",
            "Epoch 834: train loss: 0.1503334939479828\n",
            "Epoch 835: train loss: 0.15030275285243988\n",
            "Epoch 836: train loss: 0.1502729058265686\n",
            "Epoch 837: train loss: 0.15024296939373016\n",
            "Epoch 838: train loss: 0.15021225810050964\n",
            "Epoch 839: train loss: 0.15018242597579956\n",
            "Epoch 840: train loss: 0.15015263855457306\n",
            "Epoch 841: train loss: 0.15012194216251373\n",
            "Epoch 842: train loss: 0.150092214345932\n",
            "Epoch 843: train loss: 0.1500624269247055\n",
            "Epoch 844: train loss: 0.15003181993961334\n",
            "Epoch 845: train loss: 0.15000207722187042\n",
            "Epoch 846: train loss: 0.1499723345041275\n",
            "Epoch 847: train loss: 0.1499418318271637\n",
            "Epoch 848: train loss: 0.14991214871406555\n",
            "Epoch 849: train loss: 0.1498824954032898\n",
            "Epoch 850: train loss: 0.1498519778251648\n",
            "Epoch 851: train loss: 0.14982235431671143\n",
            "Epoch 852: train loss: 0.14979277551174164\n",
            "Epoch 853: train loss: 0.1497623175382614\n",
            "Epoch 854: train loss: 0.14973275363445282\n",
            "Epoch 855: train loss: 0.1497032344341278\n",
            "Epoch 856: train loss: 0.14967283606529236\n",
            "Epoch 857: train loss: 0.14964327216148376\n",
            "Epoch 858: train loss: 0.14961384236812592\n",
            "Epoch 859: train loss: 0.14958345890045166\n",
            "Epoch 860: train loss: 0.14955393970012665\n",
            "Epoch 861: train loss: 0.14952456951141357\n",
            "Epoch 862: train loss: 0.1494942456483841\n",
            "Epoch 863: train loss: 0.14946478605270386\n",
            "Epoch 864: train loss: 0.14943550527095795\n",
            "Epoch 865: train loss: 0.14940524101257324\n",
            "Epoch 866: train loss: 0.14937575161457062\n",
            "Epoch 867: train loss: 0.1493464708328247\n",
            "Epoch 868: train loss: 0.14931637048721313\n",
            "Epoch 869: train loss: 0.1492869257926941\n",
            "Epoch 870: train loss: 0.14925771951675415\n",
            "Epoch 871: train loss: 0.14922760426998138\n",
            "Epoch 872: train loss: 0.1491982340812683\n",
            "Epoch 873: train loss: 0.14916910231113434\n",
            "Epoch 874: train loss: 0.14913900196552277\n",
            "Epoch 875: train loss: 0.14910966157913208\n",
            "Epoch 876: train loss: 0.14908063411712646\n",
            "Epoch 877: train loss: 0.14905057847499847\n",
            "Epoch 878: train loss: 0.14902125298976898\n",
            "Epoch 879: train loss: 0.14899228513240814\n",
            "Epoch 880: train loss: 0.14896231889724731\n",
            "Epoch 881: train loss: 0.1489330381155014\n",
            "Epoch 882: train loss: 0.14890414476394653\n",
            "Epoch 883: train loss: 0.1488742083311081\n",
            "Epoch 884: train loss: 0.14884497225284576\n",
            "Epoch 885: train loss: 0.1488160938024521\n",
            "Epoch 886: train loss: 0.1487862467765808\n",
            "Epoch 887: train loss: 0.1487569808959961\n",
            "Epoch 888: train loss: 0.14872822165489197\n",
            "Epoch 889: train loss: 0.14869838953018188\n",
            "Epoch 890: train loss: 0.14866922795772552\n",
            "Epoch 891: train loss: 0.1486404985189438\n",
            "Epoch 892: train loss: 0.14861074090003967\n",
            "Epoch 893: train loss: 0.14858156442642212\n",
            "Epoch 894: train loss: 0.14855295419692993\n",
            "Epoch 895: train loss: 0.1485232263803482\n",
            "Epoch 896: train loss: 0.14849409461021423\n",
            "Epoch 897: train loss: 0.14846555888652802\n",
            "Epoch 898: train loss: 0.14843589067459106\n",
            "Epoch 899: train loss: 0.14840680360794067\n",
            "Epoch 900: train loss: 0.14837828278541565\n",
            "Epoch 901: train loss: 0.14834873378276825\n",
            "Epoch 902: train loss: 0.14831960201263428\n",
            "Epoch 903: train loss: 0.1482912003993988\n",
            "Epoch 904: train loss: 0.1482616662979126\n",
            "Epoch 905: train loss: 0.1482325792312622\n",
            "Epoch 906: train loss: 0.1482042372226715\n",
            "Epoch 907: train loss: 0.14817474782466888\n",
            "Epoch 908: train loss: 0.14814573526382446\n",
            "Epoch 909: train loss: 0.14811748266220093\n",
            "Epoch 910: train loss: 0.14808796346187592\n",
            "Epoch 911: train loss: 0.14805902540683746\n",
            "Epoch 912: train loss: 0.14803080260753632\n",
            "Epoch 913: train loss: 0.14800137281417847\n",
            "Epoch 914: train loss: 0.14797241985797882\n",
            "Epoch 915: train loss: 0.14794430136680603\n",
            "Epoch 916: train loss: 0.14791494607925415\n",
            "Epoch 917: train loss: 0.1478859931230545\n",
            "Epoch 918: train loss: 0.1478579342365265\n",
            "Epoch 919: train loss: 0.14782863855361938\n",
            "Epoch 920: train loss: 0.14779974520206451\n",
            "Epoch 921: train loss: 0.14777176082134247\n",
            "Epoch 922: train loss: 0.14774250984191895\n",
            "Epoch 923: train loss: 0.14771360158920288\n",
            "Epoch 924: train loss: 0.147685706615448\n",
            "Epoch 925: train loss: 0.14765645563602448\n",
            "Epoch 926: train loss: 0.14762762188911438\n",
            "Epoch 927: train loss: 0.14759981632232666\n",
            "Epoch 928: train loss: 0.1475706547498703\n",
            "Epoch 929: train loss: 0.14754177629947662\n",
            "Epoch 930: train loss: 0.14751404523849487\n",
            "Epoch 931: train loss: 0.1474849432706833\n",
            "Epoch 932: train loss: 0.1474560648202896\n",
            "Epoch 933: train loss: 0.14742842316627502\n",
            "Epoch 934: train loss: 0.14739938080310822\n",
            "Epoch 935: train loss: 0.1473705768585205\n",
            "Epoch 936: train loss: 0.1473429799079895\n",
            "Epoch 937: train loss: 0.1473139524459839\n",
            "Epoch 938: train loss: 0.14728516340255737\n",
            "Epoch 939: train loss: 0.14725765585899353\n",
            "Epoch 940: train loss: 0.14722871780395508\n",
            "Epoch 941: train loss: 0.14719992876052856\n",
            "Epoch 942: train loss: 0.1471724808216095\n",
            "Epoch 943: train loss: 0.14714358747005463\n",
            "Epoch 944: train loss: 0.1471148133277893\n",
            "Epoch 945: train loss: 0.14708749949932098\n",
            "Epoch 946: train loss: 0.14705859124660492\n",
            "Epoch 947: train loss: 0.14702989161014557\n",
            "Epoch 948: train loss: 0.14700260758399963\n",
            "Epoch 949: train loss: 0.14697375893592834\n",
            "Epoch 950: train loss: 0.14694508910179138\n",
            "Epoch 951: train loss: 0.14691777527332306\n",
            "Epoch 952: train loss: 0.14688919484615326\n",
            "Epoch 953: train loss: 0.14686046540737152\n",
            "Epoch 954: train loss: 0.14683298766613007\n",
            "Epoch 955: train loss: 0.14680472016334534\n",
            "Epoch 956: train loss: 0.14677603542804718\n",
            "Epoch 957: train loss: 0.14674846827983856\n",
            "Epoch 958: train loss: 0.14672036468982697\n",
            "Epoch 959: train loss: 0.1466917097568512\n",
            "Epoch 960: train loss: 0.14666414260864258\n",
            "Epoch 961: train loss: 0.14663606882095337\n",
            "Epoch 962: train loss: 0.14660748839378357\n",
            "Epoch 963: train loss: 0.14657996594905853\n",
            "Epoch 964: train loss: 0.1465519815683365\n",
            "Epoch 965: train loss: 0.14652343094348907\n",
            "Epoch 966: train loss: 0.14649586379528046\n",
            "Epoch 967: train loss: 0.14646805822849274\n",
            "Epoch 968: train loss: 0.1464395374059677\n",
            "Epoch 969: train loss: 0.14641201496124268\n",
            "Epoch 970: train loss: 0.14638425409793854\n",
            "Epoch 971: train loss: 0.1463557779788971\n",
            "Epoch 972: train loss: 0.14632825553417206\n",
            "Epoch 973: train loss: 0.1463005691766739\n",
            "Epoch 974: train loss: 0.1462721973657608\n",
            "Epoch 975: train loss: 0.14624464511871338\n",
            "Epoch 976: train loss: 0.14621706306934357\n",
            "Epoch 977: train loss: 0.14618870615959167\n",
            "Epoch 978: train loss: 0.14616116881370544\n",
            "Epoch 979: train loss: 0.14613370597362518\n",
            "Epoch 980: train loss: 0.14610537886619568\n",
            "Epoch 981: train loss: 0.14607782661914825\n",
            "Epoch 982: train loss: 0.14605043828487396\n",
            "Epoch 983: train loss: 0.14602220058441162\n",
            "Epoch 984: train loss: 0.1459946483373642\n",
            "Epoch 985: train loss: 0.14596733450889587\n",
            "Epoch 986: train loss: 0.14593912661075592\n",
            "Epoch 987: train loss: 0.14591160416603088\n",
            "Epoch 988: train loss: 0.1458844095468521\n",
            "Epoch 989: train loss: 0.14585623145103455\n",
            "Epoch 990: train loss: 0.1458287090063095\n",
            "Epoch 991: train loss: 0.1458016186952591\n",
            "Epoch 992: train loss: 0.14577347040176392\n",
            "Epoch 993: train loss: 0.14574593305587769\n",
            "Epoch 994: train loss: 0.14571890234947205\n",
            "Epoch 995: train loss: 0.14569085836410522\n",
            "Epoch 996: train loss: 0.1456632912158966\n",
            "Epoch 997: train loss: 0.1456363946199417\n",
            "Epoch 998: train loss: 0.1456083059310913\n",
            "Epoch 999: train loss: 0.14558082818984985\n",
            "Epoch 1000: train loss: 0.14555397629737854\n",
            "Epoch 1001: train loss: 0.1455259919166565\n",
            "Epoch 1002: train loss: 0.14549846947193146\n",
            "Epoch 1003: train loss: 0.1454716920852661\n",
            "Epoch 1004: train loss: 0.14544379711151123\n",
            "Epoch 1005: train loss: 0.1454162448644638\n",
            "Epoch 1006: train loss: 0.14538957178592682\n",
            "Epoch 1007: train loss: 0.14536169171333313\n",
            "Epoch 1008: train loss: 0.1453341543674469\n",
            "Epoch 1009: train loss: 0.14530760049819946\n",
            "Epoch 1010: train loss: 0.14527973532676697\n",
            "Epoch 1011: train loss: 0.14525221288204193\n",
            "Epoch 1012: train loss: 0.14522574841976166\n",
            "Epoch 1013: train loss: 0.14519795775413513\n",
            "Epoch 1014: train loss: 0.1451704055070877\n",
            "Epoch 1015: train loss: 0.1451440453529358\n",
            "Epoch 1016: train loss: 0.14511631429195404\n",
            "Epoch 1017: train loss: 0.14508871734142303\n",
            "Epoch 1018: train loss: 0.14506244659423828\n",
            "Epoch 1019: train loss: 0.14503473043441772\n",
            "Epoch 1020: train loss: 0.14500722289085388\n",
            "Epoch 1021: train loss: 0.1449810117483139\n",
            "Epoch 1022: train loss: 0.1449533998966217\n",
            "Epoch 1023: train loss: 0.14492584764957428\n",
            "Epoch 1024: train loss: 0.14489972591400146\n",
            "Epoch 1025: train loss: 0.14487212896347046\n",
            "Epoch 1026: train loss: 0.14484460651874542\n",
            "Epoch 1027: train loss: 0.14481844007968903\n",
            "Epoch 1028: train loss: 0.14479117095470428\n",
            "Epoch 1029: train loss: 0.14476363360881805\n",
            "Epoch 1030: train loss: 0.14473722875118256\n",
            "Epoch 1031: train loss: 0.1447102129459381\n",
            "Epoch 1032: train loss: 0.14468269050121307\n",
            "Epoch 1033: train loss: 0.144656240940094\n",
            "Epoch 1034: train loss: 0.14462940394878387\n",
            "Epoch 1035: train loss: 0.14460191130638123\n",
            "Epoch 1036: train loss: 0.14457547664642334\n",
            "Epoch 1037: train loss: 0.1445486694574356\n",
            "Epoch 1038: train loss: 0.14452128112316132\n",
            "Epoch 1039: train loss: 0.14449478685855865\n",
            "Epoch 1040: train loss: 0.14446806907653809\n",
            "Epoch 1041: train loss: 0.14444075524806976\n",
            "Epoch 1042: train loss: 0.14441420137882233\n",
            "Epoch 1043: train loss: 0.14438767731189728\n",
            "Epoch 1044: train loss: 0.14436034858226776\n",
            "Epoch 1045: train loss: 0.1443338841199875\n",
            "Epoch 1046: train loss: 0.14430734515190125\n",
            "Epoch 1047: train loss: 0.1442800909280777\n",
            "Epoch 1048: train loss: 0.14425355195999146\n",
            "Epoch 1049: train loss: 0.14422720670700073\n",
            "Epoch 1050: train loss: 0.14419995248317719\n",
            "Epoch 1051: train loss: 0.14417342841625214\n",
            "Epoch 1052: train loss: 0.14414718747138977\n",
            "Epoch 1053: train loss: 0.1441200226545334\n",
            "Epoch 1054: train loss: 0.14409343898296356\n",
            "Epoch 1055: train loss: 0.14406727254390717\n",
            "Epoch 1056: train loss: 0.14404018223285675\n",
            "Epoch 1057: train loss: 0.14401358366012573\n",
            "Epoch 1058: train loss: 0.1439875215291977\n",
            "Epoch 1059: train loss: 0.14396043121814728\n",
            "Epoch 1060: train loss: 0.14393386244773865\n",
            "Epoch 1061: train loss: 0.14390790462493896\n",
            "Epoch 1062: train loss: 0.14388082921504974\n",
            "Epoch 1063: train loss: 0.14385423064231873\n",
            "Epoch 1064: train loss: 0.1438283771276474\n",
            "Epoch 1065: train loss: 0.14380134642124176\n",
            "Epoch 1066: train loss: 0.14377476274967194\n",
            "Epoch 1067: train loss: 0.14374901354312897\n",
            "Epoch 1068: train loss: 0.14372208714485168\n",
            "Epoch 1069: train loss: 0.14369547367095947\n",
            "Epoch 1070: train loss: 0.14366984367370605\n",
            "Epoch 1071: train loss: 0.14364288747310638\n",
            "Epoch 1072: train loss: 0.14361628890037537\n",
            "Epoch 1073: train loss: 0.1435907483100891\n",
            "Epoch 1074: train loss: 0.1435638964176178\n",
            "Epoch 1075: train loss: 0.1435372233390808\n",
            "Epoch 1076: train loss: 0.14351174235343933\n",
            "Epoch 1077: train loss: 0.1434849500656128\n",
            "Epoch 1078: train loss: 0.1434582769870758\n",
            "Epoch 1079: train loss: 0.14343294501304626\n",
            "Epoch 1080: train loss: 0.14340613782405853\n",
            "Epoch 1081: train loss: 0.14337946474552155\n",
            "Epoch 1082: train loss: 0.14335419237613678\n",
            "Epoch 1083: train loss: 0.14332762360572815\n",
            "Epoch 1084: train loss: 0.14330095052719116\n",
            "Epoch 1085: train loss: 0.1432754099369049\n",
            "Epoch 1086: train loss: 0.14324910938739777\n",
            "Epoch 1087: train loss: 0.14322246611118317\n",
            "Epoch 1088: train loss: 0.14319685101509094\n",
            "Epoch 1089: train loss: 0.1431707739830017\n",
            "Epoch 1090: train loss: 0.14314411580562592\n",
            "Epoch 1091: train loss: 0.14311844110488892\n",
            "Epoch 1092: train loss: 0.14309248328208923\n",
            "Epoch 1093: train loss: 0.14306588470935822\n",
            "Epoch 1094: train loss: 0.14304019510746002\n",
            "Epoch 1095: train loss: 0.14301437139511108\n",
            "Epoch 1096: train loss: 0.14298780262470245\n",
            "Epoch 1097: train loss: 0.1429620385169983\n",
            "Epoch 1098: train loss: 0.1429363191127777\n",
            "Epoch 1099: train loss: 0.14290980994701385\n",
            "Epoch 1100: train loss: 0.1428840309381485\n",
            "Epoch 1101: train loss: 0.14285844564437866\n",
            "Epoch 1102: train loss: 0.1428319662809372\n",
            "Epoch 1103: train loss: 0.142806276679039\n",
            "Epoch 1104: train loss: 0.14278070628643036\n",
            "Epoch 1105: train loss: 0.14275436103343964\n",
            "Epoch 1106: train loss: 0.1427285075187683\n",
            "Epoch 1107: train loss: 0.1427031010389328\n",
            "Epoch 1108: train loss: 0.14267675578594208\n",
            "Epoch 1109: train loss: 0.14265090227127075\n",
            "Epoch 1110: train loss: 0.142625629901886\n",
            "Epoch 1111: train loss: 0.14259932935237885\n",
            "Epoch 1112: train loss: 0.14257344603538513\n",
            "Epoch 1113: train loss: 0.14254826307296753\n",
            "Epoch 1114: train loss: 0.1425219476222992\n",
            "Epoch 1115: train loss: 0.14249610900878906\n",
            "Epoch 1116: train loss: 0.1424710601568222\n",
            "Epoch 1117: train loss: 0.14244472980499268\n",
            "Epoch 1118: train loss: 0.14241887629032135\n",
            "Epoch 1119: train loss: 0.14239394664764404\n",
            "Epoch 1120: train loss: 0.14236772060394287\n",
            "Epoch 1121: train loss: 0.14234180748462677\n",
            "Epoch 1122: train loss: 0.14231693744659424\n",
            "Epoch 1123: train loss: 0.14229078590869904\n",
            "Epoch 1124: train loss: 0.14226476848125458\n",
            "Epoch 1125: train loss: 0.14224006235599518\n",
            "Epoch 1126: train loss: 0.14221395552158356\n",
            "Epoch 1127: train loss: 0.1421879380941391\n",
            "Epoch 1128: train loss: 0.14216333627700806\n",
            "Epoch 1129: train loss: 0.14213725924491882\n",
            "Epoch 1130: train loss: 0.14211134612560272\n",
            "Epoch 1131: train loss: 0.14208661019802094\n",
            "Epoch 1132: train loss: 0.14206087589263916\n",
            "Epoch 1133: train loss: 0.14203481376171112\n",
            "Epoch 1134: train loss: 0.1420099288225174\n",
            "Epoch 1135: train loss: 0.14198441803455353\n",
            "Epoch 1136: train loss: 0.14195849001407623\n",
            "Epoch 1137: train loss: 0.14193342626094818\n",
            "Epoch 1138: train loss: 0.14190812408924103\n",
            "Epoch 1139: train loss: 0.14188224077224731\n",
            "Epoch 1140: train loss: 0.14185717701911926\n",
            "Epoch 1141: train loss: 0.14183202385902405\n",
            "Epoch 1142: train loss: 0.14180611073970795\n",
            "Epoch 1143: train loss: 0.14178098738193512\n",
            "Epoch 1144: train loss: 0.14175592362880707\n",
            "Epoch 1145: train loss: 0.14173005521297455\n",
            "Epoch 1146: train loss: 0.1417049616575241\n",
            "Epoch 1147: train loss: 0.14167998731136322\n",
            "Epoch 1148: train loss: 0.14165420830249786\n",
            "Epoch 1149: train loss: 0.14162902534008026\n",
            "Epoch 1150: train loss: 0.1416042298078537\n",
            "Epoch 1151: train loss: 0.14157842099666595\n",
            "Epoch 1152: train loss: 0.14155322313308716\n",
            "Epoch 1153: train loss: 0.14152851700782776\n",
            "Epoch 1154: train loss: 0.14150279760360718\n",
            "Epoch 1155: train loss: 0.1414775848388672\n",
            "Epoch 1156: train loss: 0.14145298302173615\n",
            "Epoch 1157: train loss: 0.14142729341983795\n",
            "Epoch 1158: train loss: 0.14140193164348602\n",
            "Epoch 1159: train loss: 0.1413774937391281\n",
            "Epoch 1160: train loss: 0.14135189354419708\n",
            "Epoch 1161: train loss: 0.14132656157016754\n",
            "Epoch 1162: train loss: 0.1413022130727768\n",
            "Epoch 1163: train loss: 0.14127661287784576\n",
            "Epoch 1164: train loss: 0.14125120639801025\n",
            "Epoch 1165: train loss: 0.14122702181339264\n",
            "Epoch 1166: train loss: 0.141201451420784\n",
            "Epoch 1167: train loss: 0.1411760300397873\n",
            "Epoch 1168: train loss: 0.14115194976329803\n",
            "Epoch 1169: train loss: 0.14112640917301178\n",
            "Epoch 1170: train loss: 0.14110106229782104\n",
            "Epoch 1171: train loss: 0.14107686281204224\n",
            "Epoch 1172: train loss: 0.14105163514614105\n",
            "Epoch 1173: train loss: 0.14102621376514435\n",
            "Epoch 1174: train loss: 0.14100182056427002\n",
            "Epoch 1175: train loss: 0.1409769058227539\n",
            "Epoch 1176: train loss: 0.14095145463943481\n",
            "Epoch 1177: train loss: 0.1409270167350769\n",
            "Epoch 1178: train loss: 0.14090223610401154\n",
            "Epoch 1179: train loss: 0.1408768892288208\n",
            "Epoch 1180: train loss: 0.14085228741168976\n",
            "Epoch 1181: train loss: 0.1408277153968811\n",
            "Epoch 1182: train loss: 0.14080239832401276\n",
            "Epoch 1183: train loss: 0.14077773690223694\n",
            "Epoch 1184: train loss: 0.14075331389904022\n",
            "Epoch 1185: train loss: 0.14072805643081665\n",
            "Epoch 1186: train loss: 0.14070335030555725\n",
            "Epoch 1187: train loss: 0.14067906141281128\n",
            "Epoch 1188: train loss: 0.14065375924110413\n",
            "Epoch 1189: train loss: 0.14062906801700592\n",
            "Epoch 1190: train loss: 0.14060483872890472\n",
            "Epoch 1191: train loss: 0.14057961106300354\n",
            "Epoch 1192: train loss: 0.14055491983890533\n",
            "Epoch 1193: train loss: 0.14053082466125488\n",
            "Epoch 1194: train loss: 0.14050567150115967\n",
            "Epoch 1195: train loss: 0.14048084616661072\n",
            "Epoch 1196: train loss: 0.14045682549476624\n",
            "Epoch 1197: train loss: 0.1404317319393158\n",
            "Epoch 1198: train loss: 0.14040690660476685\n",
            "Epoch 1199: train loss: 0.1403830498456955\n",
            "Epoch 1200: train loss: 0.14035795629024506\n",
            "Epoch 1201: train loss: 0.14033305644989014\n",
            "Epoch 1202: train loss: 0.14030931890010834\n",
            "Epoch 1203: train loss: 0.14028427004814148\n",
            "Epoch 1204: train loss: 0.14025937020778656\n",
            "Epoch 1205: train loss: 0.14023572206497192\n",
            "Epoch 1206: train loss: 0.14021074771881104\n",
            "Epoch 1207: train loss: 0.14018584787845612\n",
            "Epoch 1208: train loss: 0.1401621699333191\n",
            "Epoch 1209: train loss: 0.1401374787092209\n",
            "Epoch 1210: train loss: 0.1401124894618988\n",
            "Epoch 1211: train loss: 0.14008861780166626\n",
            "Epoch 1212: train loss: 0.14006419479846954\n",
            "Epoch 1213: train loss: 0.14003930985927582\n",
            "Epoch 1214: train loss: 0.14001527428627014\n",
            "Epoch 1215: train loss: 0.13999103009700775\n",
            "Epoch 1216: train loss: 0.1399661898612976\n",
            "Epoch 1217: train loss: 0.13994209468364716\n",
            "Epoch 1218: train loss: 0.13991795480251312\n",
            "Epoch 1219: train loss: 0.13989311456680298\n",
            "Epoch 1220: train loss: 0.13986901938915253\n",
            "Epoch 1221: train loss: 0.13984504342079163\n",
            "Epoch 1222: train loss: 0.13982024788856506\n",
            "Epoch 1223: train loss: 0.13979601860046387\n",
            "Epoch 1224: train loss: 0.1397722214460373\n",
            "Epoch 1225: train loss: 0.1397474855184555\n",
            "Epoch 1226: train loss: 0.13972321152687073\n",
            "Epoch 1227: train loss: 0.13969950377941132\n",
            "Epoch 1228: train loss: 0.1396748125553131\n",
            "Epoch 1229: train loss: 0.13965043425559998\n",
            "Epoch 1230: train loss: 0.13962695002555847\n",
            "Epoch 1231: train loss: 0.1396021991968155\n",
            "Epoch 1232: train loss: 0.13957786560058594\n",
            "Epoch 1233: train loss: 0.1395544707775116\n",
            "Epoch 1234: train loss: 0.1395297795534134\n",
            "Epoch 1235: train loss: 0.13950537145137787\n",
            "Epoch 1236: train loss: 0.13948211073875427\n",
            "Epoch 1237: train loss: 0.13945752382278442\n",
            "Epoch 1238: train loss: 0.13943299651145935\n",
            "Epoch 1239: train loss: 0.13940982520580292\n",
            "Epoch 1240: train loss: 0.13938529789447784\n",
            "Epoch 1241: train loss: 0.13936085999011993\n",
            "Epoch 1242: train loss: 0.13933755457401276\n",
            "Epoch 1243: train loss: 0.13931329548358917\n",
            "Epoch 1244: train loss: 0.13928881287574768\n",
            "Epoch 1245: train loss: 0.13926535844802856\n",
            "Epoch 1246: train loss: 0.13924138247966766\n",
            "Epoch 1247: train loss: 0.13921691477298737\n",
            "Epoch 1248: train loss: 0.1391933113336563\n",
            "Epoch 1249: train loss: 0.13916949927806854\n",
            "Epoch 1250: train loss: 0.13914507627487183\n",
            "Epoch 1251: train loss: 0.1391213983297348\n",
            "Epoch 1252: train loss: 0.13909777998924255\n",
            "Epoch 1253: train loss: 0.1390734165906906\n",
            "Epoch 1254: train loss: 0.13904963433742523\n",
            "Epoch 1255: train loss: 0.13902612030506134\n",
            "Epoch 1256: train loss: 0.13900181651115417\n",
            "Epoch 1257: train loss: 0.13897795975208282\n",
            "Epoch 1258: train loss: 0.13895465433597565\n",
            "Epoch 1259: train loss: 0.1389302909374237\n",
            "Epoch 1260: train loss: 0.13890640437602997\n",
            "Epoch 1261: train loss: 0.13888326287269592\n",
            "Epoch 1262: train loss: 0.13885898888111115\n",
            "Epoch 1263: train loss: 0.13883502781391144\n",
            "Epoch 1264: train loss: 0.13881197571754456\n",
            "Epoch 1265: train loss: 0.13878768682479858\n",
            "Epoch 1266: train loss: 0.13876372575759888\n",
            "Epoch 1267: train loss: 0.13874076306819916\n",
            "Epoch 1268: train loss: 0.13871657848358154\n",
            "Epoch 1269: train loss: 0.13869252800941467\n",
            "Epoch 1270: train loss: 0.1386696994304657\n",
            "Epoch 1271: train loss: 0.13864554464817047\n",
            "Epoch 1272: train loss: 0.138621523976326\n",
            "Epoch 1273: train loss: 0.13859860599040985\n",
            "Epoch 1274: train loss: 0.1385747790336609\n",
            "Epoch 1275: train loss: 0.13855066895484924\n",
            "Epoch 1276: train loss: 0.13852757215499878\n",
            "Epoch 1277: train loss: 0.1385040432214737\n",
            "Epoch 1278: train loss: 0.13847997784614563\n",
            "Epoch 1279: train loss: 0.13845673203468323\n",
            "Epoch 1280: train loss: 0.13843335211277008\n",
            "Epoch 1281: train loss: 0.1384093165397644\n",
            "Epoch 1282: train loss: 0.13838598132133484\n",
            "Epoch 1283: train loss: 0.13836276531219482\n",
            "Epoch 1284: train loss: 0.1383388489484787\n",
            "Epoch 1285: train loss: 0.1383153796195984\n",
            "Epoch 1286: train loss: 0.1382923722267151\n",
            "Epoch 1287: train loss: 0.1382683515548706\n",
            "Epoch 1288: train loss: 0.13824491202831268\n",
            "Epoch 1289: train loss: 0.13822199404239655\n",
            "Epoch 1290: train loss: 0.13819807767868042\n",
            "Epoch 1291: train loss: 0.13817453384399414\n",
            "Epoch 1292: train loss: 0.13815172016620636\n",
            "Epoch 1293: train loss: 0.1381279081106186\n",
            "Epoch 1294: train loss: 0.13810424506664276\n",
            "Epoch 1295: train loss: 0.1380816549062729\n",
            "Epoch 1296: train loss: 0.13805778324604034\n",
            "Epoch 1297: train loss: 0.13803412020206451\n",
            "Epoch 1298: train loss: 0.13801166415214539\n",
            "Epoch 1299: train loss: 0.13798780739307404\n",
            "Epoch 1300: train loss: 0.137964129447937\n",
            "Epoch 1301: train loss: 0.13794158399105072\n",
            "Epoch 1302: train loss: 0.13791805505752563\n",
            "Epoch 1303: train loss: 0.13789433240890503\n",
            "Epoch 1304: train loss: 0.1378716230392456\n",
            "Epoch 1305: train loss: 0.13784834742546082\n",
            "Epoch 1306: train loss: 0.13782468438148499\n",
            "Epoch 1307: train loss: 0.13780175149440765\n",
            "Epoch 1308: train loss: 0.13777872920036316\n",
            "Epoch 1309: train loss: 0.13775503635406494\n",
            "Epoch 1310: train loss: 0.13773208856582642\n",
            "Epoch 1311: train loss: 0.13770918548107147\n",
            "Epoch 1312: train loss: 0.13768552243709564\n",
            "Epoch 1313: train loss: 0.13766250014305115\n",
            "Epoch 1314: train loss: 0.13763979077339172\n",
            "Epoch 1315: train loss: 0.13761617243289948\n",
            "Epoch 1316: train loss: 0.137593075633049\n",
            "Epoch 1317: train loss: 0.13757050037384033\n",
            "Epoch 1318: train loss: 0.13754692673683167\n",
            "Epoch 1319: train loss: 0.13752366602420807\n",
            "Epoch 1320: train loss: 0.13750123977661133\n",
            "Epoch 1321: train loss: 0.13747775554656982\n",
            "Epoch 1322: train loss: 0.13745446503162384\n",
            "Epoch 1323: train loss: 0.13743214309215546\n",
            "Epoch 1324: train loss: 0.13740864396095276\n",
            "Epoch 1325: train loss: 0.13738535344600677\n",
            "Epoch 1326: train loss: 0.13736313581466675\n",
            "Epoch 1327: train loss: 0.13733968138694763\n",
            "Epoch 1328: train loss: 0.13731634616851807\n",
            "Epoch 1329: train loss: 0.13729417324066162\n",
            "Epoch 1330: train loss: 0.137270987033844\n",
            "Epoch 1331: train loss: 0.13724762201309204\n",
            "Epoch 1332: train loss: 0.1372252106666565\n",
            "Epoch 1333: train loss: 0.13720229268074036\n",
            "Epoch 1334: train loss: 0.1371789127588272\n",
            "Epoch 1335: train loss: 0.13715635240077972\n",
            "Epoch 1336: train loss: 0.13713368773460388\n",
            "Epoch 1337: train loss: 0.13711029291152954\n",
            "Epoch 1338: train loss: 0.13708770275115967\n",
            "Epoch 1339: train loss: 0.13706515729427338\n",
            "Epoch 1340: train loss: 0.1370418518781662\n",
            "Epoch 1341: train loss: 0.13701912760734558\n",
            "Epoch 1342: train loss: 0.1369967758655548\n",
            "Epoch 1343: train loss: 0.13697347044944763\n",
            "Epoch 1344: train loss: 0.13695068657398224\n",
            "Epoch 1345: train loss: 0.13692845404148102\n",
            "Epoch 1346: train loss: 0.13690519332885742\n",
            "Epoch 1347: train loss: 0.13688227534294128\n",
            "Epoch 1348: train loss: 0.1368602067232132\n",
            "Epoch 1349: train loss: 0.13683705031871796\n",
            "Epoch 1350: train loss: 0.13681411743164062\n",
            "Epoch 1351: train loss: 0.1367921680212021\n",
            "Epoch 1352: train loss: 0.13676905632019043\n",
            "Epoch 1353: train loss: 0.13674597442150116\n",
            "Epoch 1354: train loss: 0.13672414422035217\n",
            "Epoch 1355: train loss: 0.1367010474205017\n",
            "Epoch 1356: train loss: 0.1366780549287796\n",
            "Epoch 1357: train loss: 0.13665610551834106\n",
            "Epoch 1358: train loss: 0.13663333654403687\n",
            "Epoch 1359: train loss: 0.13661028444766998\n",
            "Epoch 1360: train loss: 0.13658811151981354\n",
            "Epoch 1361: train loss: 0.13656562566757202\n",
            "Epoch 1362: train loss: 0.13654257357120514\n",
            "Epoch 1363: train loss: 0.13652029633522034\n",
            "Epoch 1364: train loss: 0.13649803400039673\n",
            "Epoch 1365: train loss: 0.13647496700286865\n",
            "Epoch 1366: train loss: 0.13645263016223907\n",
            "Epoch 1367: train loss: 0.1364305466413498\n",
            "Epoch 1368: train loss: 0.1364075392484665\n",
            "Epoch 1369: train loss: 0.13638509809970856\n",
            "Epoch 1370: train loss: 0.13636313378810883\n",
            "Epoch 1371: train loss: 0.1363401710987091\n",
            "Epoch 1372: train loss: 0.13631758093833923\n",
            "Epoch 1373: train loss: 0.13629576563835144\n",
            "Epoch 1374: train loss: 0.1362728774547577\n",
            "Epoch 1375: train loss: 0.13625021278858185\n",
            "Epoch 1376: train loss: 0.13622862100601196\n",
            "Epoch 1377: train loss: 0.13620571792125702\n",
            "Epoch 1378: train loss: 0.136182963848114\n",
            "Epoch 1379: train loss: 0.13616147637367249\n",
            "Epoch 1380: train loss: 0.1361386626958847\n",
            "Epoch 1381: train loss: 0.1361159235239029\n",
            "Epoch 1382: train loss: 0.13609427213668823\n",
            "Epoch 1383: train loss: 0.13607177138328552\n",
            "Epoch 1384: train loss: 0.1360490322113037\n",
            "Epoch 1385: train loss: 0.13602714240550995\n",
            "Epoch 1386: train loss: 0.13600493967533112\n",
            "Epoch 1387: train loss: 0.13598226010799408\n",
            "Epoch 1388: train loss: 0.1359601467847824\n",
            "Epoch 1389: train loss: 0.13593822717666626\n",
            "Epoch 1390: train loss: 0.13591554760932922\n",
            "Epoch 1391: train loss: 0.13589343428611755\n",
            "Epoch 1392: train loss: 0.13587158918380737\n",
            "Epoch 1393: train loss: 0.13584890961647034\n",
            "Epoch 1394: train loss: 0.1358267068862915\n",
            "Epoch 1395: train loss: 0.13580507040023804\n",
            "Epoch 1396: train loss: 0.135782390832901\n",
            "Epoch 1397: train loss: 0.1357600837945938\n",
            "Epoch 1398: train loss: 0.13573859632015228\n",
            "Epoch 1399: train loss: 0.13571597635746002\n",
            "Epoch 1400: train loss: 0.13569359481334686\n",
            "Epoch 1401: train loss: 0.13567222654819489\n",
            "Epoch 1402: train loss: 0.135649636387825\n",
            "Epoch 1403: train loss: 0.13562718033790588\n",
            "Epoch 1404: train loss: 0.13560591638088226\n",
            "Epoch 1405: train loss: 0.13558357954025269\n",
            "Epoch 1406: train loss: 0.13556107878684998\n",
            "Epoch 1407: train loss: 0.13553959131240845\n",
            "Epoch 1408: train loss: 0.13551750779151917\n",
            "Epoch 1409: train loss: 0.13549502193927765\n",
            "Epoch 1410: train loss: 0.1354733407497406\n",
            "Epoch 1411: train loss: 0.135451540350914\n",
            "Epoch 1412: train loss: 0.13542908430099487\n",
            "Epoch 1413: train loss: 0.13540726900100708\n",
            "Epoch 1414: train loss: 0.13538560271263123\n",
            "Epoch 1415: train loss: 0.13536317646503448\n",
            "Epoch 1416: train loss: 0.1353413164615631\n",
            "Epoch 1417: train loss: 0.13531985878944397\n",
            "Epoch 1418: train loss: 0.13529744744300842\n",
            "Epoch 1419: train loss: 0.1352754533290863\n",
            "Epoch 1420: train loss: 0.1352541595697403\n",
            "Epoch 1421: train loss: 0.13523177802562714\n",
            "Epoch 1422: train loss: 0.13520970940589905\n",
            "Epoch 1423: train loss: 0.13518857955932617\n",
            "Epoch 1424: train loss: 0.1351662427186966\n",
            "Epoch 1425: train loss: 0.13514405488967896\n",
            "Epoch 1426: train loss: 0.13512305915355682\n",
            "Epoch 1427: train loss: 0.13510076701641083\n",
            "Epoch 1428: train loss: 0.1350785791873932\n",
            "Epoch 1429: train loss: 0.1350574791431427\n",
            "Epoch 1430: train loss: 0.13503552973270416\n",
            "Epoch 1431: train loss: 0.13501328229904175\n",
            "Epoch 1432: train loss: 0.13499197363853455\n",
            "Epoch 1433: train loss: 0.13497024774551392\n",
            "Epoch 1434: train loss: 0.1349480301141739\n",
            "Epoch 1435: train loss: 0.13492660224437714\n",
            "Epoch 1436: train loss: 0.1349051594734192\n",
            "Epoch 1437: train loss: 0.13488300144672394\n",
            "Epoch 1438: train loss: 0.13486139476299286\n",
            "Epoch 1439: train loss: 0.13484010100364685\n",
            "Epoch 1440: train loss: 0.1348179578781128\n",
            "Epoch 1441: train loss: 0.13479626178741455\n",
            "Epoch 1442: train loss: 0.13477511703968048\n",
            "Epoch 1443: train loss: 0.1347530335187912\n",
            "Epoch 1444: train loss: 0.1347312033176422\n",
            "Epoch 1445: train loss: 0.13471026718616486\n",
            "Epoch 1446: train loss: 0.13468819856643677\n",
            "Epoch 1447: train loss: 0.1346662938594818\n",
            "Epoch 1448: train loss: 0.13464553654193878\n",
            "Epoch 1449: train loss: 0.13462351262569427\n",
            "Epoch 1450: train loss: 0.13460153341293335\n",
            "Epoch 1451: train loss: 0.13458073139190674\n",
            "Epoch 1452: train loss: 0.1345590054988861\n",
            "Epoch 1453: train loss: 0.1345369815826416\n",
            "Epoch 1454: train loss: 0.1345159411430359\n",
            "Epoch 1455: train loss: 0.13449448347091675\n",
            "Epoch 1456: train loss: 0.13447250425815582\n",
            "Epoch 1457: train loss: 0.1344512403011322\n",
            "Epoch 1458: train loss: 0.13443005084991455\n",
            "Epoch 1459: train loss: 0.13440817594528198\n",
            "Epoch 1460: train loss: 0.1343867927789688\n",
            "Epoch 1461: train loss: 0.13436578214168549\n",
            "Epoch 1462: train loss: 0.13434384763240814\n",
            "Epoch 1463: train loss: 0.1343224048614502\n",
            "Epoch 1464: train loss: 0.13430151343345642\n",
            "Epoch 1465: train loss: 0.13427968323230743\n",
            "Epoch 1466: train loss: 0.13425809144973755\n",
            "Epoch 1467: train loss: 0.1342373639345169\n",
            "Epoch 1468: train loss: 0.13421553373336792\n",
            "Epoch 1469: train loss: 0.13419388234615326\n",
            "Epoch 1470: train loss: 0.13417337834835052\n",
            "Epoch 1471: train loss: 0.13415154814720154\n",
            "Epoch 1472: train loss: 0.13412989675998688\n",
            "Epoch 1473: train loss: 0.13410919904708862\n",
            "Epoch 1474: train loss: 0.13408775627613068\n",
            "Epoch 1475: train loss: 0.13406604528427124\n",
            "Epoch 1476: train loss: 0.13404518365859985\n",
            "Epoch 1477: train loss: 0.1340239942073822\n",
            "Epoch 1478: train loss: 0.13400228321552277\n",
            "Epoch 1479: train loss: 0.13398125767707825\n",
            "Epoch 1480: train loss: 0.1339603066444397\n",
            "Epoch 1481: train loss: 0.13393861055374146\n",
            "Epoch 1482: train loss: 0.13391748070716858\n",
            "Epoch 1483: train loss: 0.13389667868614197\n",
            "Epoch 1484: train loss: 0.1338750720024109\n",
            "Epoch 1485: train loss: 0.13385379314422607\n",
            "Epoch 1486: train loss: 0.13383318483829498\n",
            "Epoch 1487: train loss: 0.13381153345108032\n",
            "Epoch 1488: train loss: 0.13379018008708954\n",
            "Epoch 1489: train loss: 0.13376976549625397\n",
            "Epoch 1490: train loss: 0.13374818861484528\n",
            "Epoch 1491: train loss: 0.13372668623924255\n",
            "Epoch 1492: train loss: 0.1337064504623413\n",
            "Epoch 1493: train loss: 0.1336849182844162\n",
            "Epoch 1494: train loss: 0.13366346061229706\n",
            "Epoch 1495: train loss: 0.13364306092262268\n",
            "Epoch 1496: train loss: 0.13362181186676025\n",
            "Epoch 1497: train loss: 0.13360029458999634\n",
            "Epoch 1498: train loss: 0.13357964158058167\n",
            "Epoch 1499: train loss: 0.1335587054491043\n",
            "Epoch 1500: train loss: 0.13353733718395233\n",
            "Epoch 1501: train loss: 0.13351641595363617\n",
            "Epoch 1502: train loss: 0.13349570333957672\n",
            "Epoch 1503: train loss: 0.13347433507442474\n",
            "Epoch 1504: train loss: 0.13345330953598022\n",
            "Epoch 1505: train loss: 0.13343285024166107\n",
            "Epoch 1506: train loss: 0.13341142237186432\n",
            "Epoch 1507: train loss: 0.1333903819322586\n",
            "Epoch 1508: train loss: 0.1333700567483902\n",
            "Epoch 1509: train loss: 0.13334867358207703\n",
            "Epoch 1510: train loss: 0.13332746922969818\n",
            "Epoch 1511: train loss: 0.13330739736557007\n",
            "Epoch 1512: train loss: 0.13328604400157928\n",
            "Epoch 1513: train loss: 0.1332647204399109\n",
            "Epoch 1514: train loss: 0.1332445740699768\n",
            "Epoch 1515: train loss: 0.13322347402572632\n",
            "Epoch 1516: train loss: 0.13320223987102509\n",
            "Epoch 1517: train loss: 0.13318176567554474\n",
            "Epoch 1518: train loss: 0.1331610530614853\n",
            "Epoch 1519: train loss: 0.13313978910446167\n",
            "Epoch 1520: train loss: 0.13311919569969177\n",
            "Epoch 1521: train loss: 0.13309869170188904\n",
            "Epoch 1522: train loss: 0.13307739794254303\n",
            "Epoch 1523: train loss: 0.1330566704273224\n",
            "Epoch 1524: train loss: 0.13303636014461517\n",
            "Epoch 1525: train loss: 0.13301515579223633\n",
            "Epoch 1526: train loss: 0.13299427926540375\n",
            "Epoch 1527: train loss: 0.13297413289546967\n",
            "Epoch 1528: train loss: 0.1329529732465744\n",
            "Epoch 1529: train loss: 0.13293202221393585\n",
            "Epoch 1530: train loss: 0.1329120546579361\n",
            "Epoch 1531: train loss: 0.13289086520671844\n",
            "Epoch 1532: train loss: 0.13286979496479034\n",
            "Epoch 1533: train loss: 0.13284993171691895\n",
            "Epoch 1534: train loss: 0.13282901048660278\n",
            "Epoch 1535: train loss: 0.1328079104423523\n",
            "Epoch 1536: train loss: 0.13278771936893463\n",
            "Epoch 1537: train loss: 0.13276714086532593\n",
            "Epoch 1538: train loss: 0.13274605572223663\n",
            "Epoch 1539: train loss: 0.13272568583488464\n",
            "Epoch 1540: train loss: 0.13270533084869385\n",
            "Epoch 1541: train loss: 0.13268429040908813\n",
            "Epoch 1542: train loss: 0.1326637864112854\n",
            "Epoch 1543: train loss: 0.13264362514019012\n",
            "Epoch 1544: train loss: 0.132622629404068\n",
            "Epoch 1545: train loss: 0.13260196149349213\n",
            "Epoch 1546: train loss: 0.13258199393749237\n",
            "Epoch 1547: train loss: 0.13256101310253143\n",
            "Epoch 1548: train loss: 0.1325402706861496\n",
            "Epoch 1549: train loss: 0.13252054154872894\n",
            "Epoch 1550: train loss: 0.13249951601028442\n",
            "Epoch 1551: train loss: 0.13247865438461304\n",
            "Epoch 1552: train loss: 0.13245895504951477\n",
            "Epoch 1553: train loss: 0.13243819773197174\n",
            "Epoch 1554: train loss: 0.13241735100746155\n",
            "Epoch 1555: train loss: 0.1323973387479782\n",
            "Epoch 1556: train loss: 0.13237695395946503\n",
            "Epoch 1557: train loss: 0.13235607743263245\n",
            "Epoch 1558: train loss: 0.13233590126037598\n",
            "Epoch 1559: train loss: 0.1323157399892807\n",
            "Epoch 1560: train loss: 0.13229487836360931\n",
            "Epoch 1561: train loss: 0.1322745978832245\n",
            "Epoch 1562: train loss: 0.13225461542606354\n",
            "Epoch 1563: train loss: 0.13223381340503693\n",
            "Epoch 1564: train loss: 0.13221336901187897\n",
            "Epoch 1565: train loss: 0.13219355046749115\n",
            "Epoch 1566: train loss: 0.13217280805110931\n",
            "Epoch 1567: train loss: 0.1321522295475006\n",
            "Epoch 1568: train loss: 0.13213258981704712\n",
            "Epoch 1569: train loss: 0.13211187720298767\n",
            "Epoch 1570: train loss: 0.1320912092924118\n",
            "Epoch 1571: train loss: 0.13207167387008667\n",
            "Epoch 1572: train loss: 0.13205114006996155\n",
            "Epoch 1573: train loss: 0.13203050196170807\n",
            "Epoch 1574: train loss: 0.13201063871383667\n",
            "Epoch 1575: train loss: 0.1319904327392578\n",
            "Epoch 1576: train loss: 0.13196974992752075\n",
            "Epoch 1577: train loss: 0.1319497525691986\n",
            "Epoch 1578: train loss: 0.13192981481552124\n",
            "Epoch 1579: train loss: 0.13190916180610657\n",
            "Epoch 1580: train loss: 0.1318889856338501\n",
            "Epoch 1581: train loss: 0.13186922669410706\n",
            "Epoch 1582: train loss: 0.13184866309165955\n",
            "Epoch 1583: train loss: 0.13182838261127472\n",
            "Epoch 1584: train loss: 0.131808802485466\n",
            "Epoch 1585: train loss: 0.1317882239818573\n",
            "Epoch 1586: train loss: 0.13176782429218292\n",
            "Epoch 1587: train loss: 0.13174843788146973\n",
            "Epoch 1588: train loss: 0.13172784447669983\n",
            "Epoch 1589: train loss: 0.13170740008354187\n",
            "Epoch 1590: train loss: 0.13168805837631226\n",
            "Epoch 1591: train loss: 0.13166768848896027\n",
            "Epoch 1592: train loss: 0.1316472440958023\n",
            "Epoch 1593: train loss: 0.13162757456302643\n",
            "Epoch 1594: train loss: 0.1316075176000595\n",
            "Epoch 1595: train loss: 0.13158708810806274\n",
            "Epoch 1596: train loss: 0.13156723976135254\n",
            "Epoch 1597: train loss: 0.1315474808216095\n",
            "Epoch 1598: train loss: 0.13152706623077393\n",
            "Epoch 1599: train loss: 0.13150709867477417\n",
            "Epoch 1600: train loss: 0.13148754835128784\n",
            "Epoch 1601: train loss: 0.13146713376045227\n",
            "Epoch 1602: train loss: 0.13144703209400177\n",
            "Epoch 1603: train loss: 0.1314275711774826\n",
            "Epoch 1604: train loss: 0.1314072459936142\n",
            "Epoch 1605: train loss: 0.13138702511787415\n",
            "Epoch 1606: train loss: 0.13136781752109528\n",
            "Epoch 1607: train loss: 0.13134746253490448\n",
            "Epoch 1608: train loss: 0.13132715225219727\n",
            "Epoch 1609: train loss: 0.13130797445774078\n",
            "Epoch 1610: train loss: 0.13128787279129028\n",
            "Epoch 1611: train loss: 0.13126756250858307\n",
            "Epoch 1612: train loss: 0.13124804198741913\n",
            "Epoch 1613: train loss: 0.1312282830476761\n",
            "Epoch 1614: train loss: 0.13120798766613007\n",
            "Epoch 1615: train loss: 0.1311883181333542\n",
            "Epoch 1616: train loss: 0.13116878271102905\n",
            "Epoch 1617: train loss: 0.1311485320329666\n",
            "Epoch 1618: train loss: 0.13112866878509521\n",
            "Epoch 1619: train loss: 0.13110941648483276\n",
            "Epoch 1620: train loss: 0.13108913600444794\n",
            "Epoch 1621: train loss: 0.13106918334960938\n",
            "Epoch 1622: train loss: 0.13105006515979767\n",
            "Epoch 1623: train loss: 0.13102981448173523\n",
            "Epoch 1624: train loss: 0.13100974261760712\n",
            "Epoch 1625: train loss: 0.13099080324172974\n",
            "Epoch 1626: train loss: 0.13097059726715088\n",
            "Epoch 1627: train loss: 0.13095054030418396\n",
            "Epoch 1628: train loss: 0.13093142211437225\n",
            "Epoch 1629: train loss: 0.13091160356998444\n",
            "Epoch 1630: train loss: 0.13089147210121155\n",
            "Epoch 1631: train loss: 0.13087207078933716\n",
            "Epoch 1632: train loss: 0.13085253536701202\n",
            "Epoch 1633: train loss: 0.1308324784040451\n",
            "Epoch 1634: train loss: 0.130812868475914\n",
            "Epoch 1635: train loss: 0.13079358637332916\n",
            "Epoch 1636: train loss: 0.13077354431152344\n",
            "Epoch 1637: train loss: 0.1307538002729416\n",
            "Epoch 1638: train loss: 0.13073474168777466\n",
            "Epoch 1639: train loss: 0.13071471452713013\n",
            "Epoch 1640: train loss: 0.13069483637809753\n",
            "Epoch 1641: train loss: 0.13067594170570374\n",
            "Epoch 1642: train loss: 0.1306559443473816\n",
            "Epoch 1643: train loss: 0.13063596189022064\n",
            "Epoch 1644: train loss: 0.13061711192131042\n",
            "Epoch 1645: train loss: 0.13059736788272858\n",
            "Epoch 1646: train loss: 0.13057738542556763\n",
            "Epoch 1647: train loss: 0.13055823743343353\n",
            "Epoch 1648: train loss: 0.13053882122039795\n",
            "Epoch 1649: train loss: 0.1305188685655594\n",
            "Epoch 1650: train loss: 0.1304994821548462\n",
            "Epoch 1651: train loss: 0.1304803341627121\n",
            "Epoch 1652: train loss: 0.13046042621135712\n",
            "Epoch 1653: train loss: 0.13044092059135437\n",
            "Epoch 1654: train loss: 0.13042192161083221\n",
            "Epoch 1655: train loss: 0.13040205836296082\n",
            "Epoch 1656: train loss: 0.13038240373134613\n",
            "Epoch 1657: train loss: 0.1303636133670807\n",
            "Epoch 1658: train loss: 0.1303437203168869\n",
            "Epoch 1659: train loss: 0.13032397627830505\n",
            "Epoch 1660: train loss: 0.1303052455186844\n",
            "Epoch 1661: train loss: 0.13028565049171448\n",
            "Epoch 1662: train loss: 0.13026580214500427\n",
            "Epoch 1663: train loss: 0.1302468329668045\n",
            "Epoch 1664: train loss: 0.13022752106189728\n",
            "Epoch 1665: train loss: 0.13020776212215424\n",
            "Epoch 1666: train loss: 0.13018855452537537\n",
            "Epoch 1667: train loss: 0.13016951084136963\n",
            "Epoch 1668: train loss: 0.13014976680278778\n",
            "Epoch 1669: train loss: 0.13013041019439697\n",
            "Epoch 1670: train loss: 0.13011153042316437\n",
            "Epoch 1671: train loss: 0.1300918161869049\n",
            "Epoch 1672: train loss: 0.13007235527038574\n",
            "Epoch 1673: train loss: 0.13005372881889343\n",
            "Epoch 1674: train loss: 0.13003401458263397\n",
            "Epoch 1675: train loss: 0.13001440465450287\n",
            "Epoch 1676: train loss: 0.12999588251113892\n",
            "Epoch 1677: train loss: 0.12997619807720184\n",
            "Epoch 1678: train loss: 0.12995663285255432\n",
            "Epoch 1679: train loss: 0.12993793189525604\n",
            "Epoch 1680: train loss: 0.1299186646938324\n",
            "Epoch 1681: train loss: 0.1298990398645401\n",
            "Epoch 1682: train loss: 0.1298801302909851\n",
            "Epoch 1683: train loss: 0.12986108660697937\n",
            "Epoch 1684: train loss: 0.12984152138233185\n",
            "Epoch 1685: train loss: 0.12982237339019775\n",
            "Epoch 1686: train loss: 0.1298035979270935\n",
            "Epoch 1687: train loss: 0.12978404760360718\n",
            "Epoch 1688: train loss: 0.12976473569869995\n",
            "Epoch 1689: train loss: 0.1297461986541748\n",
            "Epoch 1690: train loss: 0.12972663342952728\n",
            "Epoch 1691: train loss: 0.1297072321176529\n",
            "Epoch 1692: train loss: 0.12968887388706207\n",
            "Epoch 1693: train loss: 0.12966927886009216\n",
            "Epoch 1694: train loss: 0.12964987754821777\n",
            "Epoch 1695: train loss: 0.1296314299106598\n",
            "Epoch 1696: train loss: 0.12961222231388092\n",
            "Epoch 1697: train loss: 0.12959274649620056\n",
            "Epoch 1698: train loss: 0.1295739710330963\n",
            "Epoch 1699: train loss: 0.1295551210641861\n",
            "Epoch 1700: train loss: 0.12953566014766693\n",
            "Epoch 1701: train loss: 0.12951667606830597\n",
            "Epoch 1702: train loss: 0.12949801981449127\n",
            "Epoch 1703: train loss: 0.1294785887002945\n",
            "Epoch 1704: train loss: 0.1294594705104828\n",
            "Epoch 1705: train loss: 0.129441037774086\n",
            "Epoch 1706: train loss: 0.1294216811656952\n",
            "Epoch 1707: train loss: 0.12940238416194916\n",
            "Epoch 1708: train loss: 0.12938417494297028\n",
            "Epoch 1709: train loss: 0.12936478853225708\n",
            "Epoch 1710: train loss: 0.12934549152851105\n",
            "Epoch 1711: train loss: 0.1293271780014038\n",
            "Epoch 1712: train loss: 0.1293080747127533\n",
            "Epoch 1713: train loss: 0.12928879261016846\n",
            "Epoch 1714: train loss: 0.12927013635635376\n",
            "Epoch 1715: train loss: 0.1292514204978943\n",
            "Epoch 1716: train loss: 0.12923213839530945\n",
            "Epoch 1717: train loss: 0.1292133331298828\n",
            "Epoch 1718: train loss: 0.12919482588768005\n",
            "Epoch 1719: train loss: 0.12917551398277283\n",
            "Epoch 1720: train loss: 0.12915658950805664\n",
            "Epoch 1721: train loss: 0.1291382759809494\n",
            "Epoch 1722: train loss: 0.12911900877952576\n",
            "Epoch 1723: train loss: 0.12909990549087524\n",
            "Epoch 1724: train loss: 0.1290818452835083\n",
            "Epoch 1725: train loss: 0.12906260788440704\n",
            "Epoch 1726: train loss: 0.12904345989227295\n",
            "Epoch 1727: train loss: 0.12902525067329407\n",
            "Epoch 1728: train loss: 0.12900632619857788\n",
            "Epoch 1729: train loss: 0.12898720800876617\n",
            "Epoch 1730: train loss: 0.12896870076656342\n",
            "Epoch 1731: train loss: 0.1289501190185547\n",
            "Epoch 1732: train loss: 0.1289309710264206\n",
            "Epoch 1733: train loss: 0.1289122998714447\n",
            "Epoch 1734: train loss: 0.12889395654201508\n",
            "Epoch 1735: train loss: 0.1288747787475586\n",
            "Epoch 1736: train loss: 0.12885598838329315\n",
            "Epoch 1737: train loss: 0.12883785367012024\n",
            "Epoch 1738: train loss: 0.12881875038146973\n",
            "Epoch 1739: train loss: 0.12879972159862518\n",
            "Epoch 1740: train loss: 0.12878184020519257\n",
            "Epoch 1741: train loss: 0.12876273691654205\n",
            "Epoch 1742: train loss: 0.1287437081336975\n",
            "Epoch 1743: train loss: 0.12872569262981415\n",
            "Epoch 1744: train loss: 0.1287068873643875\n",
            "Epoch 1745: train loss: 0.12868788838386536\n",
            "Epoch 1746: train loss: 0.12866954505443573\n",
            "Epoch 1747: train loss: 0.12865109741687775\n",
            "Epoch 1748: train loss: 0.1286320835351944\n",
            "Epoch 1749: train loss: 0.12861350178718567\n",
            "Epoch 1750: train loss: 0.12859535217285156\n",
            "Epoch 1751: train loss: 0.1285763829946518\n",
            "Epoch 1752: train loss: 0.12855763733386993\n",
            "Epoch 1753: train loss: 0.12853968143463135\n",
            "Epoch 1754: train loss: 0.12852074205875397\n",
            "Epoch 1755: train loss: 0.12850189208984375\n",
            "Epoch 1756: train loss: 0.1284840703010559\n",
            "Epoch 1757: train loss: 0.12846516072750092\n",
            "Epoch 1758: train loss: 0.12844626605510712\n",
            "Epoch 1759: train loss: 0.12842832505702972\n",
            "Epoch 1760: train loss: 0.1284097135066986\n",
            "Epoch 1761: train loss: 0.12839087843894958\n",
            "Epoch 1762: train loss: 0.12837262451648712\n",
            "Epoch 1763: train loss: 0.12835432589054108\n",
            "Epoch 1764: train loss: 0.12833547592163086\n",
            "Epoch 1765: train loss: 0.12831705808639526\n",
            "Epoch 1766: train loss: 0.12829901278018951\n",
            "Epoch 1767: train loss: 0.12828022241592407\n",
            "Epoch 1768: train loss: 0.12826155126094818\n",
            "Epoch 1769: train loss: 0.1282438188791275\n",
            "Epoch 1770: train loss: 0.12822498381137848\n",
            "Epoch 1771: train loss: 0.12820623815059662\n",
            "Epoch 1772: train loss: 0.12818853557109833\n",
            "Epoch 1773: train loss: 0.12816990911960602\n",
            "Epoch 1774: train loss: 0.12815116345882416\n",
            "Epoch 1775: train loss: 0.1281331330537796\n",
            "Epoch 1776: train loss: 0.12811492383480072\n",
            "Epoch 1777: train loss: 0.12809619307518005\n",
            "Epoch 1778: train loss: 0.12807795405387878\n",
            "Epoch 1779: train loss: 0.12805992364883423\n",
            "Epoch 1780: train loss: 0.12804122269153595\n",
            "Epoch 1781: train loss: 0.12802287936210632\n",
            "Epoch 1782: train loss: 0.1280050426721573\n",
            "Epoch 1783: train loss: 0.12798628211021423\n",
            "Epoch 1784: train loss: 0.12796783447265625\n",
            "Epoch 1785: train loss: 0.1279502511024475\n",
            "Epoch 1786: train loss: 0.12793153524398804\n",
            "Epoch 1787: train loss: 0.12791292369365692\n",
            "Epoch 1788: train loss: 0.1278952807188034\n",
            "Epoch 1789: train loss: 0.1278769075870514\n",
            "Epoch 1790: train loss: 0.12785832583904266\n",
            "Epoch 1791: train loss: 0.12784036993980408\n",
            "Epoch 1792: train loss: 0.12782230973243713\n",
            "Epoch 1793: train loss: 0.12780365347862244\n",
            "Epoch 1794: train loss: 0.12778551876544952\n",
            "Epoch 1795: train loss: 0.12776778638362885\n",
            "Epoch 1796: train loss: 0.12774913012981415\n",
            "Epoch 1797: train loss: 0.1277308315038681\n",
            "Epoch 1798: train loss: 0.12771326303482056\n",
            "Epoch 1799: train loss: 0.12769468128681183\n",
            "Epoch 1800: train loss: 0.12767620384693146\n",
            "Epoch 1801: train loss: 0.12765881419181824\n",
            "Epoch 1802: train loss: 0.1276402771472931\n",
            "Epoch 1803: train loss: 0.1276218146085739\n",
            "Epoch 1804: train loss: 0.12760426104068756\n",
            "Epoch 1805: train loss: 0.12758606672286987\n",
            "Epoch 1806: train loss: 0.1275676190853119\n",
            "Epoch 1807: train loss: 0.12754970788955688\n",
            "Epoch 1808: train loss: 0.12753188610076904\n",
            "Epoch 1809: train loss: 0.12751340866088867\n",
            "Epoch 1810: train loss: 0.12749533355236053\n",
            "Epoch 1811: train loss: 0.1274777501821518\n",
            "Epoch 1812: train loss: 0.1274593025445938\n",
            "Epoch 1813: train loss: 0.12744104862213135\n",
            "Epoch 1814: train loss: 0.12742364406585693\n",
            "Epoch 1815: train loss: 0.12740522623062134\n",
            "Epoch 1816: train loss: 0.1273869127035141\n",
            "Epoch 1817: train loss: 0.12736952304840088\n",
            "Epoch 1818: train loss: 0.12735134363174438\n",
            "Epoch 1819: train loss: 0.12733294069766998\n",
            "Epoch 1820: train loss: 0.12731532752513885\n",
            "Epoch 1821: train loss: 0.12729749083518982\n",
            "Epoch 1822: train loss: 0.127279132604599\n",
            "Epoch 1823: train loss: 0.127261221408844\n",
            "Epoch 1824: train loss: 0.12724369764328003\n",
            "Epoch 1825: train loss: 0.1272253543138504\n",
            "Epoch 1826: train loss: 0.12720729410648346\n",
            "Epoch 1827: train loss: 0.12718988955020905\n",
            "Epoch 1828: train loss: 0.1271716058254242\n",
            "Epoch 1829: train loss: 0.1271534413099289\n",
            "Epoch 1830: train loss: 0.1271362602710724\n",
            "Epoch 1831: train loss: 0.12711796164512634\n",
            "Epoch 1832: train loss: 0.12709975242614746\n",
            "Epoch 1833: train loss: 0.1270824670791626\n",
            "Epoch 1834: train loss: 0.1270645260810852\n",
            "Epoch 1835: train loss: 0.12704628705978394\n",
            "Epoch 1836: train loss: 0.1270286589860916\n",
            "Epoch 1837: train loss: 0.12701106071472168\n",
            "Epoch 1838: train loss: 0.12699288129806519\n",
            "Epoch 1839: train loss: 0.12697502970695496\n",
            "Epoch 1840: train loss: 0.12695764005184174\n",
            "Epoch 1841: train loss: 0.12693947553634644\n",
            "Epoch 1842: train loss: 0.12692147493362427\n",
            "Epoch 1843: train loss: 0.12690430879592896\n",
            "Epoch 1844: train loss: 0.12688615918159485\n",
            "Epoch 1845: train loss: 0.1268681138753891\n",
            "Epoch 1846: train loss: 0.1268509477376938\n",
            "Epoch 1847: train loss: 0.1268330067396164\n",
            "Epoch 1848: train loss: 0.12681493163108826\n",
            "Epoch 1849: train loss: 0.12679742276668549\n",
            "Epoch 1850: train loss: 0.12677991390228271\n",
            "Epoch 1851: train loss: 0.1267617642879486\n",
            "Epoch 1852: train loss: 0.1267441213130951\n",
            "Epoch 1853: train loss: 0.1267268806695938\n",
            "Epoch 1854: train loss: 0.1267087757587433\n",
            "Epoch 1855: train loss: 0.12669086456298828\n",
            "Epoch 1856: train loss: 0.1266738772392273\n",
            "Epoch 1857: train loss: 0.12665578722953796\n",
            "Epoch 1858: train loss: 0.1266377568244934\n",
            "Epoch 1859: train loss: 0.12662075459957123\n",
            "Epoch 1860: train loss: 0.1266029328107834\n",
            "Epoch 1861: train loss: 0.12658488750457764\n",
            "Epoch 1862: train loss: 0.12656760215759277\n",
            "Epoch 1863: train loss: 0.12655015289783478\n",
            "Epoch 1864: train loss: 0.12653213739395142\n",
            "Epoch 1865: train loss: 0.12651461362838745\n",
            "Epoch 1866: train loss: 0.12649738788604736\n",
            "Epoch 1867: train loss: 0.12647943198680878\n",
            "Epoch 1868: train loss: 0.1264616847038269\n",
            "Epoch 1869: train loss: 0.1264447420835495\n",
            "Epoch 1870: train loss: 0.1264268159866333\n",
            "Epoch 1871: train loss: 0.1264089047908783\n",
            "Epoch 1872: train loss: 0.12639202177524567\n",
            "Epoch 1873: train loss: 0.12637430429458618\n",
            "Epoch 1874: train loss: 0.12635637819766998\n",
            "Epoch 1875: train loss: 0.12633924186229706\n",
            "Epoch 1876: train loss: 0.12632179260253906\n",
            "Epoch 1877: train loss: 0.12630394101142883\n",
            "Epoch 1878: train loss: 0.1262865513563156\n",
            "Epoch 1879: train loss: 0.1262693852186203\n",
            "Epoch 1880: train loss: 0.12625154852867126\n",
            "Epoch 1881: train loss: 0.12623393535614014\n",
            "Epoch 1882: train loss: 0.12621702253818512\n",
            "Epoch 1883: train loss: 0.12619920074939728\n",
            "Epoch 1884: train loss: 0.1261814385652542\n",
            "Epoch 1885: train loss: 0.12616470456123352\n",
            "Epoch 1886: train loss: 0.12614692747592926\n",
            "Epoch 1887: train loss: 0.12612919509410858\n",
            "Epoch 1888: train loss: 0.12611226737499237\n",
            "Epoch 1889: train loss: 0.12609481811523438\n",
            "Epoch 1890: train loss: 0.1260770857334137\n",
            "Epoch 1891: train loss: 0.12605980038642883\n",
            "Epoch 1892: train loss: 0.1260427087545395\n",
            "Epoch 1893: train loss: 0.12602499127388\n",
            "Epoch 1894: train loss: 0.1260075569152832\n",
            "Epoch 1895: train loss: 0.12599070370197296\n",
            "Epoch 1896: train loss: 0.1259729415178299\n",
            "Epoch 1897: train loss: 0.12595535814762115\n",
            "Epoch 1898: train loss: 0.12593869864940643\n",
            "Epoch 1899: train loss: 0.12592102587223053\n",
            "Epoch 1900: train loss: 0.1259033977985382\n",
            "Epoch 1901: train loss: 0.12588657438755035\n",
            "Epoch 1902: train loss: 0.12586922943592072\n",
            "Epoch 1903: train loss: 0.12585154175758362\n",
            "Epoch 1904: train loss: 0.12583446502685547\n",
            "Epoch 1905: train loss: 0.1258174180984497\n",
            "Epoch 1906: train loss: 0.12579980492591858\n",
            "Epoch 1907: train loss: 0.12578247487545013\n",
            "Epoch 1908: train loss: 0.12576574087142944\n",
            "Epoch 1909: train loss: 0.12574809789657593\n",
            "Epoch 1910: train loss: 0.12573055922985077\n",
            "Epoch 1911: train loss: 0.12571409344673157\n",
            "Epoch 1912: train loss: 0.12569643557071686\n",
            "Epoch 1913: train loss: 0.1256788820028305\n",
            "Epoch 1914: train loss: 0.12566231191158295\n",
            "Epoch 1915: train loss: 0.12564493715763092\n",
            "Epoch 1916: train loss: 0.12562742829322815\n",
            "Epoch 1917: train loss: 0.12561050057411194\n",
            "Epoch 1918: train loss: 0.12559351325035095\n",
            "Epoch 1919: train loss: 0.1255759745836258\n",
            "Epoch 1920: train loss: 0.1255587935447693\n",
            "Epoch 1921: train loss: 0.12554208934307098\n",
            "Epoch 1922: train loss: 0.1255245804786682\n",
            "Epoch 1923: train loss: 0.1255072057247162\n",
            "Epoch 1924: train loss: 0.12549073994159698\n",
            "Epoch 1925: train loss: 0.1254732459783554\n",
            "Epoch 1926: train loss: 0.125455841422081\n",
            "Epoch 1927: train loss: 0.12543925642967224\n",
            "Epoch 1928: train loss: 0.1254221498966217\n",
            "Epoch 1929: train loss: 0.12540464103221893\n",
            "Epoch 1930: train loss: 0.12538781762123108\n",
            "Epoch 1931: train loss: 0.12537096440792084\n",
            "Epoch 1932: train loss: 0.12535348534584045\n",
            "Epoch 1933: train loss: 0.1253364235162735\n",
            "Epoch 1934: train loss: 0.12531983852386475\n",
            "Epoch 1935: train loss: 0.12530243396759033\n",
            "Epoch 1936: train loss: 0.12528516352176666\n",
            "Epoch 1937: train loss: 0.12526878714561462\n",
            "Epoch 1938: train loss: 0.1252513825893402\n",
            "Epoch 1939: train loss: 0.12523408234119415\n",
            "Epoch 1940: train loss: 0.12521764636039734\n",
            "Epoch 1941: train loss: 0.1252005249261856\n",
            "Epoch 1942: train loss: 0.12518320977687836\n",
            "Epoch 1943: train loss: 0.12516646087169647\n",
            "Epoch 1944: train loss: 0.1251496821641922\n",
            "Epoch 1945: train loss: 0.12513233721256256\n",
            "Epoch 1946: train loss: 0.12511536478996277\n",
            "Epoch 1947: train loss: 0.12509888410568237\n",
            "Epoch 1948: train loss: 0.12508150935173035\n",
            "Epoch 1949: train loss: 0.1250644475221634\n",
            "Epoch 1950: train loss: 0.12504810094833374\n",
            "Epoch 1951: train loss: 0.12503083050251007\n",
            "Epoch 1952: train loss: 0.12501363456249237\n",
            "Epoch 1953: train loss: 0.1249973326921463\n",
            "Epoch 1954: train loss: 0.12498030066490173\n",
            "Epoch 1955: train loss: 0.12496302276849747\n",
            "Epoch 1956: train loss: 0.12494644522666931\n",
            "Epoch 1957: train loss: 0.1249297708272934\n",
            "Epoch 1958: train loss: 0.12491253018379211\n",
            "Epoch 1959: train loss: 0.12489569932222366\n",
            "Epoch 1960: train loss: 0.12487926334142685\n",
            "Epoch 1961: train loss: 0.12486205995082855\n",
            "Epoch 1962: train loss: 0.12484505027532578\n",
            "Epoch 1963: train loss: 0.12482884526252747\n",
            "Epoch 1964: train loss: 0.12481165677309036\n",
            "Epoch 1965: train loss: 0.12479452788829803\n",
            "Epoch 1966: train loss: 0.12477830052375793\n",
            "Epoch 1967: train loss: 0.12476137280464172\n",
            "Epoch 1968: train loss: 0.12474420666694641\n",
            "Epoch 1969: train loss: 0.12472770363092422\n",
            "Epoch 1970: train loss: 0.12471120804548264\n",
            "Epoch 1971: train loss: 0.12469401955604553\n",
            "Epoch 1972: train loss: 0.12467721849679947\n",
            "Epoch 1973: train loss: 0.12466093897819519\n",
            "Epoch 1974: train loss: 0.12464383989572525\n",
            "Epoch 1975: train loss: 0.12462690472602844\n",
            "Epoch 1976: train loss: 0.12461081147193909\n",
            "Epoch 1977: train loss: 0.12459377199411392\n",
            "Epoch 1978: train loss: 0.12457673996686935\n",
            "Epoch 1979: train loss: 0.12456059455871582\n",
            "Epoch 1980: train loss: 0.12454379349946976\n",
            "Epoch 1981: train loss: 0.124526746571064\n",
            "Epoch 1982: train loss: 0.12451031059026718\n",
            "Epoch 1983: train loss: 0.12449381500482559\n",
            "Epoch 1984: train loss: 0.12447681277990341\n",
            "Epoch 1985: train loss: 0.1244601234793663\n",
            "Epoch 1986: train loss: 0.12444394826889038\n",
            "Epoch 1987: train loss: 0.12442690879106522\n",
            "Epoch 1988: train loss: 0.12441009283065796\n",
            "Epoch 1989: train loss: 0.12439408898353577\n",
            "Epoch 1990: train loss: 0.12437710165977478\n",
            "Epoch 1991: train loss: 0.12436015903949738\n",
            "Epoch 1992: train loss: 0.1243441104888916\n",
            "Epoch 1993: train loss: 0.12432746589183807\n",
            "Epoch 1994: train loss: 0.12431051582098007\n",
            "Epoch 1995: train loss: 0.12429411709308624\n",
            "Epoch 1996: train loss: 0.12427782267332077\n",
            "Epoch 1997: train loss: 0.12426087260246277\n",
            "Epoch 1998: train loss: 0.12424425035715103\n",
            "Epoch 1999: train loss: 0.12422817945480347\n",
            "Epoch 2000: train loss: 0.12421127408742905\n",
            "Epoch 2001: train loss: 0.12419447302818298\n",
            "Epoch 2002: train loss: 0.12417864054441452\n",
            "Epoch 2003: train loss: 0.12416178733110428\n",
            "Epoch 2004: train loss: 0.12414500117301941\n",
            "Epoch 2005: train loss: 0.12412892282009125\n",
            "Epoch 2006: train loss: 0.12411241233348846\n",
            "Epoch 2007: train loss: 0.12409557402133942\n",
            "Epoch 2008: train loss: 0.12407924979925156\n",
            "Epoch 2009: train loss: 0.12406308203935623\n",
            "Epoch 2010: train loss: 0.12404622882604599\n",
            "Epoch 2011: train loss: 0.1240297257900238\n",
            "Epoch 2012: train loss: 0.12401372939348221\n",
            "Epoch 2013: train loss: 0.12399695068597794\n",
            "Epoch 2014: train loss: 0.12398019433021545\n",
            "Epoch 2015: train loss: 0.12396445870399475\n",
            "Epoch 2016: train loss: 0.12394768744707108\n",
            "Epoch 2017: train loss: 0.12393093854188919\n",
            "Epoch 2018: train loss: 0.12391502410173416\n",
            "Epoch 2019: train loss: 0.12389864772558212\n",
            "Epoch 2020: train loss: 0.12388196587562561\n",
            "Epoch 2021: train loss: 0.12386563420295715\n",
            "Epoch 2022: train loss: 0.12384960800409317\n",
            "Epoch 2023: train loss: 0.1238328292965889\n",
            "Epoch 2024: train loss: 0.12381637841463089\n",
            "Epoch 2025: train loss: 0.12380056083202362\n",
            "Epoch 2026: train loss: 0.12378382682800293\n",
            "Epoch 2027: train loss: 0.12376722693443298\n",
            "Epoch 2028: train loss: 0.12375150620937347\n",
            "Epoch 2029: train loss: 0.1237349733710289\n",
            "Epoch 2030: train loss: 0.12371832877397537\n",
            "Epoch 2031: train loss: 0.12370229512453079\n",
            "Epoch 2032: train loss: 0.12368616461753845\n",
            "Epoch 2033: train loss: 0.12366948276758194\n",
            "Epoch 2034: train loss: 0.12365326285362244\n",
            "Epoch 2035: train loss: 0.12363733351230621\n",
            "Epoch 2036: train loss: 0.12362071871757507\n",
            "Epoch 2037: train loss: 0.12360426038503647\n",
            "Epoch 2038: train loss: 0.12358862161636353\n",
            "Epoch 2039: train loss: 0.12357201427221298\n",
            "Epoch 2040: train loss: 0.123555488884449\n",
            "Epoch 2041: train loss: 0.12353982776403427\n",
            "Epoch 2042: train loss: 0.12352344393730164\n",
            "Epoch 2043: train loss: 0.12350684404373169\n",
            "Epoch 2044: train loss: 0.12349085509777069\n",
            "Epoch 2045: train loss: 0.12347486615180969\n",
            "Epoch 2046: train loss: 0.12345831096172333\n",
            "Epoch 2047: train loss: 0.12344209849834442\n",
            "Epoch 2048: train loss: 0.12342637032270432\n",
            "Epoch 2049: train loss: 0.12340981513261795\n",
            "Epoch 2050: train loss: 0.12339343875646591\n",
            "Epoch 2051: train loss: 0.12337792664766312\n",
            "Epoch 2052: train loss: 0.12336139380931854\n",
            "Epoch 2053: train loss: 0.12334494292736053\n",
            "Epoch 2054: train loss: 0.12332937866449356\n",
            "Epoch 2055: train loss: 0.12331311404705048\n",
            "Epoch 2056: train loss: 0.12329667061567307\n",
            "Epoch 2057: train loss: 0.12328076362609863\n",
            "Epoch 2058: train loss: 0.12326482683420181\n",
            "Epoch 2059: train loss: 0.1232483834028244\n",
            "Epoch 2060: train loss: 0.12323223799467087\n",
            "Epoch 2061: train loss: 0.12321662157773972\n",
            "Epoch 2062: train loss: 0.1232001781463623\n",
            "Epoch 2063: train loss: 0.12318380177021027\n",
            "Epoch 2064: train loss: 0.12316849082708359\n",
            "Epoch 2065: train loss: 0.12315203994512558\n",
            "Epoch 2066: train loss: 0.12313570827245712\n",
            "Epoch 2067: train loss: 0.12312014400959015\n",
            "Epoch 2068: train loss: 0.1231040358543396\n",
            "Epoch 2069: train loss: 0.12308768182992935\n",
            "Epoch 2070: train loss: 0.12307174503803253\n",
            "Epoch 2071: train loss: 0.12305609881877899\n",
            "Epoch 2072: train loss: 0.12303968518972397\n",
            "Epoch 2073: train loss: 0.12302354723215103\n",
            "Epoch 2074: train loss: 0.12300807982683182\n",
            "Epoch 2075: train loss: 0.12299176305532455\n",
            "Epoch 2076: train loss: 0.12297549843788147\n",
            "Epoch 2077: train loss: 0.12296011298894882\n",
            "Epoch 2078: train loss: 0.12294398248195648\n",
            "Epoch 2079: train loss: 0.1229276955127716\n",
            "Epoch 2080: train loss: 0.12291201949119568\n",
            "Epoch 2081: train loss: 0.12289620190858841\n",
            "Epoch 2082: train loss: 0.12287994474172592\n",
            "Epoch 2083: train loss: 0.12286405265331268\n",
            "Epoch 2084: train loss: 0.12284853309392929\n",
            "Epoch 2085: train loss: 0.12283226102590561\n",
            "Epoch 2086: train loss: 0.12281613051891327\n",
            "Epoch 2087: train loss: 0.1228008046746254\n",
            "Epoch 2088: train loss: 0.1227845773100853\n",
            "Epoch 2089: train loss: 0.1227683424949646\n",
            "Epoch 2090: train loss: 0.12275303900241852\n",
            "Epoch 2091: train loss: 0.12273707985877991\n",
            "Epoch 2092: train loss: 0.1227208599448204\n",
            "Epoch 2093: train loss: 0.12270519137382507\n",
            "Epoch 2094: train loss: 0.1226896122097969\n",
            "Epoch 2095: train loss: 0.12267334759235382\n",
            "Epoch 2096: train loss: 0.12265747040510178\n",
            "Epoch 2097: train loss: 0.1226421445608139\n",
            "Epoch 2098: train loss: 0.12262599170207977\n",
            "Epoch 2099: train loss: 0.12260988354682922\n",
            "Epoch 2100: train loss: 0.12259473651647568\n",
            "Epoch 2101: train loss: 0.12257855385541916\n",
            "Epoch 2102: train loss: 0.12256250530481339\n",
            "Epoch 2103: train loss: 0.12254712730646133\n",
            "Epoch 2104: train loss: 0.12253130227327347\n",
            "Epoch 2105: train loss: 0.12251526862382889\n",
            "Epoch 2106: train loss: 0.12249956279993057\n",
            "Epoch 2107: train loss: 0.12248414754867554\n",
            "Epoch 2108: train loss: 0.1224680244922638\n",
            "Epoch 2109: train loss: 0.12245211005210876\n",
            "Epoch 2110: train loss: 0.12243692576885223\n",
            "Epoch 2111: train loss: 0.12242086231708527\n",
            "Epoch 2112: train loss: 0.12240485846996307\n",
            "Epoch 2113: train loss: 0.12238969653844833\n",
            "Epoch 2114: train loss: 0.12237385660409927\n",
            "Epoch 2115: train loss: 0.12235784530639648\n",
            "Epoch 2116: train loss: 0.12234235554933548\n",
            "Epoch 2117: train loss: 0.12232684344053268\n",
            "Epoch 2118: train loss: 0.1223108097910881\n",
            "Epoch 2119: train loss: 0.12229511141777039\n",
            "Epoch 2120: train loss: 0.12227991223335266\n",
            "Epoch 2121: train loss: 0.12226388603448868\n",
            "Epoch 2122: train loss: 0.12224797904491425\n",
            "Epoch 2123: train loss: 0.12223301082849503\n",
            "Epoch 2124: train loss: 0.12221704423427582\n",
            "Epoch 2125: train loss: 0.12220106273889542\n",
            "Epoch 2126: train loss: 0.12218587845563889\n",
            "Epoch 2127: train loss: 0.12217017263174057\n",
            "Epoch 2128: train loss: 0.12215431034564972\n",
            "Epoch 2129: train loss: 0.12213881313800812\n",
            "Epoch 2130: train loss: 0.12212350219488144\n",
            "Epoch 2131: train loss: 0.12210755050182343\n",
            "Epoch 2132: train loss: 0.1220918670296669\n",
            "Epoch 2133: train loss: 0.1220768541097641\n",
            "Epoch 2134: train loss: 0.12206088751554489\n",
            "Epoch 2135: train loss: 0.12204508483409882\n",
            "Epoch 2136: train loss: 0.122030109167099\n",
            "Epoch 2137: train loss: 0.12201438844203949\n",
            "Epoch 2138: train loss: 0.12199845910072327\n",
            "Epoch 2139: train loss: 0.12198321521282196\n",
            "Epoch 2140: train loss: 0.1219678521156311\n",
            "Epoch 2141: train loss: 0.12195195257663727\n",
            "Epoch 2142: train loss: 0.12193641811609268\n",
            "Epoch 2143: train loss: 0.1219213604927063\n",
            "Epoch 2144: train loss: 0.12190552055835724\n",
            "Epoch 2145: train loss: 0.12188978493213654\n",
            "Epoch 2146: train loss: 0.12187492102384567\n",
            "Epoch 2147: train loss: 0.1218590959906578\n",
            "Epoch 2148: train loss: 0.12184331566095352\n",
            "Epoch 2149: train loss: 0.12182831019163132\n",
            "Epoch 2150: train loss: 0.12181279808282852\n",
            "Epoch 2151: train loss: 0.12179698795080185\n",
            "Epoch 2152: train loss: 0.12178169935941696\n",
            "Epoch 2153: train loss: 0.12176654487848282\n",
            "Epoch 2154: train loss: 0.12175078690052032\n",
            "Epoch 2155: train loss: 0.12173523008823395\n",
            "Epoch 2156: train loss: 0.1217203438282013\n",
            "Epoch 2157: train loss: 0.12170460820198059\n",
            "Epoch 2158: train loss: 0.12168888002634048\n",
            "Epoch 2159: train loss: 0.12167402356863022\n",
            "Epoch 2160: train loss: 0.12165846675634384\n",
            "Epoch 2161: train loss: 0.1216428205370903\n",
            "Epoch 2162: train loss: 0.12162759900093079\n",
            "Epoch 2163: train loss: 0.12161245942115784\n",
            "Epoch 2164: train loss: 0.12159673869609833\n",
            "Epoch 2165: train loss: 0.12158134579658508\n",
            "Epoch 2166: train loss: 0.12156646698713303\n",
            "Epoch 2167: train loss: 0.12155075371265411\n",
            "Epoch 2168: train loss: 0.12153512984514236\n",
            "Epoch 2169: train loss: 0.12152045965194702\n",
            "Epoch 2170: train loss: 0.1215047538280487\n",
            "Epoch 2171: train loss: 0.12148915976285934\n",
            "Epoch 2172: train loss: 0.12147440761327744\n",
            "Epoch 2173: train loss: 0.1214589849114418\n",
            "Epoch 2174: train loss: 0.12144334614276886\n",
            "Epoch 2175: train loss: 0.12142817676067352\n",
            "Epoch 2176: train loss: 0.12141323834657669\n",
            "Epoch 2177: train loss: 0.12139760702848434\n",
            "Epoch 2178: train loss: 0.12138210982084274\n",
            "Epoch 2179: train loss: 0.12136747688055038\n",
            "Epoch 2180: train loss: 0.12135189026594162\n",
            "Epoch 2181: train loss: 0.12133632600307465\n",
            "Epoch 2182: train loss: 0.12132161855697632\n",
            "Epoch 2183: train loss: 0.12130630761384964\n",
            "Epoch 2184: train loss: 0.12129072844982147\n",
            "Epoch 2185: train loss: 0.12127569317817688\n",
            "Epoch 2186: train loss: 0.12126066535711288\n",
            "Epoch 2187: train loss: 0.12124510109424591\n",
            "Epoch 2188: train loss: 0.12122981250286102\n",
            "Epoch 2189: train loss: 0.12121513485908508\n",
            "Epoch 2190: train loss: 0.12119957059621811\n",
            "Epoch 2191: train loss: 0.12118411064147949\n",
            "Epoch 2192: train loss: 0.1211695596575737\n",
            "Epoch 2193: train loss: 0.12115418165922165\n",
            "Epoch 2194: train loss: 0.12113872170448303\n",
            "Epoch 2195: train loss: 0.12112385034561157\n",
            "Epoch 2196: train loss: 0.12110885977745056\n",
            "Epoch 2197: train loss: 0.12109331786632538\n",
            "Epoch 2198: train loss: 0.12107820808887482\n",
            "Epoch 2199: train loss: 0.1210634782910347\n",
            "Epoch 2200: train loss: 0.1210479736328125\n",
            "Epoch 2201: train loss: 0.12103265523910522\n",
            "Epoch 2202: train loss: 0.1210181787610054\n",
            "Epoch 2203: train loss: 0.12100274860858917\n",
            "Epoch 2204: train loss: 0.12098734080791473\n",
            "Epoch 2205: train loss: 0.12097273021936417\n",
            "Epoch 2206: train loss: 0.1209576353430748\n",
            "Epoch 2207: train loss: 0.12094217538833618\n",
            "Epoch 2208: train loss: 0.12092722952365875\n",
            "Epoch 2209: train loss: 0.12091245502233505\n",
            "Epoch 2210: train loss: 0.12089701741933823\n",
            "Epoch 2211: train loss: 0.1208818331360817\n",
            "Epoch 2212: train loss: 0.12086734920740128\n",
            "Epoch 2213: train loss: 0.12085197120904922\n",
            "Epoch 2214: train loss: 0.12083667516708374\n",
            "Epoch 2215: train loss: 0.12082214653491974\n",
            "Epoch 2216: train loss: 0.12080702930688858\n",
            "Epoch 2217: train loss: 0.1207917109131813\n",
            "Epoch 2218: train loss: 0.12077688425779343\n",
            "Epoch 2219: train loss: 0.12076214700937271\n",
            "Epoch 2220: train loss: 0.12074678391218185\n",
            "Epoch 2221: train loss: 0.12073173373937607\n",
            "Epoch 2222: train loss: 0.12071724236011505\n",
            "Epoch 2223: train loss: 0.12070189416408539\n",
            "Epoch 2224: train loss: 0.12068662792444229\n",
            "Epoch 2225: train loss: 0.1206723004579544\n",
            "Epoch 2226: train loss: 0.12065713852643967\n",
            "Epoch 2227: train loss: 0.12064185738563538\n",
            "Epoch 2228: train loss: 0.12062718719244003\n",
            "Epoch 2229: train loss: 0.12061241269111633\n",
            "Epoch 2230: train loss: 0.12059715390205383\n",
            "Epoch 2231: train loss: 0.12058225274085999\n",
            "Epoch 2232: train loss: 0.12056776881217957\n",
            "Epoch 2233: train loss: 0.1205524355173111\n",
            "Epoch 2234: train loss: 0.12053731828927994\n",
            "Epoch 2235: train loss: 0.12052304297685623\n",
            "Epoch 2236: train loss: 0.12050782144069672\n",
            "Epoch 2237: train loss: 0.12049266695976257\n",
            "Epoch 2238: train loss: 0.12047819793224335\n",
            "Epoch 2239: train loss: 0.12046337127685547\n",
            "Epoch 2240: train loss: 0.12044815719127655\n",
            "Epoch 2241: train loss: 0.12043341994285583\n",
            "Epoch 2242: train loss: 0.12041889131069183\n",
            "Epoch 2243: train loss: 0.12040369212627411\n",
            "Epoch 2244: train loss: 0.1203887015581131\n",
            "Epoch 2245: train loss: 0.1203744113445282\n",
            "Epoch 2246: train loss: 0.12035924196243286\n",
            "Epoch 2247: train loss: 0.12034416198730469\n",
            "Epoch 2248: train loss: 0.1203298345208168\n",
            "Epoch 2249: train loss: 0.12031496316194534\n",
            "Epoch 2250: train loss: 0.12029983848333359\n",
            "Epoch 2251: train loss: 0.12028518319129944\n",
            "Epoch 2252: train loss: 0.12027069926261902\n",
            "Epoch 2253: train loss: 0.12025555968284607\n",
            "Epoch 2254: train loss: 0.12024065852165222\n",
            "Epoch 2255: train loss: 0.1202264279127121\n",
            "Epoch 2256: train loss: 0.12021136283874512\n",
            "Epoch 2257: train loss: 0.12019628286361694\n",
            "Epoch 2258: train loss: 0.12018214166164398\n",
            "Epoch 2259: train loss: 0.12016721069812775\n",
            "Epoch 2260: train loss: 0.12015216797590256\n",
            "Epoch 2261: train loss: 0.12013766169548035\n",
            "Epoch 2262: train loss: 0.12012315541505814\n",
            "Epoch 2263: train loss: 0.12010810524225235\n",
            "Epoch 2264: train loss: 0.12009328603744507\n",
            "Epoch 2265: train loss: 0.12007904052734375\n",
            "Epoch 2266: train loss: 0.12006402760744095\n",
            "Epoch 2267: train loss: 0.12004904448986053\n",
            "Epoch 2268: train loss: 0.12003497034311295\n",
            "Epoch 2269: train loss: 0.12002008408308029\n",
            "Epoch 2270: train loss: 0.12000510096549988\n",
            "Epoch 2271: train loss: 0.11999072879552841\n",
            "Epoch 2272: train loss: 0.11997619271278381\n",
            "Epoch 2273: train loss: 0.1199611946940422\n",
            "Epoch 2274: train loss: 0.11994659155607224\n",
            "Epoch 2275: train loss: 0.1199323758482933\n",
            "Epoch 2276: train loss: 0.11991739273071289\n",
            "Epoch 2277: train loss: 0.11990252137184143\n",
            "Epoch 2278: train loss: 0.11988848447799683\n",
            "Epoch 2279: train loss: 0.1198735460639\n",
            "Epoch 2280: train loss: 0.11985869705677032\n",
            "Epoch 2281: train loss: 0.11984450370073318\n",
            "Epoch 2282: train loss: 0.11982988566160202\n",
            "Epoch 2283: train loss: 0.11981500685214996\n",
            "Epoch 2284: train loss: 0.11980044096708298\n",
            "Epoch 2285: train loss: 0.11978622525930405\n",
            "Epoch 2286: train loss: 0.11977134644985199\n",
            "Epoch 2287: train loss: 0.11975657194852829\n",
            "Epoch 2288: train loss: 0.11974263936281204\n",
            "Epoch 2289: train loss: 0.119727723300457\n",
            "Epoch 2290: train loss: 0.11971290409564972\n",
            "Epoch 2291: train loss: 0.11969883739948273\n",
            "Epoch 2292: train loss: 0.11968425661325455\n",
            "Epoch 2293: train loss: 0.1196693703532219\n",
            "Epoch 2294: train loss: 0.11965500563383102\n",
            "Epoch 2295: train loss: 0.11964073777198792\n",
            "Epoch 2296: train loss: 0.11962593346834183\n",
            "Epoch 2297: train loss: 0.11961130052804947\n",
            "Epoch 2298: train loss: 0.11959736049175262\n",
            "Epoch 2299: train loss: 0.11958247423171997\n",
            "Epoch 2300: train loss: 0.11956777423620224\n",
            "Epoch 2301: train loss: 0.11955380439758301\n",
            "Epoch 2302: train loss: 0.11953922361135483\n",
            "Epoch 2303: train loss: 0.11952444911003113\n",
            "Epoch 2304: train loss: 0.11951013654470444\n",
            "Epoch 2305: train loss: 0.11949591338634491\n",
            "Epoch 2306: train loss: 0.11948121339082718\n",
            "Epoch 2307: train loss: 0.11946660280227661\n",
            "Epoch 2308: train loss: 0.11945267766714096\n",
            "Epoch 2309: train loss: 0.11943795531988144\n",
            "Epoch 2310: train loss: 0.11942324787378311\n",
            "Epoch 2311: train loss: 0.11940936744213104\n",
            "Epoch 2312: train loss: 0.11939484626054764\n",
            "Epoch 2313: train loss: 0.11938012391328812\n",
            "Epoch 2314: train loss: 0.11936589330434799\n",
            "Epoch 2315: train loss: 0.11935176700353622\n",
            "Epoch 2316: train loss: 0.1193370595574379\n",
            "Epoch 2317: train loss: 0.11932260543107986\n",
            "Epoch 2318: train loss: 0.1193087100982666\n",
            "Epoch 2319: train loss: 0.1192939504981041\n",
            "Epoch 2320: train loss: 0.11927934736013412\n",
            "Epoch 2321: train loss: 0.11926554888486862\n",
            "Epoch 2322: train loss: 0.11925111711025238\n",
            "Epoch 2323: train loss: 0.11923643946647644\n",
            "Epoch 2324: train loss: 0.11922229081392288\n",
            "Epoch 2325: train loss: 0.11920809000730515\n",
            "Epoch 2326: train loss: 0.11919347941875458\n",
            "Epoch 2327: train loss: 0.1191791445016861\n",
            "Epoch 2328: train loss: 0.11916525661945343\n",
            "Epoch 2329: train loss: 0.11915066093206406\n",
            "Epoch 2330: train loss: 0.1191360354423523\n",
            "Epoch 2331: train loss: 0.11912231147289276\n",
            "Epoch 2332: train loss: 0.11910790205001831\n",
            "Epoch 2333: train loss: 0.11909332871437073\n",
            "Epoch 2334: train loss: 0.11907926201820374\n",
            "Epoch 2335: train loss: 0.11906515806913376\n",
            "Epoch 2336: train loss: 0.11905059218406677\n",
            "Epoch 2337: train loss: 0.11903629451990128\n",
            "Epoch 2338: train loss: 0.11902248114347458\n",
            "Epoch 2339: train loss: 0.11900792270898819\n",
            "Epoch 2340: train loss: 0.11899345368146896\n",
            "Epoch 2341: train loss: 0.11897978186607361\n",
            "Epoch 2342: train loss: 0.11896532773971558\n",
            "Epoch 2343: train loss: 0.11895086616277695\n",
            "Epoch 2344: train loss: 0.11893682181835175\n",
            "Epoch 2345: train loss: 0.11892279982566833\n",
            "Epoch 2346: train loss: 0.11890825629234314\n",
            "Epoch 2347: train loss: 0.11889399588108063\n",
            "Epoch 2348: train loss: 0.11888029426336288\n",
            "Epoch 2349: train loss: 0.11886577308177948\n",
            "Epoch 2350: train loss: 0.11885132640600204\n",
            "Epoch 2351: train loss: 0.11883778870105743\n",
            "Epoch 2352: train loss: 0.11882341653108597\n",
            "Epoch 2353: train loss: 0.11880892515182495\n",
            "Epoch 2354: train loss: 0.11879506707191467\n",
            "Epoch 2355: train loss: 0.11878105252981186\n",
            "Epoch 2356: train loss: 0.11876658350229263\n",
            "Epoch 2357: train loss: 0.11875244975090027\n",
            "Epoch 2358: train loss: 0.11873874068260193\n",
            "Epoch 2359: train loss: 0.11872424930334091\n",
            "Epoch 2360: train loss: 0.11870989948511124\n",
            "Epoch 2361: train loss: 0.118696428835392\n",
            "Epoch 2362: train loss: 0.11868198215961456\n",
            "Epoch 2363: train loss: 0.11866762489080429\n",
            "Epoch 2364: train loss: 0.11865391582250595\n",
            "Epoch 2365: train loss: 0.11863984912633896\n",
            "Epoch 2366: train loss: 0.11862543970346451\n",
            "Epoch 2367: train loss: 0.1186114251613617\n",
            "Epoch 2368: train loss: 0.11859772354364395\n",
            "Epoch 2369: train loss: 0.11858335137367249\n",
            "Epoch 2370: train loss: 0.1185690388083458\n",
            "Epoch 2371: train loss: 0.11855562031269073\n",
            "Epoch 2372: train loss: 0.11854126304388046\n",
            "Epoch 2373: train loss: 0.11852693557739258\n",
            "Epoch 2374: train loss: 0.118513323366642\n",
            "Epoch 2375: train loss: 0.118499256670475\n",
            "Epoch 2376: train loss: 0.11848494410514832\n",
            "Epoch 2377: train loss: 0.11847101151943207\n",
            "Epoch 2378: train loss: 0.11845732480287552\n",
            "Epoch 2379: train loss: 0.11844301223754883\n",
            "Epoch 2380: train loss: 0.11842881143093109\n",
            "Epoch 2381: train loss: 0.11841542273759842\n",
            "Epoch 2382: train loss: 0.11840112507343292\n",
            "Epoch 2383: train loss: 0.11838685721158981\n",
            "Epoch 2384: train loss: 0.11837330460548401\n",
            "Epoch 2385: train loss: 0.11835935711860657\n",
            "Epoch 2386: train loss: 0.11834508925676346\n",
            "Epoch 2387: train loss: 0.11833114922046661\n",
            "Epoch 2388: train loss: 0.11831756681203842\n",
            "Epoch 2389: train loss: 0.11830329895019531\n",
            "Epoch 2390: train loss: 0.11828913539648056\n",
            "Epoch 2391: train loss: 0.11827583611011505\n",
            "Epoch 2392: train loss: 0.11826154589653015\n",
            "Epoch 2393: train loss: 0.11824739724397659\n",
            "Epoch 2394: train loss: 0.11823388934135437\n",
            "Epoch 2395: train loss: 0.11821992695331573\n",
            "Epoch 2396: train loss: 0.11820574849843979\n",
            "Epoch 2397: train loss: 0.1181919127702713\n",
            "Epoch 2398: train loss: 0.11817839741706848\n",
            "Epoch 2399: train loss: 0.11816416680812836\n",
            "Epoch 2400: train loss: 0.11815005540847778\n",
            "Epoch 2401: train loss: 0.11813679337501526\n",
            "Epoch 2402: train loss: 0.11812267452478409\n",
            "Epoch 2403: train loss: 0.11810851842164993\n",
            "Epoch 2404: train loss: 0.1180950179696083\n",
            "Epoch 2405: train loss: 0.1180812418460846\n",
            "Epoch 2406: train loss: 0.11806706339120865\n",
            "Epoch 2407: train loss: 0.11805328726768494\n",
            "Epoch 2408: train loss: 0.11803977191448212\n",
            "Epoch 2409: train loss: 0.11802564561367035\n",
            "Epoch 2410: train loss: 0.11801154166460037\n",
            "Epoch 2411: train loss: 0.1179983913898468\n",
            "Epoch 2412: train loss: 0.11798425763845444\n",
            "Epoch 2413: train loss: 0.11797018349170685\n",
            "Epoch 2414: train loss: 0.1179567500948906\n",
            "Epoch 2415: train loss: 0.11794301122426987\n",
            "Epoch 2416: train loss: 0.1179288700222969\n",
            "Epoch 2417: train loss: 0.11791513115167618\n",
            "Epoch 2418: train loss: 0.11790172755718231\n",
            "Epoch 2419: train loss: 0.11788769066333771\n",
            "Epoch 2420: train loss: 0.1178736463189125\n",
            "Epoch 2421: train loss: 0.11786055564880371\n",
            "Epoch 2422: train loss: 0.11784639209508896\n",
            "Epoch 2423: train loss: 0.11783246695995331\n",
            "Epoch 2424: train loss: 0.11781907826662064\n",
            "Epoch 2425: train loss: 0.11780541390180588\n",
            "Epoch 2426: train loss: 0.11779134720563889\n",
            "Epoch 2427: train loss: 0.11777765303850174\n",
            "Epoch 2428: train loss: 0.11776430159807205\n",
            "Epoch 2429: train loss: 0.11775027215480804\n",
            "Epoch 2430: train loss: 0.1177363395690918\n",
            "Epoch 2431: train loss: 0.11772321909666061\n",
            "Epoch 2432: train loss: 0.11770924925804138\n",
            "Epoch 2433: train loss: 0.11769530177116394\n",
            "Epoch 2434: train loss: 0.11768194288015366\n",
            "Epoch 2435: train loss: 0.11766832321882248\n",
            "Epoch 2436: train loss: 0.11765436083078384\n",
            "Epoch 2437: train loss: 0.11764071136713028\n",
            "Epoch 2438: train loss: 0.11762744188308716\n",
            "Epoch 2439: train loss: 0.11761350929737091\n",
            "Epoch 2440: train loss: 0.11759955435991287\n",
            "Epoch 2441: train loss: 0.11758647114038467\n",
            "Epoch 2442: train loss: 0.11757269501686096\n",
            "Epoch 2443: train loss: 0.1175587847828865\n",
            "Epoch 2444: train loss: 0.11754536628723145\n",
            "Epoch 2445: train loss: 0.1175319105386734\n",
            "Epoch 2446: train loss: 0.11751798540353775\n",
            "Epoch 2447: train loss: 0.11750432103872299\n",
            "Epoch 2448: train loss: 0.11749117821455002\n",
            "Epoch 2449: train loss: 0.11747723817825317\n",
            "Epoch 2450: train loss: 0.11746339499950409\n",
            "Epoch 2451: train loss: 0.11745034903287888\n",
            "Epoch 2452: train loss: 0.11743663996458054\n",
            "Epoch 2453: train loss: 0.11742281168699265\n",
            "Epoch 2454: train loss: 0.11740937829017639\n",
            "Epoch 2455: train loss: 0.1173960417509079\n",
            "Epoch 2456: train loss: 0.11738219857215881\n",
            "Epoch 2457: train loss: 0.11736852675676346\n",
            "Epoch 2458: train loss: 0.11735541373491287\n",
            "Epoch 2459: train loss: 0.11734159290790558\n",
            "Epoch 2460: train loss: 0.11732782423496246\n",
            "Epoch 2461: train loss: 0.11731479316949844\n",
            "Epoch 2462: train loss: 0.11730112880468369\n",
            "Epoch 2463: train loss: 0.11728730797767639\n",
            "Epoch 2464: train loss: 0.1172739714384079\n",
            "Epoch 2465: train loss: 0.11726066470146179\n",
            "Epoch 2466: train loss: 0.11724689602851868\n",
            "Epoch 2467: train loss: 0.1172332689166069\n",
            "Epoch 2468: train loss: 0.11722033470869064\n",
            "Epoch 2469: train loss: 0.11720647662878036\n",
            "Epoch 2470: train loss: 0.11719280481338501\n",
            "Epoch 2471: train loss: 0.11717977374792099\n",
            "Epoch 2472: train loss: 0.11716623604297638\n",
            "Epoch 2473: train loss: 0.11715252697467804\n",
            "Epoch 2474: train loss: 0.11713913828134537\n",
            "Epoch 2475: train loss: 0.11712595820426941\n",
            "Epoch 2476: train loss: 0.11711219698190689\n",
            "Epoch 2477: train loss: 0.11709862947463989\n",
            "Epoch 2478: train loss: 0.1170857846736908\n",
            "Epoch 2479: train loss: 0.11707199364900589\n",
            "Epoch 2480: train loss: 0.11705831438302994\n",
            "Epoch 2481: train loss: 0.11704530566930771\n",
            "Epoch 2482: train loss: 0.11703184247016907\n",
            "Epoch 2483: train loss: 0.11701814830303192\n",
            "Epoch 2484: train loss: 0.11700484901666641\n",
            "Epoch 2485: train loss: 0.11699175834655762\n",
            "Epoch 2486: train loss: 0.1169780045747757\n",
            "Epoch 2487: train loss: 0.11696451902389526\n",
            "Epoch 2488: train loss: 0.11695167422294617\n",
            "Epoch 2489: train loss: 0.11693798005580902\n",
            "Epoch 2490: train loss: 0.11692441999912262\n",
            "Epoch 2491: train loss: 0.11691144108772278\n",
            "Epoch 2492: train loss: 0.1168980523943901\n",
            "Epoch 2493: train loss: 0.11688443273305893\n",
            "Epoch 2494: train loss: 0.11687109619379044\n",
            "Epoch 2495: train loss: 0.1168581172823906\n",
            "Epoch 2496: train loss: 0.11684448271989822\n",
            "Epoch 2497: train loss: 0.11683093756437302\n",
            "Epoch 2498: train loss: 0.11681825667619705\n",
            "Epoch 2499: train loss: 0.11680460721254349\n",
            "Epoch 2500: train loss: 0.11679105460643768\n",
            "Epoch 2501: train loss: 0.11677813529968262\n",
            "Epoch 2502: train loss: 0.11676482856273651\n",
            "Epoch 2503: train loss: 0.11675119400024414\n",
            "Epoch 2504: train loss: 0.1167379766702652\n",
            "Epoch 2505: train loss: 0.11672504246234894\n",
            "Epoch 2506: train loss: 0.11671148240566254\n",
            "Epoch 2507: train loss: 0.11669792979955673\n",
            "Epoch 2508: train loss: 0.11668525636196136\n",
            "Epoch 2509: train loss: 0.11667180061340332\n",
            "Epoch 2510: train loss: 0.1166582703590393\n",
            "Epoch 2511: train loss: 0.1166451945900917\n",
            "Epoch 2512: train loss: 0.11663217842578888\n",
            "Epoch 2513: train loss: 0.11661861091852188\n",
            "Epoch 2514: train loss: 0.11660528182983398\n",
            "Epoch 2515: train loss: 0.11659257113933563\n",
            "Epoch 2516: train loss: 0.11657898873090744\n",
            "Epoch 2517: train loss: 0.11656557023525238\n",
            "Epoch 2518: train loss: 0.11655278503894806\n",
            "Epoch 2519: train loss: 0.11653950065374374\n",
            "Epoch 2520: train loss: 0.11652601510286331\n",
            "Epoch 2521: train loss: 0.11651292443275452\n",
            "Epoch 2522: train loss: 0.11650000512599945\n",
            "Epoch 2523: train loss: 0.11648651957511902\n",
            "Epoch 2524: train loss: 0.1164732351899147\n",
            "Epoch 2525: train loss: 0.11646055430173874\n",
            "Epoch 2526: train loss: 0.11644703149795532\n",
            "Epoch 2527: train loss: 0.11643366515636444\n",
            "Epoch 2528: train loss: 0.1164209395647049\n",
            "Epoch 2529: train loss: 0.11640772223472595\n",
            "Epoch 2530: train loss: 0.11639432609081268\n",
            "Epoch 2531: train loss: 0.11638124287128448\n",
            "Epoch 2532: train loss: 0.11636842787265778\n",
            "Epoch 2533: train loss: 0.11635494232177734\n",
            "Epoch 2534: train loss: 0.11634165048599243\n",
            "Epoch 2535: train loss: 0.11632915586233139\n",
            "Epoch 2536: train loss: 0.11631566286087036\n",
            "Epoch 2537: train loss: 0.11630238592624664\n",
            "Epoch 2538: train loss: 0.11628962308168411\n",
            "Epoch 2539: train loss: 0.11627650260925293\n",
            "Epoch 2540: train loss: 0.11626319587230682\n",
            "Epoch 2541: train loss: 0.11625004559755325\n",
            "Epoch 2542: train loss: 0.11623737215995789\n",
            "Epoch 2543: train loss: 0.11622399091720581\n",
            "Epoch 2544: train loss: 0.1162106990814209\n",
            "Epoch 2545: train loss: 0.11619815975427628\n",
            "Epoch 2546: train loss: 0.11618494987487793\n",
            "Epoch 2547: train loss: 0.11617157608270645\n",
            "Epoch 2548: train loss: 0.11615873128175735\n",
            "Epoch 2549: train loss: 0.11614588648080826\n",
            "Epoch 2550: train loss: 0.11613255739212036\n",
            "Epoch 2551: train loss: 0.11611941456794739\n",
            "Epoch 2552: train loss: 0.11610688269138336\n",
            "Epoch 2553: train loss: 0.11609352380037308\n",
            "Epoch 2554: train loss: 0.11608027666807175\n",
            "Epoch 2555: train loss: 0.1160677820444107\n",
            "Epoch 2556: train loss: 0.11605462431907654\n",
            "Epoch 2557: train loss: 0.11604135483503342\n",
            "Epoch 2558: train loss: 0.11602848768234253\n",
            "Epoch 2559: train loss: 0.11601573973894119\n",
            "Epoch 2560: train loss: 0.11600250005722046\n",
            "Epoch 2561: train loss: 0.1159893125295639\n",
            "Epoch 2562: train loss: 0.11597684770822525\n",
            "Epoch 2563: train loss: 0.11596358567476273\n",
            "Epoch 2564: train loss: 0.11595037579536438\n",
            "Epoch 2565: train loss: 0.11593787372112274\n",
            "Epoch 2566: train loss: 0.11592483520507812\n",
            "Epoch 2567: train loss: 0.11591163277626038\n",
            "Epoch 2568: train loss: 0.1158987432718277\n",
            "Epoch 2569: train loss: 0.11588616669178009\n",
            "Epoch 2570: train loss: 0.11587293446063995\n",
            "Epoch 2571: train loss: 0.11585979163646698\n",
            "Epoch 2572: train loss: 0.11584741622209549\n",
            "Epoch 2573: train loss: 0.11583422869443893\n",
            "Epoch 2574: train loss: 0.11582101881504059\n",
            "Epoch 2575: train loss: 0.11580849438905716\n",
            "Epoch 2576: train loss: 0.11579561233520508\n",
            "Epoch 2577: train loss: 0.11578246206045151\n",
            "Epoch 2578: train loss: 0.11576958745718002\n",
            "Epoch 2579: train loss: 0.11575710028409958\n",
            "Epoch 2580: train loss: 0.11574387550354004\n",
            "Epoch 2581: train loss: 0.11573076248168945\n",
            "Epoch 2582: train loss: 0.11571843177080154\n",
            "Epoch 2583: train loss: 0.11570537835359573\n",
            "Epoch 2584: train loss: 0.11569228023290634\n",
            "Epoch 2585: train loss: 0.11567966639995575\n",
            "Epoch 2586: train loss: 0.11566697061061859\n",
            "Epoch 2587: train loss: 0.11565384268760681\n",
            "Epoch 2588: train loss: 0.11564087122678757\n",
            "Epoch 2589: train loss: 0.11562851816415787\n",
            "Epoch 2590: train loss: 0.11561540514230728\n",
            "Epoch 2591: train loss: 0.11560229957103729\n",
            "Epoch 2592: train loss: 0.11558988690376282\n",
            "Epoch 2593: train loss: 0.11557714641094208\n",
            "Epoch 2594: train loss: 0.11556404083967209\n",
            "Epoch 2595: train loss: 0.11555129289627075\n",
            "Epoch 2596: train loss: 0.11553879082202911\n",
            "Epoch 2597: train loss: 0.11552569270133972\n",
            "Epoch 2598: train loss: 0.11551269888877869\n",
            "Epoch 2599: train loss: 0.11550047993659973\n",
            "Epoch 2600: train loss: 0.11548744887113571\n",
            "Epoch 2601: train loss: 0.1154744029045105\n",
            "Epoch 2602: train loss: 0.11546197533607483\n",
            "Epoch 2603: train loss: 0.11544925719499588\n",
            "Epoch 2604: train loss: 0.11543618887662888\n",
            "Epoch 2605: train loss: 0.11542344093322754\n",
            "Epoch 2606: train loss: 0.11541114747524261\n",
            "Epoch 2607: train loss: 0.11539805680513382\n",
            "Epoch 2608: train loss: 0.11538513004779816\n",
            "Epoch 2609: train loss: 0.11537289619445801\n",
            "Epoch 2610: train loss: 0.11536005139350891\n",
            "Epoch 2611: train loss: 0.11534705758094788\n",
            "Epoch 2612: train loss: 0.11533453315496445\n",
            "Epoch 2613: train loss: 0.11532201617956161\n",
            "Epoch 2614: train loss: 0.11530900746583939\n",
            "Epoch 2615: train loss: 0.11529622972011566\n",
            "Epoch 2616: train loss: 0.11528397351503372\n",
            "Epoch 2617: train loss: 0.11527102440595627\n",
            "Epoch 2618: train loss: 0.11525804549455643\n",
            "Epoch 2619: train loss: 0.11524582654237747\n",
            "Epoch 2620: train loss: 0.11523307859897614\n",
            "Epoch 2621: train loss: 0.11522020399570465\n",
            "Epoch 2622: train loss: 0.11520759761333466\n",
            "Epoch 2623: train loss: 0.11519526690244675\n",
            "Epoch 2624: train loss: 0.1151822954416275\n",
            "Epoch 2625: train loss: 0.1151694506406784\n",
            "Epoch 2626: train loss: 0.11515737324953079\n",
            "Epoch 2627: train loss: 0.11514446139335632\n",
            "Epoch 2628: train loss: 0.11513155698776245\n",
            "Epoch 2629: train loss: 0.11511934548616409\n",
            "Epoch 2630: train loss: 0.11510668694972992\n",
            "Epoch 2631: train loss: 0.11509387195110321\n",
            "Epoch 2632: train loss: 0.11508124321699142\n",
            "Epoch 2633: train loss: 0.11506898701190948\n",
            "Epoch 2634: train loss: 0.1150561049580574\n",
            "Epoch 2635: train loss: 0.1150432899594307\n",
            "Epoch 2636: train loss: 0.1150311678647995\n",
            "Epoch 2637: train loss: 0.11501846462488174\n",
            "Epoch 2638: train loss: 0.11500559002161026\n",
            "Epoch 2639: train loss: 0.11499318480491638\n",
            "Epoch 2640: train loss: 0.11498089134693146\n",
            "Epoch 2641: train loss: 0.11496801674365997\n",
            "Epoch 2642: train loss: 0.11495532095432281\n",
            "Epoch 2643: train loss: 0.11494327336549759\n",
            "Epoch 2644: train loss: 0.1149304211139679\n",
            "Epoch 2645: train loss: 0.11491761356592178\n",
            "Epoch 2646: train loss: 0.11490549147129059\n",
            "Epoch 2647: train loss: 0.11489298194646835\n",
            "Epoch 2648: train loss: 0.11488012969493866\n",
            "Epoch 2649: train loss: 0.1148676648736\n",
            "Epoch 2650: train loss: 0.11485543847084045\n",
            "Epoch 2651: train loss: 0.11484268307685852\n",
            "Epoch 2652: train loss: 0.11482992768287659\n",
            "Epoch 2653: train loss: 0.1148180291056633\n",
            "Epoch 2654: train loss: 0.1148051917552948\n",
            "Epoch 2655: train loss: 0.11479248106479645\n",
            "Epoch 2656: train loss: 0.11478033661842346\n",
            "Epoch 2657: train loss: 0.11476786434650421\n",
            "Epoch 2658: train loss: 0.11475513130426407\n",
            "Epoch 2659: train loss: 0.11474262177944183\n",
            "Epoch 2660: train loss: 0.11473055928945541\n",
            "Epoch 2661: train loss: 0.1147177591919899\n",
            "Epoch 2662: train loss: 0.1147051453590393\n",
            "Epoch 2663: train loss: 0.11469317227602005\n",
            "Epoch 2664: train loss: 0.11468061804771423\n",
            "Epoch 2665: train loss: 0.1146678626537323\n",
            "Epoch 2666: train loss: 0.11465555429458618\n",
            "Epoch 2667: train loss: 0.1146434023976326\n",
            "Epoch 2668: train loss: 0.11463067680597305\n",
            "Epoch 2669: train loss: 0.11461803317070007\n",
            "Epoch 2670: train loss: 0.11460617184638977\n",
            "Epoch 2671: train loss: 0.11459346115589142\n",
            "Epoch 2672: train loss: 0.11458084732294083\n",
            "Epoch 2673: train loss: 0.1145687997341156\n",
            "Epoch 2674: train loss: 0.1145564392209053\n",
            "Epoch 2675: train loss: 0.11454376578330994\n",
            "Epoch 2676: train loss: 0.11453137546777725\n",
            "Epoch 2677: train loss: 0.11451932787895203\n",
            "Epoch 2678: train loss: 0.11450668424367905\n",
            "Epoch 2679: train loss: 0.11449406296014786\n",
            "Epoch 2680: train loss: 0.11448218673467636\n",
            "Epoch 2681: train loss: 0.11446978896856308\n",
            "Epoch 2682: train loss: 0.1144571304321289\n",
            "Epoch 2683: train loss: 0.11444490402936935\n",
            "Epoch 2684: train loss: 0.11443275213241577\n",
            "Epoch 2685: train loss: 0.114420086145401\n",
            "Epoch 2686: train loss: 0.11440766602754593\n",
            "Epoch 2687: train loss: 0.11439581215381622\n",
            "Epoch 2688: train loss: 0.11438321322202682\n",
            "Epoch 2689: train loss: 0.11437065154314041\n",
            "Epoch 2690: train loss: 0.11435875296592712\n",
            "Epoch 2691: train loss: 0.114346444606781\n",
            "Epoch 2692: train loss: 0.1143338605761528\n",
            "Epoch 2693: train loss: 0.11432155966758728\n",
            "Epoch 2694: train loss: 0.11430957168340683\n",
            "Epoch 2695: train loss: 0.1142970472574234\n",
            "Epoch 2696: train loss: 0.11428448557853699\n",
            "Epoch 2697: train loss: 0.11427276581525803\n",
            "Epoch 2698: train loss: 0.11426030844449997\n",
            "Epoch 2699: train loss: 0.11424773931503296\n",
            "Epoch 2700: train loss: 0.11423568427562714\n",
            "Epoch 2701: train loss: 0.11422364413738251\n",
            "Epoch 2702: train loss: 0.11421104520559311\n",
            "Epoch 2703: train loss: 0.11419868469238281\n",
            "Epoch 2704: train loss: 0.11418690532445908\n",
            "Epoch 2705: train loss: 0.11417440325021744\n",
            "Epoch 2706: train loss: 0.11416193842887878\n",
            "Epoch 2707: train loss: 0.11415006965398788\n",
            "Epoch 2708: train loss: 0.11413782089948654\n",
            "Epoch 2709: train loss: 0.1141253262758255\n",
            "Epoch 2710: train loss: 0.11411316692829132\n",
            "Epoch 2711: train loss: 0.11410127580165863\n",
            "Epoch 2712: train loss: 0.11408879607915878\n",
            "Epoch 2713: train loss: 0.11407633125782013\n",
            "Epoch 2714: train loss: 0.11406463384628296\n",
            "Epoch 2715: train loss: 0.11405226588249207\n",
            "Epoch 2716: train loss: 0.11403986811637878\n",
            "Epoch 2717: train loss: 0.11402782797813416\n",
            "Epoch 2718: train loss: 0.1140158548951149\n",
            "Epoch 2719: train loss: 0.11400341242551804\n",
            "Epoch 2720: train loss: 0.11399112641811371\n",
            "Epoch 2721: train loss: 0.11397942900657654\n",
            "Epoch 2722: train loss: 0.11396697908639908\n",
            "Epoch 2723: train loss: 0.11395453661680222\n",
            "Epoch 2724: train loss: 0.11394285410642624\n",
            "Epoch 2725: train loss: 0.11393062770366669\n",
            "Epoch 2726: train loss: 0.11391820758581161\n",
            "Epoch 2727: train loss: 0.11390615254640579\n",
            "Epoch 2728: train loss: 0.11389435827732086\n",
            "Epoch 2729: train loss: 0.113881915807724\n",
            "Epoch 2730: train loss: 0.1138695552945137\n",
            "Epoch 2731: train loss: 0.11385796219110489\n",
            "Epoch 2732: train loss: 0.11384568363428116\n",
            "Epoch 2733: train loss: 0.11383330076932907\n",
            "Epoch 2734: train loss: 0.11382140964269638\n",
            "Epoch 2735: train loss: 0.11380947381258011\n",
            "Epoch 2736: train loss: 0.11379709839820862\n",
            "Epoch 2737: train loss: 0.11378493160009384\n",
            "Epoch 2738: train loss: 0.11377328634262085\n",
            "Epoch 2739: train loss: 0.11376094818115234\n",
            "Epoch 2740: train loss: 0.11374863982200623\n",
            "Epoch 2741: train loss: 0.11373691260814667\n",
            "Epoch 2742: train loss: 0.11372484266757965\n",
            "Epoch 2743: train loss: 0.11371252685785294\n",
            "Epoch 2744: train loss: 0.1137005165219307\n",
            "Epoch 2745: train loss: 0.11368881165981293\n",
            "Epoch 2746: train loss: 0.11367650330066681\n",
            "Epoch 2747: train loss: 0.11366420984268188\n",
            "Epoch 2748: train loss: 0.11365270614624023\n",
            "Epoch 2749: train loss: 0.11364053934812546\n",
            "Epoch 2750: train loss: 0.11362822353839874\n",
            "Epoch 2751: train loss: 0.11361633241176605\n",
            "Epoch 2752: train loss: 0.11360447853803635\n",
            "Epoch 2753: train loss: 0.1135922446846962\n",
            "Epoch 2754: train loss: 0.1135801300406456\n",
            "Epoch 2755: train loss: 0.11356856673955917\n",
            "Epoch 2756: train loss: 0.11355631798505783\n",
            "Epoch 2757: train loss: 0.11354409158229828\n",
            "Epoch 2758: train loss: 0.11353245377540588\n",
            "Epoch 2759: train loss: 0.11352048814296722\n",
            "Epoch 2760: train loss: 0.1135082021355629\n",
            "Epoch 2761: train loss: 0.11349626630544662\n",
            "Epoch 2762: train loss: 0.11348466575145721\n",
            "Epoch 2763: train loss: 0.11347240209579468\n",
            "Epoch 2764: train loss: 0.11346019804477692\n",
            "Epoch 2765: train loss: 0.11344879865646362\n",
            "Epoch 2766: train loss: 0.11343669891357422\n",
            "Epoch 2767: train loss: 0.11342441290616989\n",
            "Epoch 2768: train loss: 0.11341270804405212\n",
            "Epoch 2769: train loss: 0.11340094357728958\n",
            "Epoch 2770: train loss: 0.11338873207569122\n",
            "Epoch 2771: train loss: 0.11337663978338242\n",
            "Epoch 2772: train loss: 0.11336524039506912\n",
            "Epoch 2773: train loss: 0.11335304379463196\n",
            "Epoch 2774: train loss: 0.11334091424942017\n",
            "Epoch 2775: train loss: 0.11332936584949493\n",
            "Epoch 2776: train loss: 0.11331744492053986\n",
            "Epoch 2777: train loss: 0.11330528557300568\n",
            "Epoch 2778: train loss: 0.11329341679811478\n",
            "Epoch 2779: train loss: 0.11328182369470596\n",
            "Epoch 2780: train loss: 0.11326967179775238\n",
            "Epoch 2781: train loss: 0.11325758695602417\n",
            "Epoch 2782: train loss: 0.11324618011713028\n",
            "Epoch 2783: train loss: 0.11323420703411102\n",
            "Epoch 2784: train loss: 0.11322206258773804\n",
            "Epoch 2785: train loss: 0.11321031302213669\n",
            "Epoch 2786: train loss: 0.11319874972105026\n",
            "Epoch 2787: train loss: 0.1131865531206131\n",
            "Epoch 2788: train loss: 0.11317454278469086\n",
            "Epoch 2789: train loss: 0.11316327750682831\n",
            "Epoch 2790: train loss: 0.11315114051103592\n",
            "Epoch 2791: train loss: 0.1131390780210495\n",
            "Epoch 2792: train loss: 0.11312759667634964\n",
            "Epoch 2793: train loss: 0.11311576515436172\n",
            "Epoch 2794: train loss: 0.1131037026643753\n",
            "Epoch 2795: train loss: 0.1130918487906456\n",
            "Epoch 2796: train loss: 0.1130804643034935\n",
            "Epoch 2797: train loss: 0.11306830495595932\n",
            "Epoch 2798: train loss: 0.11305632442235947\n",
            "Epoch 2799: train loss: 0.11304500699043274\n",
            "Epoch 2800: train loss: 0.11303309351205826\n",
            "Epoch 2801: train loss: 0.11302103847265244\n",
            "Epoch 2802: train loss: 0.11300932615995407\n",
            "Epoch 2803: train loss: 0.112997867166996\n",
            "Epoch 2804: train loss: 0.11298578977584839\n",
            "Epoch 2805: train loss: 0.11297383904457092\n",
            "Epoch 2806: train loss: 0.11296264827251434\n",
            "Epoch 2807: train loss: 0.11295056343078613\n",
            "Epoch 2808: train loss: 0.11293859779834747\n",
            "Epoch 2809: train loss: 0.11292713135480881\n",
            "Epoch 2810: train loss: 0.11291547119617462\n",
            "Epoch 2811: train loss: 0.11290349811315536\n",
            "Epoch 2812: train loss: 0.11289173364639282\n",
            "Epoch 2813: train loss: 0.11288037151098251\n",
            "Epoch 2814: train loss: 0.11286834627389908\n",
            "Epoch 2815: train loss: 0.11285638809204102\n",
            "Epoch 2816: train loss: 0.11284514516592026\n",
            "Epoch 2817: train loss: 0.11283336579799652\n",
            "Epoch 2818: train loss: 0.11282137036323547\n",
            "Epoch 2819: train loss: 0.11280975490808487\n",
            "Epoch 2820: train loss: 0.11279834806919098\n",
            "Epoch 2821: train loss: 0.11278633773326874\n",
            "Epoch 2822: train loss: 0.11277439445257187\n",
            "Epoch 2823: train loss: 0.11276330053806305\n",
            "Epoch 2824: train loss: 0.11275147646665573\n",
            "Epoch 2825: train loss: 0.11273950338363647\n",
            "Epoch 2826: train loss: 0.11272800713777542\n",
            "Epoch 2827: train loss: 0.11271660029888153\n",
            "Epoch 2828: train loss: 0.11270460486412048\n",
            "Epoch 2829: train loss: 0.11269278824329376\n",
            "Epoch 2830: train loss: 0.11268169432878494\n",
            "Epoch 2831: train loss: 0.11266973614692688\n",
            "Epoch 2832: train loss: 0.11265787482261658\n",
            "Epoch 2833: train loss: 0.11264655739068985\n",
            "Epoch 2834: train loss: 0.11263490468263626\n",
            "Epoch 2835: train loss: 0.11262302100658417\n",
            "Epoch 2836: train loss: 0.11261140555143356\n",
            "Epoch 2837: train loss: 0.11260015517473221\n",
            "Epoch 2838: train loss: 0.11258824169635773\n",
            "Epoch 2839: train loss: 0.11257641762495041\n",
            "Epoch 2840: train loss: 0.11256524175405502\n",
            "Epoch 2841: train loss: 0.11255359649658203\n",
            "Epoch 2842: train loss: 0.11254172027111053\n",
            "Epoch 2843: train loss: 0.1125301942229271\n",
            "Epoch 2844: train loss: 0.11251889169216156\n",
            "Epoch 2845: train loss: 0.11250703036785126\n",
            "Epoch 2846: train loss: 0.11249525845050812\n",
            "Epoch 2847: train loss: 0.11248421669006348\n",
            "Epoch 2848: train loss: 0.11247240006923676\n",
            "Epoch 2849: train loss: 0.11246056854724884\n",
            "Epoch 2850: train loss: 0.11244934797286987\n",
            "Epoch 2851: train loss: 0.11243779212236404\n",
            "Epoch 2852: train loss: 0.11242599785327911\n",
            "Epoch 2853: train loss: 0.1124143972992897\n",
            "Epoch 2854: train loss: 0.1124032512307167\n",
            "Epoch 2855: train loss: 0.11239147186279297\n",
            "Epoch 2856: train loss: 0.11237968504428864\n",
            "Epoch 2857: train loss: 0.11236853897571564\n",
            "Epoch 2858: train loss: 0.11235703527927399\n",
            "Epoch 2859: train loss: 0.11234521865844727\n",
            "Epoch 2860: train loss: 0.11233371496200562\n",
            "Epoch 2861: train loss: 0.11232253164052963\n",
            "Epoch 2862: train loss: 0.11231078207492828\n",
            "Epoch 2863: train loss: 0.11229905486106873\n",
            "Epoch 2864: train loss: 0.11228799819946289\n",
            "Epoch 2865: train loss: 0.11227638274431229\n",
            "Epoch 2866: train loss: 0.11226460337638855\n",
            "Epoch 2867: train loss: 0.11225324869155884\n",
            "Epoch 2868: train loss: 0.11224202811717987\n",
            "Epoch 2869: train loss: 0.11223027110099792\n",
            "Epoch 2870: train loss: 0.11221861839294434\n",
            "Epoch 2871: train loss: 0.11220769584178925\n",
            "Epoch 2872: train loss: 0.11219590157270432\n",
            "Epoch 2873: train loss: 0.11218419671058655\n",
            "Epoch 2874: train loss: 0.1121731549501419\n",
            "Epoch 2875: train loss: 0.11216171085834503\n",
            "Epoch 2876: train loss: 0.11214999854564667\n",
            "Epoch 2877: train loss: 0.1121385470032692\n",
            "Epoch 2878: train loss: 0.11212751269340515\n",
            "Epoch 2879: train loss: 0.1121157556772232\n",
            "Epoch 2880: train loss: 0.11210408061742783\n",
            "Epoch 2881: train loss: 0.11209313571453094\n",
            "Epoch 2882: train loss: 0.11208165436983109\n",
            "Epoch 2883: train loss: 0.11206997185945511\n",
            "Epoch 2884: train loss: 0.11205863952636719\n",
            "Epoch 2885: train loss: 0.11204753816127777\n",
            "Epoch 2886: train loss: 0.11203581094741821\n",
            "Epoch 2887: train loss: 0.1120242178440094\n",
            "Epoch 2888: train loss: 0.1120133027434349\n",
            "Epoch 2889: train loss: 0.11200179159641266\n",
            "Epoch 2890: train loss: 0.11199015378952026\n",
            "Epoch 2891: train loss: 0.11197885870933533\n",
            "Epoch 2892: train loss: 0.11196770519018173\n",
            "Epoch 2893: train loss: 0.11195605993270874\n",
            "Epoch 2894: train loss: 0.11194457858800888\n",
            "Epoch 2895: train loss: 0.11193373054265976\n",
            "Epoch 2896: train loss: 0.11192211508750916\n",
            "Epoch 2897: train loss: 0.11191046983003616\n",
            "Epoch 2898: train loss: 0.1118994876742363\n",
            "Epoch 2899: train loss: 0.11188817024230957\n",
            "Epoch 2900: train loss: 0.11187656223773956\n",
            "Epoch 2901: train loss: 0.11186515539884567\n",
            "Epoch 2902: train loss: 0.11185428500175476\n",
            "Epoch 2903: train loss: 0.11184269189834595\n",
            "Epoch 2904: train loss: 0.11183108389377594\n",
            "Epoch 2905: train loss: 0.11182022839784622\n",
            "Epoch 2906: train loss: 0.11180883646011353\n",
            "Epoch 2907: train loss: 0.11179722100496292\n",
            "Epoch 2908: train loss: 0.11178595572710037\n",
            "Epoch 2909: train loss: 0.11177504807710648\n",
            "Epoch 2910: train loss: 0.11176340281963348\n",
            "Epoch 2911: train loss: 0.11175190657377243\n",
            "Epoch 2912: train loss: 0.11174117028713226\n",
            "Epoch 2913: train loss: 0.11172971874475479\n",
            "Epoch 2914: train loss: 0.11171817034482956\n",
            "Epoch 2915: train loss: 0.11170700192451477\n",
            "Epoch 2916: train loss: 0.11169596761465073\n",
            "Epoch 2917: train loss: 0.11168442666530609\n",
            "Epoch 2918: train loss: 0.11167304962873459\n",
            "Epoch 2919: train loss: 0.11166224628686905\n",
            "Epoch 2920: train loss: 0.1116507351398468\n",
            "Epoch 2921: train loss: 0.11163929849863052\n",
            "Epoch 2922: train loss: 0.11162839829921722\n",
            "Epoch 2923: train loss: 0.11161716282367706\n",
            "Epoch 2924: train loss: 0.1116056814789772\n",
            "Epoch 2925: train loss: 0.11159443110227585\n",
            "Epoch 2926: train loss: 0.11158356815576553\n",
            "Epoch 2927: train loss: 0.11157205700874329\n",
            "Epoch 2928: train loss: 0.11156062036752701\n",
            "Epoch 2929: train loss: 0.11154982447624207\n",
            "Epoch 2930: train loss: 0.11153855919837952\n",
            "Epoch 2931: train loss: 0.11152710020542145\n",
            "Epoch 2932: train loss: 0.11151596158742905\n",
            "Epoch 2933: train loss: 0.11150506883859634\n",
            "Epoch 2934: train loss: 0.11149358004331589\n",
            "Epoch 2935: train loss: 0.11148220300674438\n",
            "Epoch 2936: train loss: 0.1114715039730072\n",
            "Epoch 2937: train loss: 0.11146014928817749\n",
            "Epoch 2938: train loss: 0.111448734998703\n",
            "Epoch 2939: train loss: 0.11143770068883896\n",
            "Epoch 2940: train loss: 0.11142678558826447\n",
            "Epoch 2941: train loss: 0.11141536384820938\n",
            "Epoch 2942: train loss: 0.11140400171279907\n",
            "Epoch 2943: train loss: 0.11139336228370667\n",
            "Epoch 2944: train loss: 0.1113819032907486\n",
            "Epoch 2945: train loss: 0.11137059330940247\n",
            "Epoch 2946: train loss: 0.11135977506637573\n",
            "Epoch 2947: train loss: 0.1113486960530281\n",
            "Epoch 2948: train loss: 0.11133724451065063\n",
            "Epoch 2949: train loss: 0.11132610589265823\n",
            "Epoch 2950: train loss: 0.11131536215543747\n",
            "Epoch 2951: train loss: 0.11130397021770477\n",
            "Epoch 2952: train loss: 0.11129264533519745\n",
            "Epoch 2953: train loss: 0.11128190904855728\n",
            "Epoch 2954: train loss: 0.1112707257270813\n",
            "Epoch 2955: train loss: 0.11125940084457397\n",
            "Epoch 2956: train loss: 0.11124835163354874\n",
            "Epoch 2957: train loss: 0.11123762279748917\n",
            "Epoch 2958: train loss: 0.11122622340917587\n",
            "Epoch 2959: train loss: 0.11121493577957153\n",
            "Epoch 2960: train loss: 0.11120423674583435\n",
            "Epoch 2961: train loss: 0.11119313538074493\n",
            "Epoch 2962: train loss: 0.1111818253993988\n",
            "Epoch 2963: train loss: 0.11117084324359894\n",
            "Epoch 2964: train loss: 0.111160047352314\n",
            "Epoch 2965: train loss: 0.11114872246980667\n",
            "Epoch 2966: train loss: 0.11113739013671875\n",
            "Epoch 2967: train loss: 0.11112692952156067\n",
            "Epoch 2968: train loss: 0.11111563444137573\n",
            "Epoch 2969: train loss: 0.1111043393611908\n",
            "Epoch 2970: train loss: 0.11109358817338943\n",
            "Epoch 2971: train loss: 0.11108264327049255\n",
            "Epoch 2972: train loss: 0.11107130348682404\n",
            "Epoch 2973: train loss: 0.11106014251708984\n",
            "Epoch 2974: train loss: 0.11104967445135117\n",
            "Epoch 2975: train loss: 0.11103834956884384\n",
            "Epoch 2976: train loss: 0.11102715134620667\n",
            "Epoch 2977: train loss: 0.11101648211479187\n",
            "Epoch 2978: train loss: 0.11100547760725021\n",
            "Epoch 2979: train loss: 0.11099422723054886\n",
            "Epoch 2980: train loss: 0.11098324507474899\n",
            "Epoch 2981: train loss: 0.11097259819507599\n",
            "Epoch 2982: train loss: 0.11096134781837463\n",
            "Epoch 2983: train loss: 0.11095015704631805\n",
            "Epoch 2984: train loss: 0.11093953996896744\n",
            "Epoch 2985: train loss: 0.11092853546142578\n",
            "Epoch 2986: train loss: 0.11091732233762741\n",
            "Epoch 2987: train loss: 0.11090636998414993\n",
            "Epoch 2988: train loss: 0.11089567840099335\n",
            "Epoch 2989: train loss: 0.11088452488183975\n",
            "Epoch 2990: train loss: 0.11087336391210556\n",
            "Epoch 2991: train loss: 0.1108628436923027\n",
            "Epoch 2992: train loss: 0.11085178703069687\n",
            "Epoch 2993: train loss: 0.11084063351154327\n",
            "Epoch 2994: train loss: 0.11082978546619415\n",
            "Epoch 2995: train loss: 0.11081910878419876\n",
            "Epoch 2996: train loss: 0.1108078882098198\n",
            "Epoch 2997: train loss: 0.11079677939414978\n",
            "Epoch 2998: train loss: 0.11078637093305588\n",
            "Epoch 2999: train loss: 0.1107751876115799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred = model(x_test)\n",
        "after_train = criterion(y_pred.squeeze(), y_test) \n",
        "print('Test loss after Training' , after_train.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Pc66SL1oRj",
        "outputId": "c9c26ee2-6bda-445d-d0fb-c015ef2e05b8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss after Training 2.377638339996338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Get Activations\n"
      ],
      "metadata": {
        "id": "XhlF5lry1pIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3lVGVVmzBqj",
        "outputId": "7626719a-be23-401b-cf52-5ea1dc2ef246"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.5815, -8.9559])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCUNermw2OFl",
        "outputId": "ea62579f-3b71-4121-b052-ee56e2e6a4b1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input = torch.randn(1, 1, 32, 32)\n",
        "input = x_train[0]\n",
        "out = model(input)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84qBVB8S3Hq_",
        "outputId": "084a627e-013e-4c91-a4cf-f77db4fbd162"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9637], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activations(input, layer_name):\n",
        "    activation = {}\n",
        "    def get_activation(name):\n",
        "        def hook(model, input, output):\n",
        "            activation[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    for name_to_check, layer in model.named_modules():\n",
        "        if name_to_check == layer_name:\n",
        "            break\n",
        "    layer.register_forward_hook(get_activation(layer_name))\n",
        "    \n",
        "    output = model(input)\n",
        "\n",
        "    return activation.copy()  #else will return the same actvs of model"
      ],
      "metadata": {
        "id": "NifeiLth3bGl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_name in layers:\n",
        "    print(get_activations(input, layer_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUqqWN7I0azP",
        "outputId": "9ca711b2-497e-4286-e2cb-2fe1c1440b89"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1': tensor([7.9374, 8.3800])}\n",
            "{'relu': tensor([7.9374, 8.3800])}\n",
            "{'fc2': tensor([3.2781])}\n",
            "{'sigmoid': tensor([0.9637])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Now slightly modify the input and see what happens to each layer!\n",
        "\n",
        "Try different modification levels\n"
      ],
      "metadata": {
        "id": "aARB9Xwa0V8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_2 = x_train[0] + torch.tensor([0.1,0])\n",
        "input_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B30L-rM0ZAB",
        "outputId": "4fa64dc4-0b0d-4967-d243-945487a45aa1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.6815, -8.9559])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_name in layers:\n",
        "    print(get_activations(input_2, layer_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7uJ3bIo2hzL",
        "outputId": "29991dec-06a0-4a90-cb5d-8a9c8d7825a4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1': tensor([7.9596, 8.3923])}\n",
            "{'relu': tensor([7.9596, 8.3923])}\n",
            "{'fc2': tensor([3.2521])}\n",
            "{'sigmoid': tensor([0.9627])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BVw04gwQ2wLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_3 = x_train[0] + torch.tensor([1,0])\n",
        "for layer_name in layers:\n",
        "    print(get_activations(input_3, layer_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stI_JviE2xAn",
        "outputId": "55397655-cdf4-4289-95eb-553cb58ba913"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1': tensor([8.1595, 8.5033])}\n",
            "{'relu': tensor([8.1595, 8.5033])}\n",
            "{'fc2': tensor([3.0181])}\n",
            "{'sigmoid': tensor([0.9534])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find a local motif. This is a local circuit?"
      ],
      "metadata": {
        "id": "sRR8YgAK29ID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Test more simple MLPs of actual datasets. \n",
        "\n",
        "Make dataset with actual correlation\n",
        "\n",
        "https://www.kaggle.com/code/zelongq/simple-mlp-for-titanic-survival/execution\n",
        "\n",
        "https://www.kaggle.com/code/pinocookie/pytorch-simple-mlp/notebook"
      ],
      "metadata": {
        "id": "pjDUrCz34OC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to decompose every single part of a simple but effective MLP. Patience and humility- like the NN, this method is simple, but effective. Identify motifs, and play in a lab of what happens if you combine motifs- what emergence? And mathematically, WHY?\n",
        "\n",
        "This is completely unorthodox but it may be effective."
      ],
      "metadata": {
        "id": "DiCXUjey5_iB"
      }
    }
  ]
}