{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of StyleGAN2-LatentVectors.ipynb","provenance":[{"file_id":"15qj5WXwCBEizN57089bUe3bM5RZ7MEin","timestamp":1661546603183}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"U2RxzI5LeWPh"},"source":["# Advanced StyleGAN Week 3: Latent Vectors\n","\n","Pretty much everything we’re going to do in the next couple of weeks will be about manipulating vectors. So in order to get a better sense of what we’re doing we should better understand exactly what a vector is, how to manipulat them, and the difference between a Z and W vector."]},{"cell_type":"code","metadata":{"id":"CaMKANd0ePGY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661548867298,"user_tz":240,"elapsed":8443,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"fc12a861-ab74-466e-aded-85db0b8c876a"},"source":["!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n","%cd stylegan2-ada-pytorch\n","\n","!pip install ninja"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'stylegan2-ada-pytorch'...\n","remote: Enumerating objects: 128, done.\u001b[K\n","remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n","Receiving objects: 100% (128/128), 1.12 MiB | 4.04 MiB/s, done.\n","Resolving deltas: 100% (57/57), done.\n","/content/stylegan2-ada-pytorch\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ninja\n","  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n","\u001b[K     |████████████████████████████████| 108 kB 4.2 MB/s \n","\u001b[?25hInstalling collected packages: ninja\n","Successfully installed ninja-1.10.2.3\n"]}]},{"cell_type":"markdown","metadata":{"id":"T1ZwnwAxhouT"},"source":["Let’s also download a StyleGAN model file. You can import your own, or there are many to pick on the [Awesome StyleGAN2 Pretrained Model page](https://github.com/justinpinkney/awesome-pretrained-stylegan2)."]},{"cell_type":"code","metadata":{"id":"aAUuv2mKhSh6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661548580615,"user_tz":240,"elapsed":16685,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"120e59c7-b0b3-4838-a0ab-60aa25234b47"},"source":["# !wget http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-cat-config-f.pkl"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-08-26 21:16:04--  http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-cat-config-f.pkl\n","Resolving d36zk2xti64re0.cloudfront.net (d36zk2xti64re0.cloudfront.net)... 18.65.227.14, 18.65.227.160, 18.65.227.32, ...\n","Connecting to d36zk2xti64re0.cloudfront.net (d36zk2xti64re0.cloudfront.net)|18.65.227.14|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 357418027 (341M) [application/x-www-form-urlencoded]\n","Saving to: ‘stylegan2-cat-config-f.pkl’\n","\n","stylegan2-cat-confi 100%[===================>] 340.86M  24.3MB/s    in 15s     \n","\n","2022-08-26 21:16:21 (22.2 MB/s) - ‘stylegan2-cat-config-f.pkl’ saved [357418027/357418027]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"hYj2nDVHH0Z8"},"source":["# this is completely unnecessary b/c you already have cloned it\n","# %cd ../\n","# %mkdir dvschultz\n","# %cd dvschultz\n","# !git clone https://github.com/dvschultz/stylegan2-ada-pytorch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-uOyNKsf8sw"},"source":["Set the path to your .pkl file below"]},{"cell_type":"markdown","metadata":{"id":"id7FJSJxf1bR"},"source":["Let’s import some libraries (some are python libraries, others are from the StyleGAN repo)"]},{"cell_type":"code","metadata":{"id":"8VHkwrNQfeY5"},"source":["import os\n","import re\n","\n","import dnnlib\n","import numpy as np\n","import PIL.Image\n","import torch\n","\n","import legacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pwd"],"metadata":{"id":"PHNPgtUOAMoP"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz9szopvfM61"},"source":["# network_pkl = '/content/stylegan2-ada-pytorch/stylegan2-cat-config-f.pkl'"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')  #brings google drive files to be accessible under /content/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knZ2Ub2QCSas","executionInfo":{"status":"ok","timestamp":1661548993779,"user_tz":240,"elapsed":24449,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"71a30852-4c89-44bb-89f9-28e04a6c41aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["network_pkl = \\\n","'/content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/network-snapshot-000006.pkl'"],"metadata":{"id":"VnmFmWcpBt4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3GWdiYYetmQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661549011382,"user_tz":240,"elapsed":14011,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"48f69c36-5462-4668-f3c2-ea594cdaed5a"},"source":["print('Loading networks from \"%s\"...' % network_pkl)\n","\n","device = torch.device('cuda') # we will use a GPU\n","with dnnlib.util.open_url(network_pkl) as f:\n","    G = legacy.load_network_pkl(f)['G_ema'].to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading networks from \"/content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/network-snapshot-000006.pkl\"...\n"]}]},{"cell_type":"markdown","metadata":{"id":"GXwmnIgngv8h"},"source":["## Generating images from a Z space Vector\n","\n","Now that we’ve loaded our model, we can generate a random vector.\n","\n","`seeds`, as used in the StyleGAN model, refer to a random seed value. This allows us to generate the same random values every time as long as the seed value is the same.\n","\n","`G.z_dim` in most cases is 512 (This can be customized, hence why we pull it directly from the model)"]},{"cell_type":"code","metadata":{"id":"9IM1Xu3bg-hu"},"source":["seed = 20\n","z = np.random.RandomState(seed).randn(1, G.z_dim) \n","\n","print(z)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zpIkZds6jA0f"},"source":["Next, we’ll load this vector into PyTorch."]},{"cell_type":"code","metadata":{"id":"cOvtuvlQi-KL"},"source":["z = torch.from_numpy(z).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7RW204QjMgL"},"source":["Now we can generate an image from the vector"]},{"cell_type":"code","metadata":{"id":"-BBxQ0C-h_qY"},"source":["truncation_psi = 0.7\n","noise_mode = 'const' # 'const', 'random', 'none'\n","outdir = '/content/output/'\n","\n","# make sure our output directory exists\n","os.makedirs(outdir, exist_ok=True)\n","\n","# label is for class-based models. Let's assume we're not doing that here.\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","print(img)\n","img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","print(img)\n","PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5DZ829IiswL"},"source":["## Linear Interpolation\n","\n","Let’s look at how to interpolate between zs.\n","\n","We’ll start by defining a lerp function.\n","\n","`z[0]*t + z[1]*(1-t)` where t is time (or steps between each z)"]},{"cell_type":"code","metadata":{"id":"qrFYDKHXj5fT"},"source":["def lerp(zs, steps):\n","    out = []\n","    for i in range(len(zs)-1):\n","        for index in range(steps):\n","            t = index/float(steps)\n","            out.append(zs[i+1]*t + zs[i]*(1-t))\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UYhBx6etkGLy"},"source":["Now let’s create two z vectors, then create the lerp vectors, then render them all as images:"]},{"cell_type":"code","metadata":{"id":"rcZ7VSYVkCLf"},"source":["z1 = np.random.RandomState(20).randn(1, G.z_dim)\n","z2 = np.random.RandomState(100).randn(1, G.z_dim)\n","\n","frame_zs = lerp([z1,z2], 72)\n","\n","print('how many lerp frames? ',len(frame_zs))\n","\n","outdir = '/content/output-frames/'\n","os.makedirs(outdir, exist_ok=True)\n","\n","# label is still 0\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","for idx, z in enumerate(frame_zs):\n","    z = torch.from_numpy(z).to(device)\n","    print('Generating frame %d/%d' % (idx, len(frame_zs)))\n","    img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","    PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/frame-{idx:04d}.png')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59WbUZQ7lVuj"},"source":["And finally let’s convert it to a video using ffmpeg"]},{"cell_type":"code","metadata":{"id":"UGYMRhqskcU_"},"source":["!ffmpeg -i /content/output-frames/frame-%04d.png -r 24 -vcodec libx264 -pix_fmt yuv420p /content/lerp.mp4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpJ0SBNsmHLN"},"source":["## W space interpolation\n","\n","W space produced less entangled interpolations. A couple notes about the W space:\n","\n","* The process is to take a Z vector, project it to the W space, and then interpolate in the W space.\n","* Interpolating in Z and then converting to W won’t do much. That’s because a specific vector in Z and in W should look exactly the same.\n","* You will often start with a Z vector and project it to the W. I can’t see a reason why you would do the opposite (maybe there’s some reason but it would be an edge case)"]},{"cell_type":"markdown","metadata":{"id":"yJ4dQpeRnNlr"},"source":["So let’s start by making two Z vectors and then converting them to two W vectors."]},{"cell_type":"code","metadata":{"id":"WVFc9wurl7kM"},"source":["z1 = np.random.RandomState(20).randn(1, G.z_dim)\n","z2 = np.random.RandomState(100).randn(1, G.z_dim)\n","\n","zs = [z1,z2]\n","\n","ws = []\n","for z_idx, z in enumerate(zs):\n","    z = torch.from_numpy(z).to(device)\n","    w = G.mapping(z, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","    ws.append(w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zej-5LhfnmCf"},"source":["If a Z vector is 512 dimensions (often shown as `[1, 512]`) then a W vector is multiple \"stacks\" of 512 dimensions. The number of stacks is often dependent on the resolution of the model (it’s also settable in the training config).\n","\n","If you used the cat model that I do in this demo you find it has a shape of `[1, 14, 512]`. A 1024x1024 model is usually `[1, 18, 512]`."]},{"cell_type":"code","metadata":{"id":"ABreV-T3nedb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661548757634,"user_tz":240,"elapsed":212,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8d5e86cd-1dab-4964-8f1d-d78b091001c1"},"source":["print(ws[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.1731,  2.3823,  0.2110,  ..., -0.2688, -0.5693, -0.1355],\n","         [-0.1731,  2.3823,  0.2110,  ..., -0.2688, -0.5693, -0.1355],\n","         [-0.1731,  2.3823,  0.2110,  ..., -0.2688, -0.5693, -0.1355],\n","         ...,\n","         [-0.1890,  3.2042,  0.3283,  ..., -0.4580, -0.7311, -0.1634],\n","         [-0.1890,  3.2042,  0.3283,  ..., -0.4580, -0.7311, -0.1634],\n","         [-0.1890,  3.2042,  0.3283,  ..., -0.4580, -0.7311, -0.1634]]],\n","       device='cuda:0')\n"]}]},{"cell_type":"markdown","metadata":{"id":"Wryr68HQoWy9"},"source":["The lerp code is actually the exact same (thanks numpy!)"]},{"cell_type":"code","metadata":{"id":"QLv6veNroBUp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661548759263,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bd7b8437-e950-40b8-8f80-44e890798ce9"},"source":["frame_ws = lerp(ws, 100)\n","\n","print(len(frame_ws))\n","\n","print(frame_ws[49].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100\n","torch.Size([1, 14, 512])\n"]}]},{"cell_type":"markdown","metadata":{"id":"hTSj-9lapPHI"},"source":["And now we can generate images by using the `G.synthesis` network"]},{"cell_type":"code","metadata":{"id":"0ratxn3MovrK"},"source":["outdir = '/content/output-frames-w/'\n","os.makedirs(outdir, exist_ok=True)\n","\n","for idx, w in enumerate(frame_ws): \n","    img = G.synthesis(w, noise_mode=noise_mode, force_fp32=True)\n","    img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","    PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/frame-{idx:04d}.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ETu0bPvp3ek","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661548768212,"user_tz":240,"elapsed":676,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"333fad15-66e3-4324-f5f2-8882ed84f824"},"source":["!ffmpeg -i /content/output-frames-w/frame-%04d.png -r 24 -vcodec libx264 -pix_fmt yuv420p /content/lerp-w.mp4"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n","  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n","  libavutil      55. 78.100 / 55. 78.100\n","  libavcodec     57.107.100 / 57.107.100\n","  libavformat    57. 83.100 / 57. 83.100\n","  libavdevice    57. 10.100 / 57. 10.100\n","  libavfilter     6.107.100 /  6.107.100\n","  libavresample   3.  7.  0 /  3.  7.  0\n","  libswscale      4.  8.100 /  4.  8.100\n","  libswresample   2.  9.100 /  2.  9.100\n","  libpostproc    54.  7.100 / 54.  7.100\n","Input #0, image2, from '/content/output-frames-w/frame-%04d.png':\n","  Duration: 00:00:04.00, start: 0.000000, bitrate: N/A\n","    Stream #0:0: Video: png, rgb24(pc), 256x256, 25 fps, 25 tbr, 25 tbn, 25 tbc\n","Stream mapping:\n","  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n","Press [q] to stop, [?] for help\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mprofile High, level 1.3\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n","Output #0, mp4, to '/content/lerp-w.mp4':\n","  Metadata:\n","    encoder         : Lavf57.83.100\n","    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 256x256, q=-1--1, 24 fps, 12288 tbn, 24 tbc\n","    Metadata:\n","      encoder         : Lavc57.107.100 libx264\n","    Side data:\n","      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n","\u001b[0;33mPast duration 0.639992 too large\n","\u001b[0m\u001b[0;33mPast duration 0.679985 too large\n","\u001b[0m\u001b[0;33mPast duration 0.719994 too large\n","\u001b[0m\u001b[0;33mPast duration 0.759987 too large\n","\u001b[0m\u001b[0;33mPast duration 0.799995 too large\n","\u001b[0m\u001b[0;33mPast duration 0.839989 too large\n","\u001b[0m\u001b[0;33mPast duration 0.879997 too large\n","\u001b[0m\u001b[0;33mPast duration 0.919991 too large\n","\u001b[0m\u001b[0;33mPast duration 0.959999 too large\n","\u001b[0m\u001b[0;33mPast duration 0.999992 too large\n","\u001b[0mframe=   98 fps=0.0 q=-1.0 Lsize=     238kB time=00:00:03.95 bitrate= 492.6kbits/s dup=0 drop=2 speed=10.6x    \n","video:236kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.655338%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mframe I:1     Avg QP:26.20  size: 12768\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mframe P:71    Avg QP:25.61  size:  2920\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mframe B:26    Avg QP:29.59  size:   823\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mconsecutive B-frames: 62.2%  6.1%  3.1% 28.6%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mmb I  I16..4:  0.0% 64.8% 35.2%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mmb P  I16..4:  0.0%  2.5%  0.6%  P16..4: 48.0% 25.4% 15.1%  0.0%  0.0%    skip: 8.3%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mmb B  I16..4:  0.0%  0.1%  0.1%  B16..8: 17.0%  4.2%  2.5%  direct:10.3%  skip:65.9%  L0:12.0% L1:39.4% BI:48.6%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0m8x8 transform intra:73.9% inter:55.0%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mcoded y,uvDC,uvAC intra: 96.7% 46.4% 9.9% inter: 46.4% 24.6% 0.3%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mi16 v,h,dc,p: 25%  0%  0% 75%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 12% 12% 12%  9% 12%  9% 13%  9% 13%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu:  9% 12% 11% 10% 12%  9% 14% 10% 11%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mi8c dc,h,v,p: 67% 16% 10%  7%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mWeighted P-Frames: Y:35.2% UV:19.7%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mref P L0: 56.3% 35.6%  7.5%  0.4%  0.2%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mref B L0: 97.0%  2.1%  0.9%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mref B L1: 99.7%  0.3%\n","\u001b[1;36m[libx264 @ 0x55f3c0413e00] \u001b[0mkb/s:473.10\n"]}]},{"cell_type":"code","metadata":{"id":"o0XgSZGXqNDC"},"source":[],"execution_count":null,"outputs":[]}]}