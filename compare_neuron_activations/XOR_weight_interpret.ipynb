{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Use a NN to model XOR\n",
        "\n",
        "Study how individual motifs work to compose into larger circuits. Just like how studying AND gate composes into larger computer circuit, or how studying cells and particles enables understanding on a macroscopic level.\n",
        "\n",
        "Not for a research paper, but for personal understanding, which will help for future research papers.\n",
        "\n",
        "XOR is a geometric motif in latent space. Do other such motifs exist?"
      ],
      "metadata": {
        "id": "NgcSEMUjxL64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os "
      ],
      "metadata": {
        "id": "gtIFVSFiemkY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Feedforward(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Feedforward, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size  = hidden_size\n",
        "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)  # one hidden layer\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)  #output layer w/o sigmoid\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        hidden = self.fc1(x)\n",
        "        relu = self.relu(hidden)\n",
        "        output = self.fc2(relu)\n",
        "        output = self.sigmoid(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "PQEESxRaeoLZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Feedforward(2, 2)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "WxWLSjWReokH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor(np.array([[0,0], [0,1], [1,0], [1,1]]))\n",
        "y_train = torch.FloatTensor(np.array([0,1,1,0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4OXVdjofEiE",
        "outputId": "b534ea08-a930-4e27-f8bc-8291a4c7a61c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "epoch = 10000\n",
        "\n",
        "for epoch in range(epoch):\n",
        "    #sets the gradients to zero before we start backpropagation. \n",
        "    #This is a necessary step as PyTorch accumulates the gradients from the backward passes from the previous epochs.\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    y_pred = model(x_train)\n",
        "    # Compute Loss\n",
        "    loss = criterion(y_pred.squeeze(), y_train)\n",
        "   \n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "vEfPg8mXevcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred = model(x_train)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T_uh9aPgbUs",
        "outputId": "3157c016-5f78-4722-f0b2-95550ae73116"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0526],\n",
              "        [0.9852],\n",
              "        [0.9830],\n",
              "        [0.0526]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Get activations\n"
      ],
      "metadata": {
        "id": "XhlF5lry1pIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activations(input, layer_name):\n",
        "    activation = {}\n",
        "    def get_activation(name):\n",
        "        def hook(model, input, output):\n",
        "            activation[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    for name_to_check, layer in model.named_modules():\n",
        "        if name_to_check == layer_name:\n",
        "            break\n",
        "    layer.register_forward_hook(get_activation(layer_name))\n",
        "    \n",
        "    output = model(input)\n",
        "\n",
        "    return activation.copy()  #else will return the same actvs of model"
      ],
      "metadata": {
        "id": "b0Q2cv-Gg2-b"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get previous last layer name\n",
        "named_layers = dict(model.named_modules())\n",
        "layers = list(named_layers.keys())\n",
        "\n",
        "# too many branches, so just get the converged branch points\n",
        "# '' is the entire model, so disregard it\n",
        "# layers = [x for x in layers if '.' not in x and x != '']  \n",
        "layers = [x for x in layers if x != '']  \n",
        "layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsCO7p5ghn6r",
        "outputId": "dc5ff46e-8f80-4343-a4b2-0171a3364335"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fc1', 'relu', 'fc2', 'sigmoid']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Compare different input activations\n",
        "\n",
        "Compare [0, 0] and [1, 0]"
      ],
      "metadata": {
        "id": "fe9CiE2IzobT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PcNkuzIjW_c",
        "outputId": "1eeafb4e-591d-4a9c-d92c-1832c425fd34"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_1 = x_train[0]\n",
        "out = model(input_1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtUduynfh5WV",
        "outputId": "56e525e4-eed1-409f-f25c-d160153e006c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0526], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_name in layers:\n",
        "    print(get_activations(input_1, layer_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgWQBapQhu_2",
        "outputId": "95662034-7ec6-4679-c87c-6eb7b8e83845"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': tensor([0.0526])}\n",
            "{'fc1': tensor([-3.6790e-05, -2.3193e-04])}\n",
            "{'relu': tensor([0., 0.])}\n",
            "{'fc2': tensor([-2.8915])}\n",
            "{'sigmoid': tensor([0.0526])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_2 = x_train[0] + torch.tensor([1,0])\n",
        "input_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B30L-rM0ZAB",
        "outputId": "79f2db11-92bd-48ca-f7a7-070edd27f2df"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_name in layers:\n",
        "    print(get_activations(input_2, layer_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7uJ3bIo2hzL",
        "outputId": "d1e20889-e1e9-432f-8a86-541a1449b7ca"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1': tensor([ 2.2371, -2.2851])}\n",
            "{'relu': tensor([2.2371, 0.0000])}\n",
            "{'fc2': tensor([4.0572])}\n",
            "{'sigmoid': tensor([0.9830])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_name in layers:\n",
        "    print( '[0,0]:', get_activations(input_1, layer_name), '[1,0]:', get_activations(input_2, layer_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaWcNLRT4faK",
        "outputId": "a933746b-fa4e-4a79-b84e-d1c2609b39d3"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0,0]: {'fc1': tensor([-3.6790e-05, -2.3193e-04])} [1,0]: {'fc1': tensor([ 2.2371, -2.2851])}\n",
            "[0,0]: {'relu': tensor([0., 0.])} [1,0]: {'relu': tensor([2.2371, 0.0000])}\n",
            "[0,0]: {'fc2': tensor([-2.8915])} [1,0]: {'fc2': tensor([4.0572])}\n",
            "[0,0]: {'sigmoid': tensor([0.0526])} [1,0]: {'sigmoid': tensor([0.9830])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_name in layers:\n",
        "    diff = get_activations(input_2, layer_name)[layer_name] - get_activations(input_1, layer_name)[layer_name]\n",
        "    print(diff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKvQzZZojQd_",
        "outputId": "d0e7868c-b52f-4f0a-e729-f3de04c9fdc4"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.2371, -2.2849])\n",
            "tensor([2.2371, 0.0000])\n",
            "tensor([6.9488])\n",
            "tensor([0.9304])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Why does this input change cause this difference?\n",
        "\n",
        "Why are the activations different? Let's look at how each activation is calculated, and how the change in input causes the change in activation."
      ],
      "metadata": {
        "id": "HnO3gvvxjPt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EEVG3AxlamN",
        "outputId": "5f0f41d9-7291-4e19-eef7-d22ed486ff62"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Feedforward(\n",
              "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leeiDSBBi1hR",
        "outputId": "5efe3cd8-6947-429c-eacc-338ff1055cea"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 2.2371, -2.2374],\n",
            "        [-2.2849,  2.2847]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-3.6790e-05, -2.3193e-04], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[3.1062, 3.1020]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.8915], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev_weights = model.fc1.weight.data\n",
        "prev_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PJMJ_Fcih8Y",
        "outputId": "2a5ab256-89e6-4389-a090-39fb98d8f82f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.2371, -2.2374],\n",
              "        [-2.2849,  2.2847]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in layers:\n",
        "    if isinstance(named_layers[layer], torch.nn.Linear):\n",
        "        print(layer, named_layers[layer].state_dict()['bias'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Kz7M-IlDVk",
        "outputId": "448e9c7a-5d89-4553-cd60-d36c6f7b6803"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1 tensor([-3.6790e-05, -2.3193e-04])\n",
            "fc2 tensor([-2.8915])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.weight.data * input_1 + named_layers['fc1'].state_dict()['bias']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWGz-fnHkihG",
        "outputId": "76548076-50cd-4ab2-c0a4-cc422fdff499"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.6790e-05, -2.3193e-04],\n",
              "        [-3.6790e-05, -2.3193e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.weight.data * input_2 + named_layers['fc1'].state_dict()['bias']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5SkMAulnMe0",
        "outputId": "f54a5551-6945-43b7-a596-a470b0ad36a9"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.2371e+00, -2.3193e-04],\n",
              "        [-2.2849e+00, -2.3193e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Break down each step of W * X + b"
      ],
      "metadata": {
        "id": "Pg8tBtwXtIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.weight.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8FRJFM2tKdY",
        "outputId": "8542b8ad-69d2-437a-ce00-6ed55f4e7636"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.2371, -2.2374],\n",
              "        [-2.2849,  2.2847]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTuxxnF1tgoF",
        "outputId": "8f652abf-4828-41ca-dbb5-1d53c4c4800b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1000, 0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WX_hadamard = torch.multiply(model.fc1.weight.data, input_2)\n",
        "WX_hadamard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0ZAOwGAsA2e",
        "outputId": "e4898519-4d29-410f-8aab-8d18595e425f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2237, -0.0000],\n",
              "        [-0.2285,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WX:\n",
        "\n",
        "[w1 w2]\n",
        "\n",
        "[w3 w4]\n",
        "\n",
        "w1 * (1) - w2 * 0 = w1\n",
        "\n",
        "w3 * (1) + w4 * 0 = w3\n",
        "\n",
        "Thus, when you change the input from [0, 0] to [1, 1], you are changing WX from [0, 0] to [w1, w3]. And we know that [1, 0] is supposed to be 1.\n",
        "\n",
        "But we also need [1, 1] to be 0, so it's not as simple as \"make it so the neuron outputs are bigger\". "
      ],
      "metadata": {
        "id": "MFSOGb3QtoW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix multiplication is unit conversion. Row of W multiplies by column X"
      ],
      "metadata": {
        "id": "BqRA-AOTq_DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_3 = torch.FloatTensor(np.array([1,1]))\n",
        "input_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOC5MW_xvZgd",
        "outputId": "2d3c6671-e697-4715-fc2d-51d7b6d7f774"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_name in layers:\n",
        "    print(get_activations(input_3, layer_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pl5CPs0vnTY",
        "outputId": "191efa87-e94c-41a9-bee6-2fd3bce8866e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1': tensor([-0.0003, -0.0004])}\n",
            "{'relu': tensor([0., 0.])}\n",
            "{'fc2': tensor([-2.8915])}\n",
            "{'sigmoid': tensor([0.0526])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_name in layers:\n",
        "    diff = get_activations(input_3, layer_name)[layer_name] - get_activations(input_1, layer_name)[layer_name]\n",
        "    print(diff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAYgAl6ev_aw",
        "outputId": "78d950d2-a09b-4a51-c672-7ae9329650b8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0002, -0.0002])\n",
            "tensor([0., 0.])\n",
            "tensor([0.])\n",
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[0, 0] and [1, 1] have the SAME neuron outputs after fc1. Let's look at the weights of fc1 again to see why that's the case."
      ],
      "metadata": {
        "id": "8jhHweq9wRsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.weight.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-HCBi1EwZBr",
        "outputId": "7a511ca2-e01f-4906-cd78-18ed72d57500"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.2371, -2.2374],\n",
              "        [-2.2849,  2.2847]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WX_hadamard = torch.multiply(model.fc1.weight.data, input_3)\n",
        "WX_hadamard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8_u4oI3wnuv",
        "outputId": "8d522220-9418-4a11-f87c-e4f65eea57b7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.2371, -2.2374],\n",
              "        [-2.2849,  2.2847]])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first element of WX is: 2.2371 -2.2374 = -0.002 \n",
        "\n",
        "The second element is also close to 0.\n",
        "\n",
        "So the weights were learned to make sure w1 - w2 was close to 0, as ReLU would turn that into 0.\n",
        "\n",
        "Then in fc2 (output node), the bias is negative so that sigmoid makes it less than 0. But for [0,1] and [1,0], \n",
        "\n",
        "For multiple training instances, does it do this every time? There may be multiple ways for an ANN circuit to calculate XOR.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WqmNyaNTwi04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Gradual input modification \n",
        "\n",
        "Now slightly modify the input and see what happens to each layer!\n",
        "\n",
        "Try different modification levels. As you gradually change it, how do the weights change?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Use:\n",
        "Yt_train = Yt_train.type(torch.LongTensor)\n",
        "\n",
        "https://stackoverflow.com/questions/60440292/runtimeerror-expected-scalar-type-long-but-found-float"
      ],
      "metadata": {
        "id": "aARB9Xwa0V8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_name = 'fc1'\n",
        "# step_incr = [round(x * 0.1, 1) for x in range(0, 10)]\n",
        "old_input = torch.tensor([0, 0]).type(torch.LongTensor)\n",
        "new_input = torch.tensor([0, 0]).type(torch.LongTensor)\n",
        "# for i in step_incr:\n",
        "for i in range(0, 10):\n",
        "    old_input = new_input.clone().type(torch.LongTensor)\n",
        "    new_input = old_input + torch.tensor([0.1, 0])\n",
        "    # new_input = torch.tensor([i, 0])\n",
        "    print(i)\n",
        "    diff = get_activations(new_input, layer_name)[layer_name] - get_activations(old_input, layer_name)[layer_name]\n",
        "    print(diff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "bIFRwlqQ0aNL",
        "outputId": "7145915e-1f7f-46ad-cf57-59ac282053fd"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-5eab00a72b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# new_input = torch.tensor([i, 0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-76284df124a3>\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(input, layer_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#else will return the same actvs of model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1a6cc5e12c7c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_activations(new_input, layer_name)[layer_name]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY0yrbwC2eW9",
        "outputId": "5c149b29-5f16-4c31-e324-f058b32d5dfc"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2237, -0.2287])"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwYNlOEd2p0I",
        "outputId": "35efca27-2f82-491f-e09b-d7fa8f3b5c7f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1000, 0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "old_input = new_input.clone().type(torch.LongTensor)\n",
        "old_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reBxmkiO2mEG",
        "outputId": "daaddc21-3363-491a-ea7d-d0f10a7c8334"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_input.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NZXoyw733YK",
        "outputId": "92ad3e10-e2ac-49ae-ae49-6efb46171e3c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([0, 0]).dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxnRM--C30qu",
        "outputId": "f810bb46-290e-4526-8aa4-609c927fe80b"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([0, 0]).type(torch.LongTensor).dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCAOVO0F3Swc",
        "outputId": "39556bb1-74db-4e13-f783-749d025e0203"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_activations(torch.tensor([0, 0]).type(torch.LongTensor), layer_name)[layer_name]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "DKPLobud2j_j",
        "outputId": "f301894a-ea16-4b33-e7ee-415a91e746a5"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-46b2702ffa7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-76284df124a3>\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(input, layer_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#else will return the same actvs of model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1a6cc5e12c7c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Conditions to allow NN to model XOR:\n",
        "\n",
        "- the weights were learned to make sure w1 - w2 was close to 0,\n",
        "\n",
        "Any NN with the architecture above whose weights satisfy these conditions will model XOR"
      ],
      "metadata": {
        "id": "e3YhHgKt480T"
      }
    }
  ]
}