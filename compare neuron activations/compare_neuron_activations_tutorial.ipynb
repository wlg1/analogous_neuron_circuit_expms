{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZ7rYzsPrroYOcfHFrbqAi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["For a given layer, we compare the neuron activations between two images. We will start by ranking the neurons (by index number) that have the highest activation value. We look at only the top 1000 neurons. \n","\n","We begin by loading the Inception V1 model and a dataset. We'll use the "],"metadata":{"id":"Nr8Or5eql0cm"}},{"cell_type":"markdown","source":["# Load Model"],"metadata":{"id":"IAJjuRTDBnOr"}},{"cell_type":"code","metadata":{"id":"4SMIbEnOOfpp","executionInfo":{"status":"ok","timestamp":1662229647028,"user_tz":240,"elapsed":5535,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"source":["%%capture\n","import torch\n","import torchvision.models as models\n","from torchvision import transforms\n","import copy\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os \n","\n","# model = torch.hub.load('pytorch/vision:v0.9.0', 'googlenet', pretrained=True)\n","# model = models.vgg16(pretrained=True)\n","model = models.googlenet(pretrained=True)  #w/o arg, this will not pretrain it\n","\n","model.eval() #set model in eval mode: https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n","\n","# Download ImageNet labels\n","!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n","with open(\"imagenet_classes.txt\", \"r\") as f:\n","    categories = [s.strip() for s in f.readlines()]"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","# Load Dataset\n","\n","Load kaggle.json from drive or github repo"],"metadata":{"id":"pvjBqKsmm8XU"}},{"cell_type":"code","source":[],"metadata":{"id":"biKeUKiJMGZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","! pip install kaggle\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","! kaggle datasets download iamsouravbanerjee/animal-image-dataset-90-different-animals\n","! unzip animal-image-dataset-90-different-animals.zip\n","\n","data_dir = 'animals/animals'\n","f = open(\"name of the animals.txt\", \"r\")\n","animals_list = f.readlines()\n","animals_list = [x.replace('\\n','') for x in animals_list]"],"metadata":{"id":"VbhHHBrC91uj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Get Activations\n","\n","To compare the activations in a layer for two images, we need to get the activations. The code below does just that."],"metadata":{"id":"xSsJ6LZAm4c5"}},{"cell_type":"code","source":["def get_activations(input_image, layer_name):\n","    activation = {}\n","    def get_activation(name):\n","        def hook(model, input, output):\n","            activation[name] = output.detach()\n","        return hook\n","\n","    preprocess = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","    input_tensor = preprocess(input_image)\n","    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n","    if torch.cuda.is_available():\n","        input_batch = input_batch.to('cuda')\n","        model.to('cuda')\n","\n","    for name_to_check, layer in model.named_modules():\n","        if name_to_check == layer_name:\n","            break\n","    layer.register_forward_hook(get_activation(layer_name))\n","    output = model(input_batch)\n","    return activation.copy()  #.copy(), else will return the same actvs of model"],"metadata":{"id":"DhZ_GsDgIakK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sort Neurons"],"metadata":{"id":"S3EbYBPkJHhx"}},{"cell_type":"code","source":["top_neurons_dict = {}\n","\n","# flatten activation tensor and sort from lowest to highest\n","def get_sorted_neurons(input_image, layer, filename):\n","    actv_layer = get_activations(input_image, layer)\n","    flat_layer = torch.flatten(actv_layer[layer])\n","    sorted, indices = torch.sort(flat_layer)\n","    top_neurons_dict[(filename, layer)] = indices.tolist()\n","    return indices.tolist()"],"metadata":{"id":"5JFJEbSqIedU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We memoize by storing each image's layers of activations in the dictionary 'top_neurons_dict'; otherwise, we would have to calculate the layers of activations by passing the image through the model every time, which would take a lot of time when we are comparing the same activations for several pairs of images."],"metadata":{"id":"8Eh-L1jCJW5A"}},{"cell_type":"markdown","source":[],"metadata":{"id":"nM3XsH38K0HU"}}]}