{"cells":[{"cell_type":"markdown","source":["# Import libs, mount drive"],"metadata":{"id":"DJExiMWWP0ZJ"}},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')  #brings google drive files to be accessible under /content/"],"metadata":{"id":"oTAj0S5Mwgtz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661541262679,"user_tz":240,"elapsed":25386,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c801171c-0e60-46be-d73a-34c960e5b9ef"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%%capture\n","# fixes \"NotImplementedError: Cannot convert a symbolic Tensor (Inputs/minibatch_gpu_in:0) to a numpy array.\" during run_training.py\n","!pip install numpy==1.19.5  \n","\n","import os\n","import pickle\n","import numpy as np\n","import PIL.Image\n","import scipy\n","# %tensorflow_version 1.x\n","import tensorflow as tf\n"],"metadata":{"id":"uWeNBiV4zmxz","executionInfo":{"status":"ok","timestamp":1661542240583,"user_tz":240,"elapsed":5782,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# !git clone https://github.com/NVlabs/stylegan2.git\n","!git clone https://github.com/taziksh/stylegan2  #same as https://github.com/ZKTKZ/stylegan2.git\n","%cd /content/stylegan2\n","# import dnnlib\n","# import dnnlib.tflib as tflib\n","\n","!nvcc test_nvcc.cu -o test_nvcc -run\n","!nvcc /content/stylegan2/test_nvcc.cu -o /content/stylegan2/test_nvcc -run\n","\n","print('Tensorflow version: {}'.format(tf.__version__) )\n","!nvidia-smi -L\n","print('GPU Identified at: {}'.format(tf.test.gpu_device_name()))\n","\n","!pip install tensorboard"],"metadata":{"id":"TvYrUXr8V0nG","executionInfo":{"status":"ok","timestamp":1661542288142,"user_tz":240,"elapsed":10639,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","# Loading data into tfrecords\n","\n"],"metadata":{"id":"Gh65PxRBXO40"}},{"cell_type":"markdown","metadata":{"id":"9X0mFcSfGmGr"},"source":["\n","\n","**Loading a subset of high-res faces**\n","\n","https://www.kaggle.com/datasets/subinium/highresolution-anime-face-dataset-512x512"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TLs-sEqGmyA"},"outputs":[],"source":["# %%capture\n","# ! pip install kaggle\n","# ! mkdir ~/.kaggle\n","# # ! cp /content/drive/my\\ Drive/kaggle.json ~/.kaggle/\n","# ! cp kaggle.json ~/.kaggle/\n","# ! chmod 600 ~/.kaggle/kaggle.json\n","# ! kaggle datasets download subinium/highresolution-anime-face-dataset-512x512"]},{"cell_type":"markdown","source":["Unzip only 5000 files from zip\n","\n","https://stackoverflow.com/questions/22243031/unzip-only-limited-number-of-files-in-linux"],"metadata":{"id":"8_BU5CzUbNP1"}},{"cell_type":"code","source":["# !unzip -Z1 highresolution-anime-face-dataset-512x512.zip | head -5000 | sed 's| |\\\\ |g' | xargs unzip highresolution-anime-face-dataset-512x512.zip"],"metadata":{"id":"ijgWDlXYYwk3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","**Download high-res directly from drive**\n","\n","Takes 1min to unzip 5000 imgs, then 5min to turn them to tfrecords"],"metadata":{"id":"cl7doP5nx7ZE"}},{"cell_type":"code","source":["%%capture\n","!unzip -Z1 \"/content/drive/My Drive/high_res_anime_faces.zip\" | head -5000 | sed 's| |\\\\ |g' | xargs unzip \"/content/drive/My Drive/high_res_anime_faces.zip\""],"metadata":{"id":"veA-TxLkx5kc","executionInfo":{"status":"ok","timestamp":1661542344203,"user_tz":240,"elapsed":50499,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# resize each image\n","from PIL import Image\n","! mkdir /content/portraits_resized/\n","\n","newsize = (64, 64)\n","img_dir = os.listdir('/content/portraits')\n","for filename in img_dir:\n","    path_filename = os.path.join('/content/portraits', filename)\n","    samp_image = Image.open(path_filename)\n","    samp_image = samp_image.resize(newsize)\n","    samp_image.save('/content/portraits_resized/' + filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"IThgvhkDlcLu","executionInfo":{"status":"error","timestamp":1661542346902,"user_tz":240,"elapsed":14,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9427b026-5deb-488a-b7ab-03cc7b8840db"},"execution_count":8,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-8cf8f501e163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnewsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/portraits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpath_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/portraits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/portraits'"]}]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3303,"status":"ok","timestamp":1661542375880,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"dK28KWzYkWyb","outputId":"63a6bbdb-5d13-4a47-eb5b-41a27b261d29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/stylegan2/dataset_tool.py\", line 16, in <module>\n","    import dnnlib.tflib as tflib\n","  File \"/content/stylegan2/dnnlib/tflib/__init__.py\", line 7, in <module>\n","    from . import autosummary\n","  File \"/content/stylegan2/dnnlib/tflib/autosummary.py\", line 30, in <module>\n","    from . import tfutil\n","  File \"/content/stylegan2/dnnlib/tflib/tfutil.py\", line 16, in <module>\n","    import tensorflow.contrib   # requires TensorFlow 1.x!\n","ModuleNotFoundError: No module named 'tensorflow.contrib'\n"]}],"source":["# convert images to tfrecord\n","# https://www.tensorflow.org/tutorials/load_data/tfrecord\n","\n","# !python /content/stylegan2/dataset_tool.py create_from_images ./training_data ./portraits \n","\n","# However, create_from_images doesn't seem to work if not 512 x 512 ?\n","!python /content/stylegan2/dataset_tool.py create_from_images /content/training_data /content/portraits_resized \n","\n","# !python /content/stylegan2/dataset_tool.py create_from_images \\\n","#     \"/content/drive/My Drive/high_res_anime_faces_tfrecords\" \\\n","#     ./portraits"]},{"cell_type":"markdown","source":["\n","\n","---\n","# Loading pretrained model\n"],"metadata":{"id":"OoXtREIdXVtz"}},{"cell_type":"markdown","metadata":{"id":"liKaTLyUo0Nk"},"source":["https://www.reddit.com/r/MediaSynthesis/comments/gum6f1/tfdne_edit_furry_faces_inbrowser_google_colab/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18224,"status":"ok","timestamp":1651714613765,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"pXcjXwwhzJOW","outputId":"32eeb7d5-7477-4d16-bbc3-7a895d665df7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-05 01:36:34--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\n","Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 13.224.160.5, 13.224.160.108, 13.224.160.125, ...\n","Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|13.224.160.5|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 381646055 (364M) [binary/octet-stream]\n","Saving to: ‘/content/nvidia-stylegan2-ada-ffhq-tf.pkl’\n","\n","/content/nvidia-sty 100%[===================>] 363.97M  28.2MB/s    in 17s     \n","\n","2022-05-05 01:36:52 (21.5 MB/s) - ‘/content/nvidia-stylegan2-ada-ffhq-tf.pkl’ saved [381646055/381646055]\n","\n"]}],"source":["# import gdown\n","# !wget  -O /content/network-e621.pkl https://thisfursonadoesnotexist.com/model/network-e621-r-512-3194880.pkl\n","\n","# !wget -O /content/nvidia-stylegan2-ada-ffhq-tf.pkl https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl \n"]},{"cell_type":"code","source":["# tflib.init_tf()\n","# sess = tf.get_default_session()\n","# sess.list_devices()\n","# cores = tflex.get_cores()\n","# tflex.set_override_cores(cores)\n","# _G, _D, Gs = pickle.load(open(\"/content/nvidia-stylegan2-ada-ffhq-tf.pkl\", \"rb\"))"],"metadata":{"id":"V6HnKX2J1jKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pretrained_networks\n","# network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n","# _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n","# # noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]"],"metadata":{"id":"WXYFjzYQ1QSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Only if using NVlabs:\n","# get url from gdrive_urls in https://github.com/NVlabs/stylegan2/blob/master/pretrained_networks.py\n","\n","# import gdown\n","# !wget -O /content/stylegan2-ffhq-config-f.pkl https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl \n"],"metadata":{"id":"yMuTLqLDQK64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# only if using ZKTKZ repo:\n","import gdown\n","# !wget -O /content/stylegan2-ffhq-config-f.pkl http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl\n","!wget -O /content/stylegan2-horse-config-a.pkl http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-horse-config-a.pkl\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TGxhvOk3SuQ0","executionInfo":{"status":"ok","timestamp":1651849805915,"user_tz":240,"elapsed":14986,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"77a090ba-3643-4fc2-d8ee-77e451987c56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-06 15:09:52--  http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-horse-config-a.pkl\n","Resolving d36zk2xti64re0.cloudfront.net (d36zk2xti64re0.cloudfront.net)... 18.65.227.151, 18.65.227.32, 18.65.227.14, ...\n","Connecting to d36zk2xti64re0.cloudfront.net (d36zk2xti64re0.cloudfront.net)|18.65.227.151|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 302518494 (289M) [application/x-www-form-urlencoded]\n","Saving to: ‘/content/stylegan2-horse-config-a.pkl’\n","\n","/content/stylegan2- 100%[===================>] 288.50M  29.1MB/s    in 14s     \n","\n","2022-05-06 15:10:07 (20.8 MB/s) - ‘/content/stylegan2-horse-config-a.pkl’ saved [302518494/302518494]\n","\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","# Training\n"],"metadata":{"id":"BLGjC1pYXh_9"}},{"cell_type":"markdown","metadata":{"id":"2OuZ4XYaach3"},"source":["\n","\n","https://colab.research.google.com/github/ZKTKZ/thdne/blob/master/StyleGAN2_Tazik_25GB_RAM.ipynb#scrollTo=vmeWpLdZdhq2"]},{"cell_type":"code","source":["if 'COLAB_TPU_ADDR' in os.environ:\n","    os.environ['TPU_NAME'] = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","    os.environ['NOISY'] = '1'\n","\n","tflib.init_tf()\n","sess = tf.get_default_session()\n","sess.list_devices()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScQ3JYj5XK6r","executionInfo":{"status":"ok","timestamp":1651849282030,"user_tz":240,"elapsed":189,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"abd787c8-b964-424c-861d-fa0867522ef1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 12457290748965292404),\n"," _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4667090122800505579),\n"," _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1235302875991361978),\n"," _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 15964005991, 3302370878702370035)]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# %cd training\n","# !python /content/stylegan2/training/training_loop.py --resume_pkl='gdrive:networks/stylegan2-ffhq-config-f.pkl'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHHrO7LUNAf-","executionInfo":{"status":"ok","timestamp":1651770881841,"user_tz":240,"elapsed":2335,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0423659a-7926-4bbd-f1e6-e60ade5b7144"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/stylegan2/training/training_loop.py\", line 11, in <module>\n","    import dnnlib\n","ModuleNotFoundError: No module named 'dnnlib'\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41wVnrihpz_G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651850955140,"user_tz":240,"elapsed":30030,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4fe84522-8b44-4da0-d4b8-529adfa8da73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Local submit - run_dir: /content/drive/My Drive/realfaces_anime/results/00001-stylegan2-training_data-1gpu-config-e\n","dnnlib: Running training.training_loop.training_loop() on localhost...\n","Streaming data using training.dataset.TFRecordDataset...\n","Dataset shape = [3, 512, 512]\n","Dynamic range = [0, 255]\n","Label size    = 0\n","Constructing networks...\n","Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n","Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n","\n","G                           Params    OutputShape       WeightShape     \n","---                         ---       ---               ---             \n","latents_in                  -         (?, 512)          -               \n","labels_in                   -         (?, 0)            -               \n","lod                         -         ()                -               \n","dlatent_avg                 -         (512,)            -               \n","G_mapping/latents_in        -         (?, 512)          -               \n","G_mapping/labels_in         -         (?, 0)            -               \n","G_mapping/Normalize         -         (?, 512)          -               \n","G_mapping/Dense0            262656    (?, 512)          (512, 512)      \n","G_mapping/Dense1            262656    (?, 512)          (512, 512)      \n","G_mapping/Dense2            262656    (?, 512)          (512, 512)      \n","G_mapping/Dense3            262656    (?, 512)          (512, 512)      \n","G_mapping/Dense4            262656    (?, 512)          (512, 512)      \n","G_mapping/Dense5            262656    (?, 512)          (512, 512)      \n","G_mapping/Dense6            262656    (?, 512)          (512, 512)      \n","G_mapping/Dense7            262656    (?, 512)          (512, 512)      \n","G_mapping/Broadcast         -         (?, 10, 512)      -               \n","G_mapping/dlatents_out      -         (?, 10, 512)      -               \n","Truncation/Lerp             -         (?, 10, 512)      -               \n","G_synthesis/dlatents_in     -         (?, 10, 512)      -               \n","G_synthesis/4x4/Const       8192      (?, 512, 4, 4)    (1, 512, 4, 4)  \n","G_synthesis/4x4/Conv        2622465   (?, 512, 4, 4)    (3, 3, 512, 512)\n","G_synthesis/4x4/ToRGB       264195    (?, 3, 4, 4)      (1, 1, 512, 3)  \n","G_synthesis/8x8/Conv0_up    2622465   (?, 512, 8, 8)    (3, 3, 512, 512)\n","G_synthesis/8x8/Conv1       2622465   (?, 512, 8, 8)    (3, 3, 512, 512)\n","G_synthesis/8x8/Upsample    -         (?, 3, 8, 8)      -               \n","G_synthesis/8x8/ToRGB       264195    (?, 3, 8, 8)      (1, 1, 512, 3)  \n","G_synthesis/16x16/Conv0_up  2622465   (?, 512, 16, 16)  (3, 3, 512, 512)\n","G_synthesis/16x16/Conv1     2622465   (?, 512, 16, 16)  (3, 3, 512, 512)\n","G_synthesis/16x16/Upsample  -         (?, 3, 16, 16)    -               \n","G_synthesis/16x16/ToRGB     264195    (?, 3, 16, 16)    (1, 1, 512, 3)  \n","G_synthesis/32x32/Conv0_up  2622465   (?, 512, 32, 32)  (3, 3, 512, 512)\n","G_synthesis/32x32/Conv1     2622465   (?, 512, 32, 32)  (3, 3, 512, 512)\n","G_synthesis/32x32/Upsample  -         (?, 3, 32, 32)    -               \n","G_synthesis/32x32/ToRGB     264195    (?, 3, 32, 32)    (1, 1, 512, 3)  \n","G_synthesis/64x64/Conv0_up  1442561   (?, 256, 64, 64)  (3, 3, 512, 256)\n","G_synthesis/64x64/Conv1     721409    (?, 256, 64, 64)  (3, 3, 256, 256)\n","G_synthesis/64x64/Upsample  -         (?, 3, 64, 64)    -               \n","G_synthesis/64x64/ToRGB     132099    (?, 3, 64, 64)    (1, 1, 256, 3)  \n","G_synthesis/images_out      -         (?, 3, 64, 64)    -               \n","G_synthesis/noise0          -         (1, 1, 4, 4)      -               \n","G_synthesis/noise1          -         (1, 1, 8, 8)      -               \n","G_synthesis/noise2          -         (1, 1, 8, 8)      -               \n","G_synthesis/noise3          -         (1, 1, 16, 16)    -               \n","G_synthesis/noise4          -         (1, 1, 16, 16)    -               \n","G_synthesis/noise5          -         (1, 1, 32, 32)    -               \n","G_synthesis/noise6          -         (1, 1, 32, 32)    -               \n","G_synthesis/noise7          -         (1, 1, 64, 64)    -               \n","G_synthesis/noise8          -         (1, 1, 64, 64)    -               \n","images_out                  -         (?, 3, 64, 64)    -               \n","---                         ---       ---               ---             \n","Total                       23819544                                    \n","\n","\n","D                    Params    OutputShape       WeightShape     \n","---                  ---       ---               ---             \n","images_in            -         (?, 3, 64, 64)    -               \n","labels_in            -         (?, 0)            -               \n","64x64/FromRGB        1024      (?, 256, 64, 64)  (1, 1, 3, 256)  \n","64x64/Conv0          590080    (?, 256, 64, 64)  (3, 3, 256, 256)\n","64x64/Conv1_down     1180160   (?, 512, 32, 32)  (3, 3, 256, 512)\n","64x64/Skip           131072    (?, 512, 32, 32)  (1, 1, 256, 512)\n","32x32/Conv0          2359808   (?, 512, 32, 32)  (3, 3, 512, 512)\n","32x32/Conv1_down     2359808   (?, 512, 16, 16)  (3, 3, 512, 512)\n","32x32/Skip           262144    (?, 512, 16, 16)  (1, 1, 512, 512)\n","16x16/Conv0          2359808   (?, 512, 16, 16)  (3, 3, 512, 512)\n","16x16/Conv1_down     2359808   (?, 512, 8, 8)    (3, 3, 512, 512)\n","16x16/Skip           262144    (?, 512, 8, 8)    (1, 1, 512, 512)\n","8x8/Conv0            2359808   (?, 512, 8, 8)    (3, 3, 512, 512)\n","8x8/Conv1_down       2359808   (?, 512, 4, 4)    (3, 3, 512, 512)\n","8x8/Skip             262144    (?, 512, 4, 4)    (1, 1, 512, 512)\n","4x4/MinibatchStddev  -         (?, 513, 4, 4)    -               \n","4x4/Conv             2364416   (?, 512, 4, 4)    (3, 3, 513, 512)\n","4x4/Dense0           4194816   (?, 512)          (8192, 512)     \n","Output               513       (?, 1)            (512, 1)        \n","scores_out           -         (?, 1)            -               \n","---                  ---       ---               ---             \n","Total                23407361                                    \n","\n","Building TensorFlow graph...\n","Traceback (most recent call last):\n","  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 642, in set_shape\n","    unknown_shape)\n","tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 2 in both shapes must be equal, but are 512 and 64. Shapes are [?,3,512,512] and [?,3,64,64].\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/stylegan2/run_training.py\", line 215, in <module>\n","    main()\n","  File \"/content/stylegan2/run_training.py\", line 210, in main\n","    run(**vars(args))\n","  File \"/content/stylegan2/run_training.py\", line 134, in run\n","    dnnlib.submit_run(**kwargs)\n","  File \"/content/stylegan2/dnnlib/submission/submit.py\", line 343, in submit_run\n","    return farm.submit(submit_config, host_run_dir)\n","  File \"/content/stylegan2/dnnlib/submission/internal/local.py\", line 22, in submit\n","    return run_wrapper(submit_config)\n","  File \"/content/stylegan2/dnnlib/submission/submit.py\", line 280, in run_wrapper\n","    run_func_obj(**submit_config.run_func_kwargs)\n","  File \"/content/stylegan2/training/training_loop.py\", line 270, in training_loop\n","    D_loss, D_reg = dnnlib.util.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_gpu_in, reals=reals_read, labels=labels_read, **D_loss_args)\n","  File \"/content/stylegan2/dnnlib/util.py\", line 256, in call_func_by_name\n","    return func_obj(*args, **kwargs)\n","  File \"/content/stylegan2/training/loss.py\", line 82, in D_logistic_r1\n","    real_scores_out = D.get_output_for(reals, labels, is_training=True)\n","  File \"/content/stylegan2/dnnlib/tflib/network.py\", line 221, in get_output_for\n","    out_expr = self._build_func(*final_inputs, **build_kwargs)\n","  File \"/content/stylegan2/training/networks_stylegan2.py\", line 686, in D_stylegan2\n","    images_in.set_shape([None, num_channels, min_h*2**res_log2, min_w*2**res_log2])\n","  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 645, in set_shape\n","    raise ValueError(str(e))\n","ValueError: Dimension 2 in both shapes must be equal, but are 512 and 64. Shapes are [?,3,512,512] and [?,3,64,64].\n"]}],"source":["# !python /content/stylegan2/run_training.py --num-gpus=1 --data-dir=\"/content/drive/My Drive/\" \\\n","# --config=config-f --dataset=high_res_anime_faces_tfrecords \\\n","# --total-kimg=10000 --result-dir=\"/content/drive/My Drive/realfaces_anime/results/\" \n","\n","# !python /content/stylegan2/run_training.py --num-gpus=1 --data-dir=\"/content/drive/My Drive/\" \\\n","# --config=config-a --dataset=high_res_anime_faces_tfrecords \\\n","# --total-kimg=10000 --result-dir=\"/content/drive/My Drive/realfaces_anime/results/\" \\\n","# --resume-pkl='/content/stylegan2-horse-config-a.pkl'\n","# --resume-pkl='/content/stylegan2-ffhq-config-f.pkl'\n","\n","!python /content/stylegan2/run_training.py --num-gpus=1 --data-dir=/content/stylegan2 --config=config-e --dataset=training_data \\\n","--total-kimg=10000 --result-dir=\"/content/drive/My Drive/realfaces_anime/results/\" \n","\n","# --spatial-augmentations=true --mirror-augment=true --metric=none \n","# --min-h=4 --min-w=4 --res-log2=7 --lr=0.0005 "]},{"cell_type":"code","source":["%%capture\n","# 00002-stylegan2-training_data-1gpu-config-f\n","# network-snapshot-000000\n","!python /content/stylegan2/run_generator.py generate-images --seeds=0-100 --truncation-psi=0.7 \\\n","--network='/content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/network-snapshot-000006.pkl'\n","#%cp -av /content/stylegan2/results/00000-generate-images /content/drive/'My Drive'/twist_moe/seeds-1.0\n"],"metadata":{"id":"9Rwm8Poem7tQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXzkpzBqnGBK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651191306488,"user_tz":240,"elapsed":959174,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"62f3921f-ae2c-451d-8591-efbb4973a593"},"source":["import os\n","import pickle\n","import numpy as np\n","import PIL.Image\n","import dnnlib\n","import dnnlib.tflib as tflib\n","import scipy\n","import math\n","import moviepy.editor\n","from numpy import linalg\n","\n","dir = \"/content/drive/My Drive/twist_moe/results/\"\n","training_expm = \"00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/\"\n","snapshot = \"network-snapshot-000006.pkl\"\n","network_path = dir + training_expm + snapshot\n","\n","def main():\n","    tflib.init_tf()\n","    _G, _D, Gs = pickle.load(open(network_path, \"rb\"))\n","\n","    rnd = np.random\n","    latents_a = rnd.randn(1, Gs.input_shape[1])\n","    latents_b = rnd.randn(1, Gs.input_shape[1])\n","    latents_c = rnd.randn(1, Gs.input_shape[1])\n","\n","    def circ_generator(latents_interpolate):\n","        radius = 40.0\n","\n","        latents_axis_x = (latents_a - latents_b).flatten() / linalg.norm(latents_a - latents_b)\n","        latents_axis_y = (latents_a - latents_c).flatten() / linalg.norm(latents_a - latents_c)\n","\n","        latents_x = math.sin(math.pi * 2.0 * latents_interpolate) * radius\n","        latents_y = math.cos(math.pi * 2.0 * latents_interpolate) * radius\n","\n","        latents = latents_a + latents_x * latents_axis_x + latents_y * latents_axis_y\n","        return latents\n","\n","    def mse(x, y):\n","        return (np.square(x - y)).mean()\n","\n","    def generate_from_generator_adaptive(gen_func):\n","        max_step = 1.0\n","        current_pos = 0.0\n","\n","        change_min = 10.0\n","        change_max = 11.0\n","\n","        fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n","\n","        current_latent = gen_func(current_pos)\n","        current_image = Gs.run(current_latent, None, truncation_psi=0.7, randomize_noise=False, output_transform=fmt)[0]\n","        array_list = []\n","\n","        video_length = 1.0\n","        while(current_pos < video_length):\n","            array_list.append(current_image)\n","\n","            lower = current_pos\n","            upper = current_pos + max_step\n","            current_pos = (upper + lower) / 2.0\n","\n","            current_latent = gen_func(current_pos)\n","            current_image = images = Gs.run(current_latent, None, truncation_psi=0.7, randomize_noise=False, output_transform=fmt)[0]\n","            current_mse = mse(array_list[-1], current_image)\n","\n","            while current_mse < change_min or current_mse > change_max:\n","                if current_mse < change_min:\n","                    lower = current_pos\n","                    current_pos = (upper + lower) / 2.0\n","\n","                if current_mse > change_max:\n","                    upper = current_pos\n","                    current_pos = (upper + lower) / 2.0\n","\n","\n","                current_latent = gen_func(current_pos)\n","                current_image = images = Gs.run(current_latent, None, truncation_psi=0.7, randomize_noise=False, output_transform=fmt)[0]\n","                current_mse = mse(array_list[-1], current_image)\n","            # print(current_pos, current_mse)\n","        return array_list\n","\n","    frames = generate_from_generator_adaptive(circ_generator)\n","    frames = moviepy.editor.ImageSequenceClip(frames, fps=30)\n","\n","    # Generate video.\n","    mp4_file = '/content/drive/My Drive/circular.mp4'\n","    mp4_codec = 'libx264'\n","    mp4_bitrate = '3M'\n","    mp4_fps = 20\n","\n","    frames.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate)\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[MoviePy] >>>> Building video /content/drive/My Drive/circular.mp4\n","[MoviePy] Writing video /content/drive/My Drive/circular.mp4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1368/1368 [00:14<00:00, 93.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[MoviePy] Done.\n","[MoviePy] >>>> Video ready: /content/drive/My Drive/circular.mp4 \n","\n"]}]},{"cell_type":"code","metadata":{"id":"f-HTjbmIDseN","colab":{"base_uri":"https://localhost:8080/","height":953},"executionInfo":{"status":"ok","timestamp":1651191562298,"user_tz":240,"elapsed":19549,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d9de4cb9-911d-4751-aacf-fa49f1ce7fbd"},"source":["#https://stackoverflow.com/a/57378660/8773953\n","!pip install -U kora\n","from kora.drive import upload_public\n","url = upload_public('/content/drive/My Drive/circular.mp4')\n","\n","from IPython.display import HTML\n","HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kora\n","  Downloading kora-0.9.19-py3-none-any.whl (57 kB)\n","\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 57 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from kora) (5.5.0)\n","Collecting fastcore\n","  Downloading fastcore-1.4.2-py3-none-any.whl (60 kB)\n","\u001b[K     |████████████████████████████████| 60 kB 8.8 MB/s \n","\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (21.1.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (21.3)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (2.6.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (5.1.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (1.0.18)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.8.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.8.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.4.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (0.2.5)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastcore->kora) (3.0.8)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->kora) (0.7.0)\n","Installing collected packages: fastcore, kora\n","Successfully installed fastcore-1.4.2 kora-0.9.19\n"]},{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<video src=https://drive.google.com/uc?id=10KiLHd9B23lOIlPlBwjc2-1bwgHiZxwK width=500 controls/>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["!zip -r /content/anime_faces_furry_pretr.zip /content/drive/My\\ Drive/twist_moe"],"metadata":{"id":"4hAHozsuss-b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651191623277,"user_tz":240,"elapsed":38562,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"fe578e88-ebcc-40dc-b1a9-4fdb091420ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/drive/My Drive/twist_moe/ (stored 0%)\n","  adding: content/drive/My Drive/twist_moe/results/ (stored 0%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/ (stored 0%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/submit_config.pkl (deflated 51%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/submit_config.txt (deflated 70%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/log.txt (deflated 79%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/reals.jpg (deflated 4%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/fakes_init.jpg (deflated 3%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/run.txt (deflated 20%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/fakes000000.jpg (deflated 3%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/network-snapshot-000000.pkl (deflated 7%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/metric-fid50k.txt (deflated 17%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/events.out.tfevents.1651187577.f1bde24b119b (deflated 41%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/fakes000006.jpg (deflated 2%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/network-snapshot-000006.pkl (deflated 7%)\n","  adding: content/drive/My Drive/twist_moe/results/00000-stylegan2-high_res_anime_faces_tfrecords-1gpu-config-f/_finished.txt (stored 0%)\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"stylegan2_transferlearn_ffhqToAnime.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMdOQZoezFk32ZE+LcA2Zzg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}