{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"feature_viz_tests.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN22ntMrrfN7Cyh3FvXRbGj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Feature viz impt filters\n","\n","match using:\n","\n","https://distill.pub/2017/feature-visualization/appendix/\n","\n","https://github.com/tensorflow/lucid/blob/master/lucid/modelzoo/other_models/InceptionV1.py\n","\n","https://github.com/tensorflow/lucid/blob/master/lucid/optvis/render.py"],"metadata":{"id":"EbObjR5uHVnO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WEtbyOZCVZT"},"outputs":[],"source":["# Install Lucid\n","\n","!pip install --quiet lucid==0.2.3\n","!pip install numpy==1.19.5\n","\n","%tensorflow_version 1.x  #required after tf 2.0 released\n","import numpy as np\n","import tensorflow as tf\n","\n","import lucid.modelzoo.vision_models as models\n","from lucid.misc.io import show\n","import lucid.optvis.objectives as objectives\n","import lucid.optvis.param as param\n","import lucid.optvis.render as render\n","import lucid.optvis.transform as transform\n","import operator\n","import pdb\n","\n","'''\n","https://github.com/tensorflow/lucid/blob/master/lucid/modelzoo/vision_models.py\n","https://github.com/tensorflow/lucid/blob/master/lucid/modelzoo/other_models/InceptionV1.py\n","'''\n","\n","googlenet = models.InceptionV1()\n","googlenet.load_graphdef()\n","\n","# googlenet.model_path\n","\n","# models.InceptionV1.layers\n","# models.InceptionV1().layers"]},{"cell_type":"code","source":["'''actv max image for channel/neuron 240 of layer mixed4a'''\n","\n","_ = render.render_vis(googlenet, \"mixed4a_pre_relu:240\")\n","\n","#equivalent to:\n","obj = objectives.channel(\"mixed4a_pre_relu\", 240)\n","_ = render.render_vis(googlenet, obj)\n"],"metadata":{"id":"xiv9gJdSCeGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''actv max image for channels 0+0, 0+1, 1+0, 1+1 for layer mixed4a'''\n","\n","channel = lambda n: objectives.channel(\"mixed4a_pre_relu\", n)\n","obj = channel(0) + channel(0)\n","render.render_vis(googlenet, obj)\n","for i in range(2):\n","  for j in range(2):\n","    obj = channel(i) + channel(j)\n","    print('channel '+str(i) + ' + channel '+str(j))\n","    render.render_vis(googlenet, obj)"],"metadata":{"id":"q5UYO-EICiK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Actv Max Viz: loop addition of neurons\n","\n","# obj = channel(0)\n","# for i in range(1, 3):  #if use >185, hits max recursion depth so error\n","#   obj += channel(i)\n","# for k in range(2):\n","#   render.render_vis(googlenet, obj, use_fixed_seed = True)\n","# #render.render_vis(googlenet, obj)"],"metadata":{"id":"f2qzd5QiCply"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Actv Max Viz: Weighted addition of neurons\n","\n","# obj = 0.2 * channel(20) + 0.8 * channel(21)\n","# render.render_vis(googlenet, obj)"],"metadata":{"id":"x3MFgNuRCrPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"x1WbR_tdC4os"},"execution_count":null,"outputs":[]}]}