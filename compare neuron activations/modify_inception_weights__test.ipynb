{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"modify_inception_weights__test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sdARnHfKOfpm"},"source":["### This notebook is optionally accelerated with a GPU runtime.\n","### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n","\n","----------------------------------------------------------------------\n","\n","# GoogLeNet\n","\n","*Author: Pytorch Team*\n","\n","**GoogLeNet was based on a deep convolutional neural network architecture codenamed \"Inception\" which won ImageNet 2014.**\n","\n"]},{"cell_type":"code","metadata":{"id":"kJ59A4Kx-sLG"},"source":["_ | _\n","- | -\n","![alt](https://pytorch.org/assets/images/googlenet1.png) | ![alt](https://pytorch.org/assets/images/googlenet2.png)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aq4vlN6IOfps"},"source":["### Model Description\n","\n","GoogLeNet was based on a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The 1-crop error rates on the ImageNet dataset with a pretrained model are list below.\n","\n","| Model structure | Top-1 error | Top-5 error |\n","| --------------- | ----------- | ----------- |\n","|  googlenet       | 30.22       | 10.47       |\n","\n","\n","\n","### References\n","\n"," - [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)"]},{"cell_type":"markdown","metadata":{"id":"9cNDGd1ih_nK"},"source":["Original code from:\n","https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_vision_googlenet.ipynb"]},{"cell_type":"code","metadata":{"id":"4SMIbEnOOfpp"},"source":["import torch\n","# model = torch.hub.load('pytorch/vision:v0.9.0', 'googlenet', pretrained=True)\n","\n","import torchvision.models as models\n","model = models.googlenet(pretrained=True)  #this will not pretrain it\n","\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mW9jPUoqOfpq"},"source":["All pre-trained models expect input images normalized in the same way,\n","i.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\n","The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n","and `std = [0.229, 0.224, 0.225]`.\n","\n","Here's a sample execution."]},{"cell_type":"code","metadata":{"id":"CGQHtjA6Ofpq"},"source":["# Download an example image from the pytorch website\n","import urllib\n","url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n","try: urllib.URLopener().retrieve(url, filename)\n","except: urllib.request.urlretrieve(url, filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kv3LphTvtZhd"},"source":["from google.colab import files\n","files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQjsInCrOfpr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631374174363,"user_tz":240,"elapsed":9266,"user":{"displayName":"Michael Lan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13558259605338023275"}},"outputId":"cb9db7a0-1b9a-4cb5-fe6d-947634d78a3f"},"source":["# sample execution (requires torchvision)\n","from PIL import Image\n","from torchvision import transforms\n","# input_image = Image.open(filename)\n","# input_image = Image.open('car1.jpg')\n","input_image = Image.open('face5.jpg')\n","preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","input_tensor = preprocess(input_image)\n","input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n","\n","# move the input and model to GPU for speed if available\n","if torch.cuda.is_available():\n","    input_batch = input_batch.to('cuda')\n","    model.to('cuda')\n","\n","with torch.no_grad():\n","    output = model(input_batch)\n","# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n","# print(output[0])\n","# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n","probabilities = torch.nn.functional.softmax(output[0], dim=0)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}]},{"cell_type":"code","metadata":{"id":"MUj9tM9BvfjJ"},"source":["# print(probabilities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2ZZZCs_Ofps","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631374176425,"user_tz":240,"elapsed":329,"user":{"displayName":"Michael Lan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13558259605338023275"}},"outputId":"7949ac8d-0b3d-47df-bf5c-507ce52c198d"},"source":["# Download ImageNet labels\n","!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-09-11 15:29:36--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10472 (10K) [text/plain]\n","Saving to: ‘imagenet_classes.txt’\n","\n","\rimagenet_classes.tx   0%[                    ]       0  --.-KB/s               \rimagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n","\n","2021-09-11 15:29:36 (65.5 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"ALEoijodOfps","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631374179021,"user_tz":240,"elapsed":115,"user":{"displayName":"Michael Lan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13558259605338023275"}},"outputId":"a1fbe9e7-32a2-46b5-a370-198ce7e59bd8"},"source":["# Read the categories\n","with open(\"imagenet_classes.txt\", \"r\") as f:\n","    categories = [s.strip() for s in f.readlines()]\n","# Show top categories per image\n","top5_prob, top5_catid = torch.topk(probabilities, 5)\n","for i in range(top5_prob.size(0)):\n","    print(categories[top5_catid[i]], top5_prob[i].item())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["neck brace 0.15806788206100464\n","brassiere 0.033983904868364334\n","toilet seat 0.02665269747376442\n","hair spray 0.024705681949853897\n","Band Aid 0.02386784367263317\n"]}]},{"cell_type":"markdown","metadata":{"id":"d59iBc_Uh0pH"},"source":["\n","\n","---\n","\n","Learn the shape of parts in Inception\n","\n","https://pytorch.org/vision/stable/_modules/torchvision/models/googlenet.html"]},{"cell_type":"code","metadata":{"id":"37nkddWNU5Zt"},"source":["model.conv1.conv  #3 channels, 64 filters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FrqfX2XUOL4"},"source":["model.conv1.conv.weight.data.shape  # tensor dims of weight matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VN0FZx_3UX3V"},"source":["len(model.conv1.conv.weight.data)  #num of filters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TLbK3Q8UUf6"},"source":["model.conv1.conv.weight.data[0].shape  # dims of a filter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsvmBjYNUb-k"},"source":["model.conv1.conv.weight.data[0][0].shape  #dims of a filter's channel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qV77SnkmaiDB"},"source":["\n","\n","---\n","Modify weights and run again\n"]},{"cell_type":"markdown","metadata":{"id":"nI1-F8IV9ypV"},"source":["Create a copy of the model to compare before + after changes for entire Models"]},{"cell_type":"code","metadata":{"id":"kGE4MhW7a2zZ"},"source":["import copy\n","new_mdl = copy.deepcopy(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"quR0R9Wx665J"},"source":["Set entire weight matrix of a SINGLE FILTER to 0. \n","\n","'temp_weightMatrix' is Before\n","\n","'model.conv1.conv.weight.data[0]' is After"]},{"cell_type":"code","metadata":{"id":"eiJr5exwQIxG"},"source":["temp_weightMatrix = model.conv1.conv.weight.data[0].detach().clone()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xgZhYtKRq07"},"source":["new_mdl.conv1.conv.weight.data[0] = new_mdl.conv1.conv.weight.data[0] * 0  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1mzcARkRus4"},"source":["temp_weightMatrix  #check that copy by value, not by ref"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzPqYE0PbC87"},"source":["new_mdl.conv1.conv.weight.data[0] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcBvZ2KaaO1k"},"source":["# Run again after tensor modification to see how it changed\n","\n","new_output = new_mdl(input_batch)\n","new_probabilities = torch.nn.functional.softmax(new_output[0], dim=0)\n","new_top5_prob, new_top5_catid = torch.topk(new_probabilities, 5)\n","for i in range(new_top5_prob.size(0)):\n","    row = ['before:', categories[top5_catid[i]], round(top5_prob[i].item(), 5), 'after:', categories[new_top5_catid[i]], round(new_top5_prob[i].item(), 5)]\n","    print(\"{: >10} {: >15} {: >10} {: >10} {: >15} {: >10}\".format(*row))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_jACRi8yyp1"},"source":["\n","\n","---\n","\n","\n","Get feature 4c: 447, a car detector. Set its weights to 0. Before and after on car image.\n","\n","https://distill.pub/2020/circuits/zoom-in/"]},{"cell_type":"code","metadata":{"id":"afsspz0DyyQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631330155206,"user_tz":240,"elapsed":99,"user":{"displayName":"Michael Lan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13558259605338023275"}},"outputId":"c8b6a43a-980a-46a0-f07a-d23570ab19af"},"source":["model.inception4c.branch3[1].conv.weight[63].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([24, 3, 3])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"FbFo4-pj9CAY"},"source":["new_mdl = copy.deepcopy(model)\n","new_mdl.inception4c.branch3[1].conv.weight[63].data = new_mdl.inception4c.branch3[1].conv.weight[63] * 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlOZ15NX_Ng1"},"source":["# model.inception4c.branch3[1].conv.weight.data = model.inception4c.branch3[1].conv.weight.data * 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q3y8Bcjh9kqT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631330159032,"user_tz":240,"elapsed":126,"user":{"displayName":"Michael Lan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13558259605338023275"}},"outputId":"feb7f81b-0f01-4597-997e-a04236acd9bb"},"source":["new_output = new_mdl(input_batch)\n","new_probabilities = torch.nn.functional.softmax(new_output[0], dim=0)\n","new_top5_prob, new_top5_catid = torch.topk(new_probabilities, 5)\n","for i in range(new_top5_prob.size(0)):\n","    row = ['before:', categories[top5_catid[i]], round(top5_prob[i].item(), 5), 'after:', categories[new_top5_catid[i]], round(new_top5_prob[i].item(), 5)]\n","    print(\"{: >10} {: >15} {: >10} {: >10} {: >15} {: >10}\".format(*row))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   before:      sports car    0.32742     after:      sports car    0.32742\n","   before:     convertible    0.18041     after:     convertible    0.18041\n","   before:         minivan    0.09956     after:         minivan    0.09956\n","   before:     beach wagon    0.05116     after:     beach wagon    0.05116\n","   before:          pickup    0.03388     after:          pickup    0.03388\n"]}]},{"cell_type":"markdown","metadata":{"id":"NaCckQhkg742"},"source":["---\n","\n","Get feature 4b: 418, a dog head pose detector. Set its weights to 0. Before and after on dog image.\n","\n","https://distill.pub/2020/circuits/zoom-in/"]},{"cell_type":"code","metadata":{"id":"pgTQccz1hmRN"},"source":["new_mdl = copy.deepcopy(model)\n","new_mdl.inception4b.branch3[1].conv.weight[34].data = new_mdl.inception4b.branch3[1].conv.weight[34] * 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83giWaqKhqs_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631330168139,"user_tz":240,"elapsed":94,"user":{"displayName":"Michael Lan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13558259605338023275"}},"outputId":"43eafdc3-b309-4966-e9ab-13636c3fead3"},"source":["new_output = new_mdl(input_batch)\n","new_probabilities = torch.nn.functional.softmax(new_output[0], dim=0)\n","new_top5_prob, new_top5_catid = torch.topk(new_probabilities, 5)\n","for i in range(new_top5_prob.size(0)):\n","    row = ['before:', categories[top5_catid[i]], round(top5_prob[i].item(), 5), 'after:', categories[new_top5_catid[i]], round(new_top5_prob[i].item(), 5)]\n","    print(\"{: >10} {: >15} {: >10} {: >10} {: >15} {: >10}\".format(*row))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   before:      sports car    0.32742     after:      sports car    0.32742\n","   before:     convertible    0.18041     after:     convertible    0.18041\n","   before:         minivan    0.09956     after:         minivan    0.09956\n","   before:     beach wagon    0.05116     after:     beach wagon    0.05116\n","   before:          pickup    0.03388     after:          pickup    0.03388\n"]}]},{"cell_type":"markdown","metadata":{"id":"uMrC2JMlOFvE"},"source":["\n","\n","---\n","Try looping thru and modifying layers\n"]},{"cell_type":"code","metadata":{"id":"Sx_4tzR1kiOX"},"source":["# https://stackoverflow.com/questions/54846905/pytorch-get-all-layers-of-model\n","\n","named_layers = dict(model.named_modules())\n","len(named_layers)\n","named_layers.keys()\n","# named_layers['conv1']\n","# type(model.conv1.conv) == torch.nn.modules.conv.Conv2d # True\n","# type(model.conv1) == torch.nn.modules.conv.Conv2d  # False\n","# type(named_layers['conv1.conv']) == torch.nn.modules.conv.Conv2d # True\n","\n","modifiable_layers = []\n","for layer in list(named_layers.values()):  # layer doesn't use ref; it is a copy of the model's layer\n","  if type(layer) == torch.nn.modules.conv.Conv2d:\n","    modifiable_layers.append(layer)\n","\n","modifiable_layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Et_BsY51n_9p"},"source":["new_mdl = copy.deepcopy(model)\n","for layer in new_mdl.named_modules():\n","  if type(layer) == torch.nn.modules.conv.Conv2d:\n","    layer.weight.data[0] = layer.weight.data[0] * 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXw4rdqJo-mE"},"source":["getattr(new_mdl.conv1, 'conv') = getattr(new_mdl.conv1, 'conv').weight.data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"km3aHWNm3gl1"},"source":["\n","\n","---\n","\n","Inception seems to be resilient against the change of a filter. Perhaps there are multiple filters that detect dogs. Thus, set entire layers and/or branches to 0."]},{"cell_type":"code","metadata":{"id":"0fOBgh1GTNCp"},"source":["new_mdl = copy.deepcopy(model)\n","# new_mdl.conv1.conv.weight.data = new_mdl.conv1.conv.weight.data * 0\n","# new_mdl.conv1.bn.weight.data = new_mdl.conv1.bn.weight.data * 0\n","# new_mdl.conv2.conv.weight.data = new_mdl.conv2.conv.weight.data * 0\n","new_mdl.conv3.conv.weight.data = new_mdl.conv3.conv.weight.data * 0\n","# new_mdl.inception3a.branch1.conv.weight.data[0] = new_mdl.inception3a.branch1.conv.weight.data[0] * 0\n","# new_mdl.inception3a.branch2[0].conv.weight.data[0] = new_mdl.inception3a.branch2[0].conv.weight.data[0] * 0\n","# new_mdl.inception3a.branch2[1].conv.weight.data[0] = new_mdl.inception3a.branch2[1].conv.weight.data[0] * 0\n","# new_mdl.inception3a.branch3[0].conv.weight.data[0] = new_mdl.inception3a.branch3[0].conv.weight.data[0] * 0\n","# new_mdl.inception3a.branch3[1].conv.weight.data[0] = new_mdl.inception3a.branch3[1].conv.weight.data[0] * 0\n","# new_mdl.inception3b.branch1.conv.weight.data[0] = new_mdl.inception3b.branch1.conv.weight.data[0] * 0\n","\n","# new_mdl.inception4b.branch3[1].conv.weight.data = new_mdl.inception4b.branch3[1].conv.weight.data * 0\n","# new_mdl.inception4c.branch3[1].conv.weight.data = new_mdl.inception4c.branch3[1].conv.weight.data * 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2-Xs3XmWOT9"},"source":["# new_mdl.inception3a.branch2[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqbgzREAVAyg","executionInfo":{"status":"ok","timestamp":1631330835464,"user_tz":240,"elapsed":93,"user":{"displayName":"Michael Lan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13558259605338023275"}},"outputId":"8f7a626a-734a-44c3-adc6-f544d24386c1"},"source":["new_output = new_mdl(input_batch)\n","new_probabilities = torch.nn.functional.softmax(new_output[0], dim=0)\n","new_top5_prob, new_top5_catid = torch.topk(new_probabilities, 5)\n","for i in range(new_top5_prob.size(0)):\n","    row = ['before:', categories[top5_catid[i]], round(top5_prob[i].item(), 5), 'after:', categories[new_top5_catid[i]], round(new_top5_prob[i].item(), 5)]\n","    print(\"{: >10} {: >15} {: >10} {: >10} {: >15} {: >10}\".format(*row))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   before:         Samoyed    0.93784     after:    space heater     0.4274\n","   before:      Pomeranian    0.00828     after:     loudspeaker    0.40686\n","   before:  Great Pyrenees     0.0056     after:   window screen    0.08127\n","   before:      Arctic fox    0.00553     after:        strainer    0.04828\n","   before:      white wolf    0.00474     after:           radio    0.00625\n"]}]}]}